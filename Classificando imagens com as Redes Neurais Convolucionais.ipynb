{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1 align=center>Classificando imagens com redes neurais convolucionais profundas</h1>\n",
    "<p align=center><img src=https://www.electricalelibrary.com/wp-content/uploads/2018/11/convolutional_neural_network.png></p>\n",
    "\n",
    "\n",
    "Já analisamos detalhadamente diferentes aspectos da API do *TensorFlow*, você se familiarizou com tensores e funções de decoração e aprendeu a trabalhar com os estimadores do *TensorFlow*. Aqui você aprenderá agora sobre **redes neurais convolucionais (CNNs)** para classificação de imagens. Começaremos discutindo os blocos básicos de construção das CNNs, usando uma abordagem de baixo para cima. Em seguida, mergulharemos mais fundo na arquitetura CNN e exploraremos como implementar CNNs no *TensorFlow*.\n",
    "\n",
    "### Os blocos de construção das CNNs\n",
    "CNNs são uma família de modelos que foram originalmente inspirados em como o córtex visual do cérebro humano funciona ao reconhecer objetos. O desenvolvimento das CNNs remonta à década de 1990, quando *Yann LeCun* e seus colegas propuseram uma nova arquitetura RN para classificar dígitos manuscritos de imagens (Handwritten Digit Recognition with a Back-Propagation Network, Y. LeCun e outros, 1989, publicado no Conferência de Sistemas de Processamento de Informações Neurais (NeurIPS).\n",
    "\n",
    "> ### O córtex visual humano\n",
    "> A descoberta original de como o córtex visual do nosso cérebro funciona foi feita por David H. Hubel e Torsten Wiesel em 1959, quando inseriram um microeletrodo no córtex visual primário de um gato anestesiado. Então, eles observaram que os neurônios do cérebro respondem de maneira diferente depois de projetar diferentes padrões de luz na frente do gato. Isso acabou levando à descoberta das diferentes camadas do córtex visual. Enquanto a camada primária detecta principalmente bordas e linhas, camadas de ordem superior concentram-se mais na extração de formas e padrões complexos.\n",
    "\n",
    "Devido ao excelente desempenho das CNNs para tarefas de classificação de imagens, esse tipo específico de RN *feedforward* ganhou muita atenção e levou a enormes melhorias no aprendizado de máquina para visão computacional. Vários anos depois, em 2019, *Yann LeCun* recebeu o prêmio *Turing* (o prêmio de maior prestígio em ciência da computação) por suas contribuições ao campo da Inteligência Artificial (IA), juntamente com outros dois pesquisadores, *Yoshua Bengio e Geoffrey Hinton*.\n",
    "\n",
    "A seguir, discutiremos o conceito mais amplo de RNs e por que as arquiteturas convolucionais são frequentemente descritas como **camadas de extração de recursos**. Em seguida, aprofundaremos a definição teórica do tipo de operação de convolução que é comumente usada em CNNs e percorreremos exemplos para calcular convoluções em uma e duas dimensões.\n",
    "\n",
    "### Entendendo as CNNs e as hierarquias de recursos\n",
    "\n",
    "A extração bem-sucedida de **recursos salientes (relevantes)** é fundamental para o desempenho de qualquer algoritmo de aprendizado de máquina e os modelos tradicionais de aprendizado de máquina dependem de recursos de entrada que podem vir de um especialista de domínio ou são baseados em técnicas computacionais de extração de recursos.\n",
    "\n",
    "Certos tipos de RNs, como CNNs, são capazes de aprender automaticamente os recursos de dados brutos que são mais úteis para uma tarefa específica. Por esse motivo, é comum considerar as camadas CNN como extratores de recursos: as camadas iniciais (aquelas logo após a camada de entrada) extraem **características de baixo nível** de dados brutos e as camadas posteriores (geralmente, **camadas totalmente conectadas** como em um perceptron multicamada (*MLP*)) usa esses recursos para prever um valor de destino contínuo ou rótulo de classe.\n",
    "\n",
    "Certos tipos de RNs multicamadas e, em particular, RNs convolucionais profundos (CNNs), constroem a chamada **hierarquia de recursos** combinando os recursos de baixo nível em uma forma de camada para formar recursos de alto nível. Por exemplo, se estamos lidando com imagens, os recursos de baixo nível, como bordas e bolhas, são extraídos das camadas anteriores, que são combinadas para formar recursos de alto nível. Esses recursos de alto nível podem formar formas mais complexas, como os contornos gerais de objetos como prédios, gatos ou cachorros.\n",
    "\n",
    "Como você pode ver na imagem a seguir, uma CNN calcula mapas de recursos de uma imagem de entrada, onde cada elemento vem de um patch local de pixels na imagem de entrada:\n",
    "\n",
    "<img src=https://miro.medium.com/max/1000/1*z7hd8FZeI_eodazwIapvAw.png>\n",
    "\n",
    "Este patch local de pixels é referido como o **campo receptivo local**. As CNNs geralmente têm um desempenho muito bom em tarefas relacionadas à imagem, e isso se deve em grande parte a duas ideias importantes:\n",
    "* **Conectividade esparsa**: Um único elemento no mapa de recursos é conectado a apenas um pequeno trecho de pixels. (Isto é muito diferente de conectar-se a toda a imagem de entrada como no caso dos perceptrons)\n",
    "* **Compartilhamento de parâmetros**: Os mesmos pesos são usados ​​para diferentes patches da imagem de entrada.\n",
    "\n",
    "Como consequência direta dessas duas ideias, substituir um *MLP* convencional totalmente conectado por uma camada de convolução **diminui substancialmente** o número de pesos (parâmetros) na rede e veremos uma melhoria na capacidade de capturar recursos salientes. No contexto de dados de imagem, faz sentido supor que pixels próximos são tipicamente mais relevantes entre si do que pixels distantes.\n",
    "\n",
    "Normalmente, as CNNs são compostas por várias camadas convolucionais e de subamostragem que são seguidas por uma ou mais camadas totalmente conectadas no final. As camadas totalmente conectadas são essencialmente um *MLP*, onde cada unidade de entrada, $\\small i$, está conectada a cada unidade de saída, $\\small j$, com peso $\\small w_{ij}$.\n",
    "\n",
    "Observe que as camadas de subamostragem, comumente conhecidas como **camadas de agrupamento** (*pooling layers*), não possuem parâmetros que podem ser aprendidos; por exemplo, não há pesos ou unidades de polarização nas **camadas de agrupamento**. No entanto, ambas as camadas convolucional e totalmente conectada têm pesos e vieses que são otimizados durante o treinamento.\n",
    "\n",
    "Nas seções a seguir, estudaremos as camadas convolucionais e de *pooling* com mais detalhes e veremos como elas funcionam. Para entender como as operações de convolução funcionam, vamos começar com uma convolução em uma dimensão, que às vezes é usada para trabalhar com certos tipos de dados de sequência, como texto. Depois de discutir as convoluções unidimensionais, trabalharemos com as convoluções bidimensionais típicas que são comumente aplicadas a imagens bidimensionais. \n",
    "\n",
    "### Executando convoluções discretas\n",
    "\n",
    "Uma **convolução discreta** (ou simplesmente convolução) é uma operação fundamental em uma CNN. Portanto, é importante entender como essa operação funciona. Abordaremos a definição matemática e discutiremos alguns dos algoritmos ingênuos para calcular convoluções de tensores unidimensionais (vetores) e tensores bidimensionais (matrizes).\n",
    "\n",
    "Observe que as fórmulas e descrições nesta seção são apenas para entender como funcionam as operações de convolução nas CNNs. De fato, implementações muito mais eficientes de operações convolucionais já existem em pacotes como o *TensorFlow*, como você verá mais adiante.\n",
    "\n",
    "### Convoluções discretas em uma dimensão\n",
    "Vamos começar com algumas definições e notações básicas que vamos usar. Uma convolução discreta para dois vetores, $\\small x$ e $\\small w$, é denotada por $y = x \\times w$ , em que o vetor x é nossa entrada (às vezes chamado de **signal**) e $\\small w$ é chamado de **filter** ou **kernel**. Uma convolução discreta é definida matematicamente da seguinte forma:\n",
    "\n",
    "$$\n",
    "y = x \\times w \\to y[i] = \\sum^{+ \\infty}_{k=-\\infty}x[i-k]w[k]\n",
    "$$\n",
    "\n",
    "Os colchetes, $[ \\: ]$ , são usados ​​para denotar a indexação de elementos vetoriais. O índice, $\\small i$, percorre cada elemento do vetor de saída, $\\small y$. Há duas coisas estranhas na fórmula anterior que precisamos esclarecer: $−\\infty$ a $+\\infty$ índices e indexação negativa para $\\small x$.\n",
    "O fato de a soma percorrer índices de $−\\infty$ a $+\\infty$ parece estranho, principalmente porque em aplicações de aprendizado de máquina, sempre lidamos com vetores de características finitos. Por exemplo, se $\\small x$ tem 10 traços com índices 0, 1, 2,…, 8, 9, então os índices $−\\infty$∶ −1 e 10 ∶ $+\\infty$ estão fora dos limites para $\\small x$. Portanto, para calcular corretamente a soma mostrada na fórmula anterior, assume-se que $\\small x$ e $\\small w$ são preenchidos com zeros. Isso resultará em um vetor de saída, $\\small y$, que também tem tamanho infinito, com muitos zeros também. Como isso não é útil em situações práticas, $\\small x$ é preenchido apenas com um número finito de zeros.\n",
    "\n",
    "Esse processo é chamado de **zero-padding** (preenchimento zero) ou simplesmente **padding**. Aqui, o número de zeros preenchidos em cada lado é denotado por $\\small p$. Um exemplo de preenchimento de um vetor unidimensional, $\\small x$, é mostrado na figura a seguir:\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200905045036227.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xpbmxpNTIyMzYyMjQy,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "Vamos supor que a entrada original, $\\small x$, e o filtro, $\\small w$, tenham `n` e `m` elementos, respectivamente, onde $\\small  m \\leq n$ . Portanto, o vetor preenchido, $\\small x^p$ , tem tamanho $\\small n + 2p$. A fórmula prática para calcular uma convolução discreta mudará para o seguinte:\n",
    "\n",
    "$$\n",
    "y = x \\times w \\to y[i] = \\sum^{k=m-1}_{k=0}x^{p}[i+m-k]w[k]\n",
    "$$\n",
    "\n",
    "Agora que resolvemos o problema do índice infinito, o segundo problema é indexar $\\small x$ com $\\small i + m – k$. O ponto importante a ser observado aqui é que $\\small x$ e $\\small w$ são indexados em direções diferentes nessa soma. Calcular a soma com um índice indo na direção inversa é equivalente a calcular a soma com ambos os índices na direção direta depois de inverter um desses vetores, $\\small x$ ou $\\small w$, depois de serem preenchidos. Então, podemos simplesmente calcular seu produto escalar. Vamos supor que viramos (giramos) o filtro, $\\small w$, para obter o filtro girado, $\\small w^r$. Então, o produto escalar, $\\small x[i:i+m]$. $\\small w^r$, é calculado para obter um elemento, $\\small y[i]$, onde $\\small x[i: i + m]$ é um patch de $\\small x$ com tamanho $small m$. Esta operação é repetida como em uma abordagem de janela deslizante para obter todos os elementos de saída. A figura a seguir fornece um exemplo com $\\small x = [3 \\: 2\\: 1\\: 7\\: 1\\: 2\\: 5\\: 4]$ e $\\small w = [\\dfrac{1}{2} \\: \\dfrac{3}{4} \\: 1\\:  \\dfrac{1}{4}]$ para que os três primeiros elementos de saída sejam calculados:\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200906095751465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xpbmxpNTIyMzYyMjQy,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "Você pode ver no exemplo anterior que o tamanho do preenchimento é zero ($\\small p = 0$). Observe que o filtro girado, $\\small w^r$ , é deslocado em duas células cada vez que deslocamos (**shift**). Esse deslocamento é outro hiperparâmetro de uma convolução, o **stride** (passo), $\\small s$. Neste exemplo, o passo é dois, $\\small s = 2$. Observe que o passo deve ser um número positivo menor que o tamanho do vetor de entrada. Falaremos mais sobre *padding* e *strides* posteriomente.\n",
    "\n",
    "### Preenchimento de entradas para controlar o tamanho dos mapas de recursos de saída\n",
    "Até agora, usamos apenas *zero-padding* em convoluções para calcular vetores de saída de tamanho finito. Tecnicamente, o preenchimento pode ser aplicado com qualquer $\\small p \\geq 0$ . Dependendo da escolha de $\\small p$, as células limítrofes podem ser tratadas de forma diferente das células localizadas no meio de $\\small x$.\n",
    "\n",
    "Agora, considere um exemplo onde $\\small n = 5$ e $\\small m = 3$. Então, com $\\small p=0$, $\\small x[0]$ é usado apenas no cálculo de um elemento de saída (por exemplo, $\\small y[0]$), enquanto $\\small x[1]$ é usado no cálculo de dois elementos de saída (por exemplo, $\\small y[0]$ e $\\small y[1]$). Então, você pode ver que esse tratamento diferente dos elementos de $\\small x$ pode colocar artificialmente mais ênfase no elemento do meio, $\\small x[2]$, já que ele apareceu na maioria dos cálculos. Podemos evitar esse problema se escolhermos $\\small p = 2$, caso em que cada elemento de $\\small x$ estará envolvido no cálculo de três elementos de $\\small y$.\n",
    "\n",
    "Além disso, o tamanho da saída, $\\small y$, também depende da escolha da estratégia de preenchimento que usamos.\n",
    "\n",
    "Existem três modos de preenchimento que são comumente usados na prática: *full* (completo), *same* (igual) e *valid* (válido):\n",
    "* No modo *full*, o parâmetro de *padding*, $\\small p$, é definido como $\\small p = m – 1$. O preenchimento *full* aumenta as dimensões da saída; assim, <u>raramente é usado em arquiteturas CNN</u>.\n",
    "* O *Same* preenchimento geralmente é usado para garantir que o vetor de saída tenha o mesmo tamanho que o vetor de entrada, $\\small x$. Nesse caso, o parâmetro de preenchimento, $\\small p$, é calculado de acordo com o tamanho do filtro, juntamente com o requisito de que o tamanho de entrada e o tamanho de saída sejam os mesmos.\n",
    "* Finalmente, computar uma convolução no modo *valid* refere-se ao caso em que $\\small p = 0$ (sem *padding*).\n",
    "\n",
    "![](imagens\\padding.PNG)\n",
    "\n",
    "O modo de preenchimento mais usado em CNNs é o *SAME PADDING*. Uma de suas vantagens sobre os outros modos de preenchimento é que o *Same Padding* preserva o tamanho do vetor - ou a altura e a largura das imagens de entrada quando estamos trabalhando em tarefas relacionadas a imagens em visão computacional - o que torna o projeto de uma arquitetura de rede mais conveniente.\n",
    "\n",
    "Uma grande desvantagem do *valid padding* versus preenchimento *full* e *same*, por exemplo, é que o volume dos tensores diminuirá substancialmente em RNs com muitas camadas, o que pode prejudicar o desempenho da rede.\n",
    "\n",
    "Na prática, é recomendável preservar o tamanho espacial usando o mesmo preenchimento para as camadas convolucionais e, em vez disso, diminuir o tamanho espacial por meio de camadas de *pooling*. Quanto ao *padding full*, seu tamanho resulta em uma saída maior que o tamanho da entrada. O *padding full* é geralmente usado em aplicações de **processamento de sinal** onde é importante minimizar os efeitos de contorno. No entanto, no contexto de aprendizado profundo, os efeitos de limite geralmente não são um problema, portanto, raramente vemos o *padding full* sendo usado na prática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinando o tamanho da saída de convolução\n",
    "\n",
    "O tamanho de saída de uma convolução é determinado pelo número total de vezes que deslocamos o filtro, $\\small w$, ao longo do vetor de entrada. Vamos supor que o vetor de entrada seja de tamanho $\\small n$ e o filtro seja de tamanho $\\small m$. Então, o tamanho da saída resultante de $y = x \\: * \\: w$, com *padding*, $\\small p$, e *stride*, $\\small s$, seria determinado da seguinte forma:\n",
    "\n",
    "$$\n",
    "o = [\\dfrac{n + 2p - m}{s}] + 1\n",
    "$$\n",
    "\n",
    "Aqui, $[\\:]$ denota a *floor operation*.\n",
    "\n",
    "> #### A operação de piso (*floor operation*)\n",
    "> A operação de piso retorna o maior inteiro igual ou menor que a entrada, por exemplo:\n",
    "> $$\n",
    "\\small floor(1.77) = [1.77] = 1\n",
    "$$\n",
    "\n",
    "Considere os dois casos a seguir:\n",
    "* Calcular o tamanho de saída para um vetor de entrada de tamanho 10 com um kernel de convolução de tamanho 5, *padding* 2 e *stride* 1:\n",
    "$$\\small \n",
    "n = 10,m=5, \\quad p=2, \\quad s = 1 \\to o = [\\dfrac{10 +  2 \\times 2 -5}{1}] + 1 = 10\n",
    "$$\n",
    "\n",
    "(Observe que, neste caso, o tamanho da saída acaba sendo o mesmo que a entrada; portanto, podemos concluir que este é o smodo *same-padding*.)\n",
    "\n",
    "* Como o tamanho da saída muda para o mesmo vetor de entrada quando tem um kernel de tamanho 3 e *stride* 2?\n",
    "$$\\small \n",
    "n = 10,m=3, \\quad p=2, \\quad s = 2 \\to o = [\\dfrac{10 +  2 \\times 2 -3}{2}] + 1 = 6\n",
    "$$\n",
    "\n",
    "Finalmente, para aprender a calcular convoluções em uma dimensão, uma implementação ingênua é mostrada no bloco de código a seguir e os resultados são comparados com a função `numpy.convolve`. O código é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "NumPy version:  1.21.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('NumPy version: ', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n",
      "Numpy Results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate(\n",
    "            [zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int((len(x_padded) - len(w_rot)) / s) + 1, s):\n",
    "        res.append(np.sum(\n",
    "            x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d Implementation:',\n",
    "      conv1d(x, w, p=2, s=1))\n",
    "\n",
    "print('Numpy Results:',\n",
    "      np.convolve(x, w, mode='same')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora, nos concentramos principalmente em convoluções para vetores (convoluções 1D). Começamos com o caso 1D para tornar os conceitos mais fáceis de entender. Na próxima seção, abordaremos as convoluções 2D com mais detalhes, que são os blocos de construção das CNNs para tarefas relacionadas a imagens.\n",
    "\n",
    "### Executando uma convolução discreta em 2D\n",
    "Os conceitos que você aprendeu nas seções anteriores são facilmente extensíveis para 2D. Quando lidamos com entradas 2D, como uma matriz, $\\small X_{n_1 \\times n_2}$ e a matriz do filtro, $\\small X_{m_1 \\times m_2}$, onde $\\small m_1 \\leq n_1$ e $\\small m_2\\leq n_2$, então a matriz $\\small Y = X \\: * W$ é o resultado de uma convolução 2D entre $\\small X$ e $\\small W$. Isso é definido matematicamente da seguinte forma:\n",
    "\n",
    "$$\\small \n",
    "Y = X \\: * \\: W \\to Y[i,j] = \\sum^{+ \\infty}_{k_1=-\\infty} \\sum^{+ \\infty}_{k_2=-\\infty} X[i - k_1, j - k_2]W[k_1,k_2]\n",
    "$$\n",
    "\n",
    "Observe que se você omitir uma das dimensões, a fórmula restante é exatamente a mesma que usamos anteriormente para calcular a convolução em 1D. De fato, todas as técnicas mencionadas anteriormente, como *zero-padding*, rotação da matriz do filtro e uso de *strides*, também são aplicáveis às convoluções 2D, desde que sejam estendidas para ambas as dimensões de forma independente. A figura a seguir demonstra a convolução 2D de uma matriz de entrada de tamanho $\\small 8 \\times 8$ , usando um kernel de tamanho $\\small 3 \\times 3$. A matriz de entrada é preenchida com zeros com $\\small p = 1$. Como resultado, a saída da convolução 2D terá um tamanho de $\\small 8 × 8$:\n",
    "\n",
    "![](imagens\\CNN2D.PNG)\n",
    "\n",
    "O exemplo a seguir ilustra o cálculo de uma convolução 2D entre uma matriz de entrada, $\\small X_{3 \\times 3}$, e uma matriz kernel, $\\small W_{3 \\times 3}$, usando *padding* $\\small p = (1, 1)$ e *stride* $\\small s = (2, 2)$. De acordo com o preenchimento especificado, uma camada de zeros é adicionada em cada lado da matriz de entrada, o que resulta na matriz preenchida $\\small X^{padded}_{5 \\times 5}$, como segue:\n",
    "\n",
    "![](imagens\\padded.PNG)\n",
    "\n",
    "Com o filtro anterior, o filtro girado será:\n",
    "\n",
    "$$\n",
    "W^r = \\begin{bmatrix}\n",
    "0.5 & 1 & 0.5 \\\\\n",
    "0.1 & 0.4 & 0.3 \\\\\n",
    "0.4 & 0.7 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Observe que essa rotação não é a mesma que a matriz de transposição. Para obter o filtro girado no *NumPy*, podemos escrever `W_rot=W[::-1,::-1]`. Em seguida, podemos deslocar a matriz de filtro girada ao longo da matriz de entrada preenchida, $X^{padded}$ , como uma janela deslizante e calcular a soma do produto elemento a elemento, que é denotado pelo operador $\\odot$ na figura a seguir:\n",
    "\n",
    "![](imagens\\x_padded.PNG)\n",
    "\n",
    "O resultado será a matriz $\\: \\small 2 \\times 2$, $\\small Y$.\n",
    "\n",
    "Vamos também implementar a convolução 2D de acordo com o algoritmo ingênuo descrito. O pacote `scipy.signal` fornece uma maneira de calcular a convolução 2D por meio da função `scipy.signal.convolve2d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation:\n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "\n",
      "SciPy Results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0]:p[0]+X_orig.shape[0],\n",
    "    p[1]:p[1]+X_orig.shape[1]] = X_orig\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - \n",
    "                           W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - \n",
    "                               W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0],\n",
    "                             j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return(np.array(res))\n",
    "\n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "\n",
    "print('Conv2d Implementation:\\n',\n",
    "    conv2d(X, W, p=(1, 1), s=(1, 1)))\n",
    "\n",
    "\n",
    "print('\\nSciPy Results:\\n',\n",
    "    scipy.signal.convolve2d(X, W, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Algoritmos eficientes para computação de convolução\n",
    "> Fornecemos uma implementação ingênua para calcular uma convolução 2D com o objetivo de entender os conceitos. No entanto, esta implementação é muito ineficiente em termos de requisitos de memória e complexidade computacional. Portanto, ele não deve ser usado em aplicativos de RN do mundo real. Um aspecto é que a matriz do filtro não é girada na maioria das ferramentas como o *TensorFlow*. Além disso, nos últimos anos, foram desenvolvidos algoritmos muito mais eficientes que usam a *transformada de Fourier* para calcular as convoluções. Também é importante observar que, no contexto das RNs, o tamanho de um kernel de convolução geralmente é **muito menor que o tamanho da imagem de entrada**.\n",
    ">\n",
    "> Por exemplo, CNNs modernas geralmente usam tamanhos de kernel como `1 × 1` , `3 × 3` ou `5 × 5`, para os quais foram projetados algoritmos eficientes que podem realizar as operações convolucionais com muito mais eficiência, como o algoritmo de filtragem mínima de *Winograd*.\n",
    "\n",
    "### Camadas de subamostragem\n",
    "A subamostragem é normalmente aplicada em duas formas de operações de *pooling* em CNNs: **max-pooling** e **mean-pooling** (também conhecido como **average-pooling**). A camada de agrupamento é geralmente denotada por $\\small p_{n_1 \\times n_2}$. Aqui, o subscrito determina o tamanho da vizinhança (o número de pixels adjacentes em cada dimensão) onde a operação máxima ou média é realizada. Referimo-nos a essa vizinhança como o tamanho do pool.\n",
    "\n",
    "A operação é descrita na figura a seguir. No caso, *max-pooling* pega o **valor máximo** de uma vizinhança de pixels, e o *mean-pooling* calcula sua **média**:\n",
    "\n",
    "![](imagens\\pooling.PNG)\n",
    "\n",
    "A vantagem do *pooling* é dupla:\n",
    "* *Pooling* (*max-pooling*) introduz uma invariância local. Isso significa que pequenas mudanças em uma vizinhança local não alteram o resultado do *max-pooling*.\n",
    "\n",
    "Portanto, ajuda na geração de recursos que são mais robustos ao ruído nos dados de entrada. Consulte o exemplo a seguir, que mostra que o *max-pooling* de duas matrizes de entrada diferentes, $\\small X_1$ e $\\small X_2$ , resulta na mesma saída:\n",
    "\n",
    "![](imagens\\matriz_pooling.PNG)\n",
    "\n",
    "O agrupamento diminui o tamanho dos recursos, o que resulta em **maior eficiência computacional**. Além disso, reduzir o número de recursos também pode reduzir o grau de *overfitting*.\n",
    "\n",
    "> ##### *Pooling* sobreposto versus não sobreposto\n",
    ">Tradicionalmente, o *pooling* é assumido como não sobreposto. O *pooling* é normalmente executado em vizinhanças não sobrepostas, o que pode ser feito definindo o parâmetro stride igual ao tamanho do *pooling*.\n",
    ">\n",
    "> Por exemplo, uma camada de agrupamento não sobreposta, $p_{n_1 \\times n_2}$, requer um parâmetro de passada $\\small s = (n_1,n_2)$ . Por outro lado, o agrupamento sobreposto ocorre se o *stride* for menor que o tamanho do agrupamento. Um exemplo em que o *pooling* sobreposto é usado em uma rede convolucional é descrito em *ImageNet Classification with Deep Convolutional Neural Networks*.\n",
    "\n",
    "Embora o *pooling* ainda seja uma parte essencial de muitas arquiteturas CNN, várias arquiteturas CNN também foram desenvolvidas sem o uso de camadas de *pooling*. Em vez de usar camadas de *pooling* para reduzir o tamanho do recurso, os pesquisadores usam camadas convolucionais com um *stride* de 2. De certa forma, você pode pensar em uma camada convolucional com *stride* 2 como uma camada de *pooling* com pesos aprendíveis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando tudo – implementando uma CNN\n",
    "Até agora, você aprendeu sobre os blocos básicos de construção das CNNs. Os conceitos ilustrados não são realmente mais difíceis do que as RNs multicamadas tradicionais. Podemos dizer que a operação mais importante em uma RN tradicional é a multiplicação de matrizes. Por exemplo, usamos multiplicações de matrizes para calcular as pré-ativações (ou entradas líquidas), como em $\\small z = Wx +b$. Aqui, $\\small x$ é um vetor coluna (matriz $\\mathbb{R}^{n \\times 1}$) representando pixels e $\\small W$ é a matriz de peso que conecta as entradas de pixel a cada unidade oculta.\n",
    "\n",
    "Em uma CNN, esta operação é substituída por uma operação de convolução, como em $\\small Z = W \\:*\\: X  + b$, onde $\\small X$ é uma matriz que representa os pixels em um arranjo $\\small heigth \\times width$. Em ambos os casos, as pré-ativações são passadas para uma função de ativação para obter a ativação de uma unidade oculta, $\\small A = \\phi(Z)$, onde $\\small \\phi$ é a função de ativação. Além disso, você deve se lembrar de que a subamostragem é outro bloco de construção de uma CNN, que pode aparecer na forma de *pooling*, conforme descrito na seção anterior.\n",
    "\n",
    "### Trabalhando com vários canais de entrada ou de cores\n",
    "\n",
    "Uma entrada para uma camada convolucional pode conter uma ou mais matrizes ou matrizes 2D com dimensões $\\small N_1 \\times N_2$ (por exemplo, a altura e a largura da imagem em pixels). Essas matrizes $\\small N_1 \\times N_2$ são chamadas de canais. Implementações convencionais de camadas convolucionais esperam uma representação de tensor de nível 3 como entrada, por exemplo, uma matriz tridimensional, $\\small X_{n_1 \\times N_2 \\times C_{in}}$ , onde $\\small C_{in}$ é o número de canais de entrada. Por exemplo, vamos considerar imagens como entrada para a primeira camada de uma CNN. Se a imagem for colorida e usar o modo de cor RGB, então $\\small C_{in}$ = 3 (para os canais de cores vermelho, verde e azul em RGB). No entanto, se a imagem estiver em tons de cinza, então temos $C_{in}$ = 1, pois há apenas um canal com os valores de intensidade de pixel em tons de cinza.\n",
    "\n",
    "> #### Lendo um arquivo de imagem\n",
    "> Quando trabalhamos com imagens, podemos ler imagens em *arrays NumPy* usando o tipo de dados `uint8` (unsigned 8-bit integer) para reduzir o uso de memória em comparação com tipos inteiros de 16 bits, 32 bits ou 64 bits, por exemplo.\n",
    ">\n",
    "> Inteiros de 8 bits sem sinal assumem valores no intervalo [0, 255], que são suficientes para armazenar as informações de pixel em imagens RGB, que também assumem valores no mesmo intervalo.\n",
    "> \n",
    "> O *TensorFlow* fornece um módulo para carregar/armazenar e manipular imagens por meio dos submódulos `tf.io` e `tf.image`. Vamos recapitular como ler uma imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_shape: (550, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "img_raw = tf.io.read_file(\"imagens\\passaro-cantando.jpg\")\n",
    "img = tf.image.decode_image(img_raw)\n",
    "print(f\"Image_shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao criar modelos e carregadores de dados no *TensorFlow*, é recomendável usar `tf.image `também para ler as imagens de entrada.\n",
    "\n",
    "Agora, vamos ver também um exemplo de como podemos ler uma imagem em nossa sessão *Python* usando o pacote `imageio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (550, 1000, 3)\n",
      "\n",
      "Number of Channels: 3\n",
      "\n",
      "Image data type: uint8\n",
      "\n",
      "[[[100 116 103]\n",
      "  [100 116 103]]\n",
      "\n",
      " [[100 116 103]\n",
      "  [100 116 103]]]\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "img = imageio.imread(\"imagens\\passaro-cantando.jpg\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print()\n",
    "print(f\"Number of Channels: {img.shape[2]}\")\n",
    "print()\n",
    "print(f\"Image data type: {img.dtype}\")\n",
    "print()\n",
    "print(img[100:102, 100:102, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que você está familiarizado com a estrutura dos dados de entrada, a próxima pergunta é: como podemos incorporar vários canais de entrada na operação de convolução que discutimos nas seções anteriores? A resposta é muito simples: realizamos a operação de convolução para cada canal separadamente e depois somamos os resultados usando a soma da matriz. A convolução associada a cada canal (`c`) tem sua própria matriz kernel como `W[:, :, c]`. O resultado total da pré-ativação é calculado na seguinte fórmula:\n",
    "\n",
    "![](imagens\\conv_c.PNG)\n",
    "\n",
    "O resultado final, $\\small A$, é um mapa de características. Normalmente, uma camada convolucional de uma CNN possui mais de um mapa de *features*. Se usarmos vários mapas de *features*, o tensor do kernel se tornará quadridimensional: $\\small width \\times height \\times C_{in} \\times C_{out}$. Aqui, $\\small width \\times height$ é o tamanho do kernel, $\\small C_{in}$ é o número de canais de entrada e $\\small C_{out}$ é o número de mapas de *features* de saída. Então, agora vamos incluir o número de mapas de *features* de saída na fórmula anterior e atualizá-lo, da seguinte forma:\n",
    "\n",
    "![](imagens\\conv_c_atu.PNG)\n",
    "\n",
    "Para concluir nossa discussão sobre convoluções de computação no contexto de RNs, vejamos o exemplo na figura a seguir, que mostra uma camada convolucional, seguida por uma camada de agrupamento. Neste exemplo, existem três canais de entrada. O tensor do kernel é quadridimensional. Cada matriz kernel é denotada como $\\small m_1 \\times m_2$, e existem três delas, uma para cada canal de entrada. Além disso, existem cinco desses kernels, representando cinco mapas de *features* de saída. Finalmente, há uma camada de agrupamento para subamostragem dos mapas de *features*:\n",
    "\n",
    "![](imagens\\conv_mapa.PNG)\n",
    "\n",
    "> #### Quantos parâmetros treináveis ​​existem no exemplo anterior?\n",
    "> Para ilustrar as vantagens de convolução, **compartilhamento de parâmetros** e **conectividade esparsa**, vamos trabalhar com um exemplo. A camada convolucional na rede mostrada na figura anterior é um tensor de quatro dimensões. Então, existem $\\small m_1 \\times m_2 \\times 3 \\times 5$ parâmetros associados ao kernel. Além disso, há um vetor de polarização para cada mapa de *features* de saída da camada convolucional. Assim, o tamanho do vetor de polarização é $\\small 5$. As camadas de agrupamento não possuem nenhum parâmetro (treinável); portanto, podemos escrever o seguinte:\n",
    ">$$\n",
    "\\small m_1 \\times m_2 \\times 3 \\times 5 + 5\n",
    ">$$\n",
    "> Se o tensor de entrada for de tamanho $\\small n_1 \\times n_2 \\times 3$, supondo que a convolução seja realizada com o modo de *same-padding*, os mapas de *features* de saída seriam de tamanho $\\small n_1 \\times n_2 \\times 5$ .\n",
    ">\n",
    "> Observe que se usarmos uma camada totalmente conectada em vez de uma camada convolucional, esse número será muito maior. No caso de uma camada totalmente conectada, o número de parâmetros para que a matriz de pesos atinja o mesmo número de unidades de saída seria o seguinte:\n",
    ">$$\n",
    "\\small (n_1 \\times n_2 \\times 3) \\times (n_1 \\times n_2 \\times 5) = (n_1 \\times n_2)^2 \\times 3 \\times 5\n",
    ">$$\n",
    "> Além disso, o tamanho do vetor de polarização é $\\small n_1 \\times n_2 \\times 5$ (um elemento de polarização para cada unidade de saída). Dado que $\\small m_1 < n_1$ e $m_2<n_2$, podemos ver que a diferença no número de parâmetros treináveis ​​é significativa.\n",
    "\n",
    "Por fim, como já foi mencionado, tipicamente, as operações de convolução são realizadas tratando uma imagem de entrada com múltiplos canais de cores como uma pilha de matrizes; ou seja, realizamos a convolução em cada matriz separadamente e depois somamos os resultados, conforme ilustrado na figura anterior. No entanto, as convoluções também podem ser estendidas para volumes 3D se você estiver trabalhando com conjuntos de dados 3D.\n",
    "\n",
    "### Regularizando uma RN com *dropout*\n",
    "\n",
    "Escolher o tamanho de uma rede, seja uma RN tradicional (totalmente conectada) ou uma CNN, sempre foi um problema desafiador. Por exemplo, o tamanho de uma matriz de peso e o número de camadas precisam ser ajustados para obter um desempenho razoavelmente bom. Você deve se lembrar que uma rede simples sem nenhuma camada oculta só poderia capturar um limite de decisão linear, o que não é suficiente para lidar com um problema exclusivo ou (`XOR`) ou similar. A **capacidade** de uma rede refere-se ao nível de complexidade da função que ela pode aprender a aproximar. Redes pequenas, ou redes com um número relativamente pequeno de parâmetros, têm uma capacidade baixa e, portanto, são passíveis de *underfit*, resultando em desempenho ruim, uma vez que não podem aprender a estrutura subjacente de conjuntos de dados complexos. No entanto, redes muito grandes podem resultar em *overfitting*, onde a rede memorizará os dados de treinamento e se sairá extremamente bem no conjunto de dados de treinamento, enquanto obtém um desempenho ruim no conjunto de dados de teste mantido. Quando lidamos com problemas de aprendizado de máquina do mundo real, não sabemos a priori o tamanho da rede.\n",
    "\n",
    "Uma maneira de resolver esse problema é construir uma rede com uma capacidade relativamente grande (na prática, queremos escolher uma capacidade um pouco maior do que o necessário) para se sair bem no conjunto de dados de treinamento. Então, para evitar *overfitting*, podemos aplicar um ou vários esquemas de regularização para obter um bom desempenho de generalização em novos dados, como o conjunto de dados de teste retido.\n",
    "\n",
    "A técnica de regularização `L1` e `L2`, podem prevenir ou reduzir o efeito do *overfitting* adicionando uma penalidade à perda que resulta na redução dos parâmetros de peso durante o treinamento. Embora a regularização `L1` e `L2` também possa ser usada para RNs, sendo `L2` a escolha mais comum dos dois, existem outros métodos para regularizar RNs, como *dropout*, que discutiremos nesta seção. Mas antes de passarmos a discutir o *dropout*, para usar a regularização `L2` dentro de uma rede convolucional ou totalmente conectada (densa), você pode simplesmente adicionar a penalidade `L2` à função de perda definindo o `kernel_regularizer` de uma camada específica ao usar a API *Keras*, como segue (ele modificará automaticamente a função de perda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "conv_layer = keras.layers.Conv2D(\n",
    "    filters=16,\n",
    "    kernel_size=(3,3),\n",
    "    kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "\n",
    "fc_layer = keras.layers.Dense(\n",
    "    units=16,\n",
    "    kernel_regularizer=keras.regularizers.l2(0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos últimos anos, o *dropout* emergiu como uma técnica popular para regularizar (*deep*) RNs para evitar *overfitting*, melhorando assim o desempenho da generalização. O *dropout* geralmente é aplicado às unidades ocultas de camadas superiores e funciona da seguinte forma: durante a fase de treinamento de uma RN, uma fração das unidades ocultas é descartada aleatoriamente a cada iteração com probabilidade $\\small p_{drop}$ (ou manter probabilidade $\\small p_{keep} = 1 − p_{drop}$). Essa probabilidade de *dropout* é determinada pelo usuário e a escolha comum é $\\small p = 0.5$. Ao descartar uma certa fração de neurônios de entrada, os pesos associados aos neurônios restantes são redimensionados para explicar os neurônios ausentes (descartados).\n",
    "\n",
    "O efeito desse *dropout* aleatório é que a rede é forçada a aprender uma representação redundante dos dados. Portanto, a rede não pode contar com a ativação de nenhum conjunto de unidades ocultas, pois elas podem ser desligadas a qualquer momento durante o treinamento, sendo forçada a aprender padrões mais gerais e robustos a partir dos dados.\n",
    "\n",
    "Este *dropout* aleatório pode efetivamente prevenir o *overfitting*. A figura a seguir mostra um exemplo de aplicação de *dropout* com probabilidade $\\small p = 0.5$ durante a fase de treinamento, em que metade dos neurônios ficará inativa aleatoriamente (unidades descartadas são selecionadas aleatoriamente em cada passagem de treinamento). No entanto, durante a previsão, todos os neurônios contribuirão para computar as pré-ativações da próxima camada.\n",
    "\n",
    "![](imagens\\dropout.PNG)\n",
    "\n",
    "Como mostrado aqui, um ponto importante a ser lembrado é que as unidades podem cair aleatoriamente apenas durante o treinamento, enquanto para a fase de avaliação (inferência), todas as unidades ocultas devem estar ativas (por exemplo, $p_{drop} = 0$ ou $p_{keep} = 1$). Para garantir que as ativações gerais estejam na mesma escala durante o treinamento e a previsão, as ativações dos neurônios ativos devem ser dimensionadas adequadamente (por exemplo, reduzindo pela metade a ativação se a probabilidade de *dropout* foi definida como $\\small p = 0.5$).\n",
    "\n",
    "No entanto, como é inconveniente sempre dimensionar as ativações ao fazer previsões, o *TensorFlow* e outras ferramentas dimensionam as ativações durante o treinamento (por exemplo, dobrando as ativações se a probabilidade de *dropout* foi definida como $\\small p = 0.5$). Essa abordagem é comumente chamada de dropout inverso. Embora a relação não seja imediatamente óbvia, o *dropout* pode ser interpretado como o consenso (média) de um conjunto de modelos. No aprendizado conjunto, treinamos vários modelos independentemente. Durante a previsão, usamos o consenso de todos os modelos treinados. Já sabemos que conjuntos de modelos são conhecidos por terem um desempenho melhor do que modelos únicos. No aprendizado profundo, no entanto, treinar vários modelos e coletar e calcular a média da saída de vários modelos é computacionalmente caro. Aqui, o *dropout* oferece uma solução alternativa, com uma maneira eficiente de treinar muitos modelos de uma só vez e calcular suas previsões médias no tempo de teste ou previsão. \n",
    "\n",
    "Como mencionado anteriormente, a relação entre os conjuntos de modelos e o *dropout* não é imediatamente óbvia. No entanto, considere que no *dropout*, temos um modelo diferente para cada mini-lote (devido à definição dos pesos para zero aleatoriamente durante cada passagem para frente).\n",
    "\n",
    "Então, por meio da iteração nos mini-lotes, essencialmente amostramos modelos $\\small M = 2^h$, onde $\\small h$ é o número de unidades ocultas.\n",
    "\n",
    "A restrição e o aspecto que distingue a evasão do *ensembling* regular, no entanto, é que compartilhamos os pesos sobre esses \"diferentes modelos\", o que pode ser visto como uma forma de regularização. Então, durante a \"inferência\" (por exemplo, prevendo os rótulos no conjunto de dados de teste), podemos calcular a média de todos esses modelos diferentes que amostramos durante o treinamento. **Isso é muito custoso!**.\n",
    "\n",
    "Então, calculando a média dos modelos, isto é, calculando a média geométrica da probabilidade de associação de classe que é retornada por um modelo, $\\small i$, pode ser calculado da seguinte forma:\n",
    "\n",
    "$$\\small\n",
    "p_{ensemble} = \\left   [      \\prod^M_{j=1}p^{\\left \\{ i \\right \\}}      \\right ]^{ \\dfrac{1}{M}}\n",
    "$$\n",
    "\n",
    "Agora, o truque por trás do *dropout* é que essa média geométrica dos conjuntos de modelos (aqui, $\\small M$ modelos) pode ser aproximada escalando as previsões do último (ou final) modelo amostrado durante o treinamento por um fator de $\\small 1/(1 – p)$, que é muito mais barato do que calcular a média geométrica explicitamente usando a equação anterior. (Na verdade, a aproximação é exatamente equivalente à verdadeira média geométrica se considerarmos modelos lineares.)\n",
    "\n",
    "### Funções de perda para classificação\n",
    "Provavelmente, você já viu diferentes funções de ativação, como `ReLU`, `sigmoid` e `tanh`. Algumas dessas funções de ativação, como `ReLU`, são usadas principalmente nas camadas intermediárias (ocultas) de uma RN para adicionar não linearidades ao nosso modelo. Mas outros, como `sigmoid` (para binário) e `softmax` (para multiclasse), são adicionados na última camada (saída), o que resulta em probabilidades de associação de classe como saída do modelo. Se as ativações `sigmoid` ou `softmax` não forem incluídas na camada de saída, o modelo calculará os `logits` em vez das probabilidades de associação de classe.\n",
    "\n",
    "Focando nos problemas de classificação aqui, dependendo do tipo de problema (binário versus multiclasse) e do tipo de saída (logit versus probabilidades), devemos escolher a função de perda apropriada para treinar nosso modelo. A **Binary cross-entropy** (entropia cruzada binária) é a função de perda para uma classificação binária (com uma única unidade de saída), e a **categorical cross-entropy** (entropia cruzada categórica) é a função de perda para a classificação multiclasse. Na API *Keras*, são fornecidas duas opções para perda categórica de entropia cruzada, dependendo se os rótulos de verdade do terreno estão em um formato codificado *one-hot* (por exemplo, [0, 0, 1, 0]) ou fornecidos como inteiro rótulos (por exemplo, $\\small y=2$), que também é conhecido como representação \"esparsa\" no contexto de *Keras*.\n",
    "\n",
    "A tabela a seguir descreve três funções de perda disponíveis no *Keras* para lidar com todos os três casos: classificação binária, multiclasse com rótulos de verdade do solo codificados com um *hot* e multiclasse com rótulos inteiros (esparsos). Cada uma dessas três funções de perda também tem a opção de receber as previsões na forma de `logits` ou probabilidades de pertinência de classe:\n",
    "\n",
    "![](imagens\\tabela_loss.PNG)\n",
    "\n",
    "Observe que calcular a perda de entropia cruzada fornecendo os *logits*, e não as probabilidades de associação de classe, geralmente é preferível devido a razões de estabilidade numérica. Se fornecermos *logits* como entradas para a função de perda e definir `from_logits=True`, a respectiva função do *TensorFlow* usará uma implementação mais eficiente para calcular a perda e a derivada da perda em relação aos pesos. Isso é possível porque certos termos matemáticos se cancelam e, portanto, não precisam ser calculados explicitamente ao fornecer *logits* como entradas.\n",
    "\n",
    "O código a seguir mostrará como usar essas três funções de perda com dois formatos diferentes, onde as probabilidades de *logits* ou de associação de classe são fornecidas como entradas para as funções de perda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE (w Probas): 0.3711 (w Logits): 0.3711\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "####### Binary Crossentropy\n",
    "bce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "logits = tf.constant([0.8])\n",
    "probas = tf.keras.activations.sigmoid(logits)\n",
    "\n",
    "tf.print(\n",
    "    'BCE (w Probas): {:.4f}'.format(\n",
    "        bce_probas(y_true=[1], y_pred=probas)),\n",
    "        '(w Logits): {:.4f}'.format(\n",
    "            bce_logits(y_true=[1], y_pred=logits)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE (w Probas): 0.5996 (w Logits): 0.5996\n"
     ]
    }
   ],
   "source": [
    "####### Categorical Crossentropy\n",
    "cce_probas = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False)\n",
    "cce_logits = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "logits = tf.constant([[1.5, 0.8, 2.1]])\n",
    "probas = tf.keras.activations.softmax(logits)\n",
    "\n",
    "tf.print(\n",
    "    'CCE (w Probas): {:.4f}'.format(\n",
    "        cce_probas(y_true=[[0, 0, 1]], y_pred=probas)),\n",
    "            '(w Logits): {:.4f}'.format(\n",
    "                    cce_logits(y_true=[[0, 0, 1]], y_pred=logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse CCE (w Probas): 0.5996 (w Logits): 0.5996\n"
     ]
    }
   ],
   "source": [
    "####### Sparse Categorical Crossentropy\n",
    "sp_cce_probas = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False)\n",
    "sp_cce_logits = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "tf.print(\n",
    "    'Sparse CCE (w Probas): {:.4f}'.format(\n",
    "    sp_cce_probas(y_true=[[2]], y_pred=probas)),\n",
    "    '(w Logits): {:.4f}'.format(\n",
    "    sp_cce_logits(y_true=[[2]], y_pred=logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, às vezes, você pode encontrar uma implementação em que uma perda categórica de entropia cruzada é usada para classificação binária. Normalmente, quando temos uma tarefa de classificação binária, o modelo retorna um único valor de saída para cada exemplo. Interpretamos esta saída de modelo único como a probabilidade da classe positiva (por exemplo, *classe 1*), $\\small P[classe = 1]$. Em um problema de classificação binária, está implícito que $\\small P[class = 0] = 1 – P[class = 1]$; portanto, não precisamos de uma segunda unidade de saída para obter a probabilidade da classe negativa. No entanto, às vezes os praticantes optam por retornar duas saídas para cada exemplo de treinamento e interpretá-las como probabilidades de cada classe: $\\small P[class = 0]\\: versus \\: P[class = 1]$. Então, nesse caso, é recomendado usar uma função *softmax* (em vez da sigmóide logística) para normalizar as saídas (que somam 1), e a entropia cruzada categórica é a função de perda apropriada.\n",
    "\n",
    "### Implementando uma CNN profunda usando o *TensorFlow*\n",
    "\n",
    "Você deve se lembrar de que usamos os estimadores do *TensorFlow* para problemas de reconhecimento de dígitos manuscritos, usando diferentes níveis de API do *TensorFlow*. Você também deve se lembrar de que alcançamos cerca de 89% de precisão usando o `DNNClassifier Estimator` com duas camadas ocultas.\n",
    "\n",
    "Agora, vamos implementar uma CNN e ver se ela pode obter um melhor desempenho preditivo em comparação com o *MLP (DNNClassifier)* ​​para classificar dígitos manuscritos. Observe que as camadas totalmente conectadas tiveram um bom desempenho nesse problema. No entanto, em algumas aplicações, como a leitura de números de contas bancárias a partir de dígitos manuscritos, até mesmo pequenos erros podem ser muito caros. Portanto, é crucial reduzir esse erro o máximo possível.\n",
    "\n",
    "### A arquitetura CNN multicamada\n",
    "\n",
    "A arquitetura da rede que vamos implementar é mostrada na figura a seguir. As entradas são $\\small 28 × 28$ imagens em tons de cinza. Considerando o número de canais (que é 1 para imagens em tons de cinza) e um lote de imagens de entrada, as dimensões do tensor de entrada serão $\\small batchsize \\times 28 \\times 28 \\times 1$.\n",
    "\n",
    "Os dados de entrada passam por duas camadas convolucionais que possuem um tamanho de kernel de $\\small 5 × 5$. A primeira convolução tem 32 mapas de *features* de saída e a segunda tem 64 mapas de *features* de saída. Cada camada de convolução é seguida por uma camada de subamostragem na forma de uma operação de agrupamento máximo, $\\small P_{2\\times2}$. Em seguida, uma camada totalmente conectada passa a saída para uma segunda camada totalmente conectada, que atua como a camada final de saída do *softmax*. A arquitetura da rede que vamos implementar é mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\arquitetura.PNG)\n",
    "\n",
    "As dimensões dos tensores em cada camada são as seguintes:\n",
    "* Input: [$\\small batchsize \\times 28 \\times 28 \\times 1$]\n",
    "* Conv_1: [$\\small batchsize \\times 28 \\times 28 \\times 32$]\n",
    "* Pooling_1: [$\\small batchsize \\times 14 \\times 14 \\times 32$]\n",
    "* Conv_2: [$\\small batchsize \\times 14 \\times 14 \\times 64$]\n",
    "* Pooling_2: [$\\small batchsize \\times 7 \\times 7 \\times 64$]\n",
    "* FC_1: [$\\small batchsize \\times 1024$]\n",
    "* Camada FC_2 e softmax: [$\\small batchsize \\times 10$]\n",
    "\n",
    "Para os kernels convolucionais, estamos usando `strides=1` de modo que as dimensões de entrada sejam preservadas nos mapas de *features* resultantes. Para as camadas de agrupamento, estamos usando `strides=2` para subamostrar a imagem e reduzir o tamanho dos mapas de *features* de saída. Implementaremos essa rede usando a API TensorFlow Keras.\n",
    "\n",
    "### Carregando e pré-processando os dados\n",
    "Você aprendeu duas maneiras de carregar conjuntos de dados disponíveis no módulo `tensorflow_datasets`. Uma abordagem é baseada em um processo de três etapas, e um método mais simples usa uma função chamada `load`, que envolve essas três etapas. Aqui, usaremos o primeiro método. As três etapas para carregar o conjunto de dados MNIST são as seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "## Loading the data\n",
    "mnist_bldr = tfds.builder('mnist')\n",
    "mnist_bldr.download_and_prepare()\n",
    "datasets = mnist_bldr.as_dataset(shuffle_files=False)\n",
    "mnist_train_orig = datasets['train']\n",
    "mnist_test_orig = datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados MNIST vem com um esquema de particionamento de conjunto de dados de treinamento e teste pré-especificado, mas também queremos criar uma divisão de validação da partição de treino. Observe que na terceira etapa, usamos um argumento opcional, `shuffle_files=False`, no método `.as_dataset()`. Isso evitou o embaralhamento inicial, o que é necessário para nós, pois queremos dividir o conjunto de dados de treinamento em duas partes: um conjunto de dados de treinamento menor e um conjunto de dados de validação. (Observação: se o embaralhamento inicial não foi desativado, incorreria em reordenamento do conjunto de dados toda vez que buscássemos um minilote de dados. Se fizermos isso, podemos causar uma falsa estimativa de desempenho do modelo, já que os conjuntos de dados de treinamento/validação são de fato misturados).\n",
    "\n",
    "Podemos dividir os conjuntos de dados de treinamento/validação da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "mnist_train = mnist_train_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "mnist_test = mnist_test_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "mnist_train = mnist_train.shuffle(buffer_size=BUFFER_SIZE,\n",
    "                                  reshuffle_each_iteration=False)\n",
    "\n",
    "mnist_valid = mnist_train.take(10000).batch(BATCH_SIZE)\n",
    "mnist_train = mnist_train.skip(10000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, depois de preparar o conjunto de dados, estamos prontos para implementar a CNN que acabamos de descrever.\n",
    "\n",
    "### Implementando uma CNN usando a API TensorFlow Keras\n",
    "Para implementar uma CNN no *TensorFlow*, usamos a classe Keras `Sequential` para empilhar diferentes camadas, como convolução, *pooling* e *dropout*, bem como as camadas totalmente conectadas (densas). A API de camadas Keras fornece classes para cada uma: `tf.keras.layers.Conv2D` para uma camada de convolução bidimensional; `tf.keras.layers.MaxPool2D` e `tf.keras.layers.AvgPool2D` para subamostragem (*max-pooling* e *average-pooling*); e `tf.keras.layers.Dropout` para regularização usando *dropout*. Veremos cada uma dessas classes com mais detalhes.\n",
    "\n",
    "### Configurando camadas CNN em Keras\n",
    "Construir uma camada com a classe `Conv2D` exige que especifiquemos o número de filtros de saída (que é equivalente ao número de mapas de recursos de saída) e os tamanhos do kernel.\n",
    "\n",
    "Além disso, existem parâmetros opcionais que podemos usar para configurar uma camada convolucional. Os mais usados ​​são os *strides* (com um valor padrão de 1 em ambas as dimensões $\\small x, y$) e *padding*, que podem ser *same* ou *valid*. Parâmetros de configuração adicionais estão listados na documentação oficial: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D.\n",
    "\n",
    "Vale ressaltar que geralmente, quando lemos uma imagem, a dimensão padrão para os canais é a última dimensão do *array* de tensores. Isso é chamado de formato **\"NHWC\"**, onde **N** representa o número de imagens dentro do lote, **H** e **W** representam altura e largura, respectivamente, e **C** representa canais. \n",
    "\n",
    "Observe que a classe `Conv2D` assume que as entradas estão no formato NHWC por padrão. (Outras ferramentas, como *PyTorch*, usam um formato *NCHW*.) No entanto, se você encontrar alguns dados cujos canais são colocados na primeira dimensão (a primeira dimensão após a dimensão do lote ou a segunda dimensão considerando a dimensão do lote), você deve precisa trocar os eixos em seus dados para mover os canais para a última dimensão. Ou, uma maneira alternativa de trabalhar com uma entrada formatada em NCHW é definir `data_format=\"channels_first\"`. Depois que a camada é construída, ela pode ser chamada fornecendo um tensor de quatro dimensões, com a primeira dimensão reservada para um lote de exemplos; dependendo do argumento `data_format`, a segunda ou a quarta dimensão corresponde ao canal; e as outras duas dimensões são as dimensões espaciais.\n",
    "\n",
    "Conforme mostrado na arquitetura do modelo CNN que queremos construir, cada camada de convolução é seguida por uma camada de agrupamento para subamostragem (reduzindo o tamanho dos mapas de features). As classes `MaxPool2D` e `AvgPool2D` constroem as camadas *max-pooling* e *average-pooling*, respectivamente. O argumento *pool_size* determina o tamanho da janela (ou vizinhança) que será usada para computar as operações de *max* ou *mean*. Além disso, o parâmetro `strides` pode ser usado para configurar a camada de *pooling*, conforme discutimos anteriormente.\n",
    "\n",
    "Por fim, a classe `Dropout` construirá a camada *dropout* para regularização, com a taxa de argumentos usada para determinar a probabilidade de descartar as unidades de entrada durante o treinamento. Ao chamar esta camada, seu comportamento pode ser controlado por meio de um argumento denominado `training`, para especificar se esta chamada será feita durante o treinamento ou durante a inferência.\n",
    "\n",
    "### Construindo uma CNN em Keras\n",
    "Agora que você aprendeu sobre essas classes, podemos construir o modelo CNN mostrado na figura anterior. No código a seguir, usaremos a classe `Sequential` e adicionaremos as camadas de convolução e *pooling*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(5, 5),\n",
    "    strides=(1, 1), padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='conv_1', activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), name='pool_1'))\n",
    "    \n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(5, 5),\n",
    "    strides=(1, 1), padding='same',\n",
    "    name='conv_2', activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora, adicionamos duas camadas de convolução ao modelo. Para cada camada convolucional, usamos um kernel de tamanho $\\small 5 × 5$ e *padding* `same`. Como discutido anteriormente, o uso de `padding='same'` preserva as dimensões espaciais (dimensões verticais e horizontais) dos mapas de *features* de forma que as entradas e saídas tenham a mesma altura e largura (e o número de canais pode diferir apenas em termos do número de filtros utilizados). As camadas de *max-pooling* com tamanho de *pooling* 2 × 2 e *stride* de 2 reduzirão as dimensões espaciais pela metade. (Observe que, se o parâmetro `strides` não for especificado em `MaxPool2D`, por padrão, ele será definido igual ao tamanho do *pooling*.)\n",
    "\n",
    "Embora possamos calcular o tamanho dos mapas de *features* neste estágio manualmente, a API Keras fornece um método conveniente para calcular isso para nós:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 7, 7, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_output_shape(input_shape=(16, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao fornecer a forma de entrada como uma tupla especificada neste exemplo, o método `compute_output_shape` calculou a saída para ter uma forma (16, 7, 7, 64), indicando mapas de *features* com 64 canais e um tamanho espacial de 7 × 7. A primeira dimensão corresponde à dimensão do lote, para a qual usamos 16 arbitrariamente. Poderíamos ter usado `None` em vez disso, ou seja, input_shape=(None, 28, 28, 1).\n",
    "\n",
    "A próxima camada que queremos adicionar é uma camada densa (ou totalmente conectada) para implementar um classificador no topo de nossas camadas convolucionais e de pooling. A entrada para esta camada deve ter rank $\\small 2$, ou seja, forma $\\small [batchsize \\times input\\:units]$. Assim, precisamos achatar a saída das camadas anteriores para atender a esse requisito para a camada densa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 3136])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.compute_output_shape(input_shape=(16, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o resultado de `compute_output_shape` indica, as dimensões de entrada para a camada densa estão configuradas corretamente. Em seguida, adicionaremos duas camadas densas com uma camada dropout no meio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(\n",
    "    units=1024, name='fc_1',\n",
    "    activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate=0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=10, name='fc_2',\n",
    "    activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última camada totalmente conectada, denominada `fc_2`, possui 10 unidades de saída para os 10 rótulos de classe no conjunto de dados MNIST. Além disso, usamos a ativação *softmax* para obter as probabilidades de associação de classe de cada exemplo de entrada, assumindo que as classes são mutuamente exclusivas, de modo que as probabilidades de cada exemplo somam 1. (Isso significa que um exemplo de treinamento pode pertencer a apenas uma classe).\n",
    "\n",
    "Com base no que discutimos, qual função de perda devemos usar aqui? Lembre-se de que para uma classificação multiclasse com rótulos inteiros (esparsos) (em oposição a rótulos codificados one-hot), usamos `SparseCategoricalCrossentropy`. O código a seguir chamará o método `build()` para a criação tardia da variável e compilará o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### O otimizador Adam\n",
    "> Observe que nesta implementação, usamos o `tf.keras.optimizers.Adam()` para treinar o modelo CNN. O otimizador *Adam* é um método de otimização robusto e baseado em gradiente adequado para otimização não convexa e problemas de aprendizado de máquina. Dois métodos populares de otimização inspiraram *Adam*: `RMSProp` e `AdaGrad`.\n",
    "> A principal vantagem do *Adam* está na escolha do tamanho do passo de atualização derivado da média de execução dos momentos de gradiente.\n",
    "\n",
    "Como você já sabe, podemos treinar o modelo chamando o método `fit()`. Observe que usar os métodos designados para treinamento e avaliação (como `evaluate()` e `predict()`) definirá automaticamente o modo para a camada *dropout* e redimensionará as unidades ocultas adequadamente para que não precisemos nos preocupar com isso. Em seguida, treinaremos esse modelo CNN e usaremos o conjunto de dados de validação que criamos para monitorar o progresso do aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 52s 65ms/step - loss: 0.1372 - accuracy: 0.9570 - val_loss: 0.0466 - val_accuracy: 0.9861\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.0453 - accuracy: 0.9857 - val_loss: 0.0389 - val_accuracy: 0.9880\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.0412 - val_accuracy: 0.9876\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.0354 - val_accuracy: 0.9903\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.0479 - val_accuracy: 0.9878\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0406 - val_accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0448 - val_accuracy: 0.9902\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0476 - val_accuracy: 0.9894\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.0750 - val_accuracy: 0.9876\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0403 - val_accuracy: 0.9916\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.0551 - val_accuracy: 0.9907\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0472 - val_accuracy: 0.9915\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0456 - val_accuracy: 0.9911\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0500 - val_accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0551 - val_accuracy: 0.9901\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0546 - val_accuracy: 0.9900\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0651 - val_accuracy: 0.9902\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0664 - val_accuracy: 0.9899\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.0632 - val_accuracy: 0.9906\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0753 - val_accuracy: 0.9895\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(mnist_train, epochs=NUM_EPOCHS,\n",
    "                        validation_data=mnist_valid,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminadas as 20 épocas de treinamento, podemos visualizar as curvas de aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "hist = history.history\n",
    "x_arr = np.arange(len(hist['loss'])) + 1\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist['val_accuracy'], '--<',label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você já sabe, avaliar o modelo treinado no conjunto de dados de teste pode ser feito chamando o método .evaluate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 3s 5ms/step - loss: 0.0549 - accuracy: 0.9927\n",
      "Test Acc.: 99.27%\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(mnist_test.batch(20))\n",
    "print(f\"Test Acc.: {test_results[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo CNN atinge uma precisão de **99,27%**. Em outra oportunidade, obtivemos aproximadamente 90% de precisão usando o Estimator `DNNClassifier`.\n",
    "\n",
    "Finalmente, podemos obter os resultados da previsão na forma de probabilidades de pertinência de classe e convertê-los em rótulos previstos usando a função `tf.argmax` para encontrar o elemento com a probabilidade máxima. Faremos isso para um lote de 12 exemplos e visualizaremos os rótulos de entrada e previstos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([12, 10])\n",
      "tf.Tensor([2 0 4 8 7 6 0 6 3 1 8 0], shape=(12,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_test = next(iter(mnist_test.batch(12)))\n",
    "\n",
    "preds = model(batch_test[0])\n",
    "\n",
    "tf.print(preds.shape)\n",
    "preds = tf.argmax(preds, axis=1)\n",
    "print(preds)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(2, 6, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    img = batch_test[0][i, :, :, 0]\n",
    "    ax.imshow(img, cmap='gray_r')\n",
    "    ax.text(0.9, 0.1, '{}'.format(preds[i]), \n",
    "            size=15, color='blue',\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='center', \n",
    "            transform=ax.transAxes)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação de gênero a partir de imagens de rosto usando uma CNN\n",
    "\n",
    "Nesta seção, vamos implementar uma CNN para classificação de gênero a partir de imagens de rosto usando o conjunto de dados **CelebA**. O conjunto de dados CelebA contém 202.599 imagens de rostos de celebridades. Além disso, 40 atributos faciais binários estão disponíveis para cada imagem, incluindo sexo (masculino ou feminino) e idade (jovem ou velho).\n",
    "\n",
    "Com base no que você aprendeu até agora, o objetivo desta seção é construir e treinar um modelo CNN para prever o atributo de gênero dessas imagens de rosto. Aqui, para simplificar, usaremos apenas uma pequena parte dos dados de treinamento (16.000 exemplos de treinamento) para acelerar o processo de treinamento. No entanto, para melhorar o desempenho da generalização e reduzir o *overfitting* em um conjunto de dados tão pequeno, usaremos uma técnica chamada aumento de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura acima mostra as entradas manuscritas e seus rótulos previstos. No caso desse conjunto de exemplos plotados, todos os rótulos previstos estão corretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, ao invés de usar todos os dados de treinamento e validação disponíveis, vamos pegar um subconjunto de 16.000 exemplos de treinamento e 1.000 exemplos para validação, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de imagem e aumento de dados\n",
    "O aumento de dados resume um amplo conjunto de técnicas para lidar com casos em que os dados de treinamento são limitados. Por exemplo, certas técnicas de aumento de dados nos permitem modificar ou até mesmo sintetizar artificialmente mais dados e, assim, aumentar o desempenho de uma máquina ou modelo de aprendizado profundo, reduzindo o *overfitting*.\n",
    "\n",
    "Embora o aumento de dados não seja apenas para dados de imagem, há um conjunto de transformações aplicáveis ​​exclusivamente a dados de imagem, como cortar partes de uma imagem, inverter, alterar o contraste, o brilho e a saturação. Vamos ver algumas dessas transformações que estão disponíveis através do módulo `tf.image`.\n",
    "\n",
    "No bloco de código a seguir, primeiro obteremos cinco exemplos do conjunto de dados `celeba_train` e aplicaremos cinco tipos diferentes de transformação:\n",
    "\n",
    "1. Cortar uma imagem em uma caixa delimitadora;\n",
    "2. Inverter uma imagem horizontalmente;\n",
    "3. Ajustar o contraste;\n",
    "4. Ajustar o brilho; e \n",
    "5. Recorte centralizado de uma imagem e redimensione a imagem resultante de volta ao seu tamanho original (218, 178).\n",
    "\n",
    "No código a seguir, visualizaremos os resultados dessas transformações, mostrando cada uma em uma coluna separada para comparação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1870f1194fb5b3d43b6d32e845741389586dbe9c4e1e45e17e0f6602cfe22778"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
