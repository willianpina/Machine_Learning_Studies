{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelagem de dados sequenciais usando redes neurais recorrentes</h1>\n",
    "\n",
    "<p align=center><img src=https://datascience.eu/wp-content/uploads/2020/05/image-513-1024x347.png></p>\n",
    "\n",
    "Tivemos oportunidade de focar em redes neurais convolucionais (*CNNs*), de forma a cobrir os blocos de construção das arquiteturas *CNN* e como implementar *CNNs* profundas no *TensorFlow*. Por fim, você aprendeu a usar *CNNs* para classificação de imagens.\n",
    "\n",
    "Aqui, exploraremos as redes neurais recorrentes (*RNNs*) e veremos sua aplicação na modelagem de dados sequenciais.\n",
    "\n",
    "### Apresentando dados sequenciais\n",
    "Vamos começar nossa discussão sobre *RNNs* observando a natureza dos dados sequenciais, que são mais comumente conhecidos como dados de sequência ou **sequências**. Vamos dar uma olhada nas propriedades únicas das sequências que as tornam diferentes de outros tipos de dados. Veremos então como podemos representar dados sequenciais e explorar as várias categorias de modelos para dados sequenciais, que são baseados na entrada e saída de um modelo. Isso nos ajudará a explorar a relação entre *RNNs* e sequências.\n",
    "\n",
    "### Modelagem de dados sequenciais - a ordem importa\n",
    "\n",
    "O que torna as sequências únicas, em comparação com outros tipos de dados, é que os elementos em uma sequência aparecem em uma determinada ordem e não são independentes uns dos outros. Algoritmos típicos de aprendizado de máquina para aprendizado supervisionado pressupõem que a entrada de dados é **independente e e distribuída de forma idêntica (IID)**, o que significa que os exemplos de treinamento são `mutuamente independentes` e têm a mesma distribuição subjacente.\n",
    "\n",
    "Nesse sentido, com base na suposição de independência mútua, a ordem em que os exemplos de treinamento são dados ao modelo é **irrelevante**. Por exemplo, se tivermos uma amostra composta por n exemplos de treinamento, $\\small x^{(1)}, x^{(2)},\\cdots, x^{(n)} $, a ordem em que usamos os dados para treinar nosso algoritmo de aprendizado de máquina não importa. Um exemplo desse cenário seria o conjunto de dados Iris, muito conhecido. No conjunto de dados Iris, cada flor foi medida independentemente e as medidas de uma flor não influenciam as medidas de outra flor.\n",
    "\n",
    "No entanto, essa suposição não é válida quando lidamos com sequências – por definição, **a ordem é importante**. Prever o valor de mercado de uma determinada ação seria um exemplo desse cenário. Por exemplo, suponha que temos uma amostra de n exemplos de treinamento, onde cada exemplo de treinamento representa o valor de mercado de uma determinada ação em um determinado dia. Se nossa tarefa é prever o valor do mercado de ações para os próximos três dias, faria sentido considerar os preços das ações anteriores em uma ordem de data para derivar tendências, em vez de utilizar esses exemplos de treinamento em uma ordem aleatória.\n",
    "\n",
    "> #### Dados sequenciais versus dados de séries temporais\n",
    "> Os dados de série temporal são um tipo especial de dados sequenciais, em que cada exemplo está associado a uma dimensão de tempo. Em dados de séries temporais, as amostras são coletadas em *timestamps* sucessivos e, portanto, a dimensão de tempo determina a ordem entre os pontos de dados. Por exemplo, preços de ações e registros de voz ou fala são dados de séries temporais.\n",
    ">\n",
    "> Por outro lado, nem todos os dados sequenciais têm a dimensão temporal, por exemplo, dados de texto ou sequências de DNA, onde os exemplos são ordenados, mas não se qualificam como dados de séries temporais. Como você verá, abordaremos alguns exemplos de Processamento de Linguagem Natural (NLP) e modelagem de texto que não são dados de série temporal, mas observe que as *RNNs* também podem ser usados para dados de série temporal.\n",
    "\n",
    "### Representando sequências\n",
    "\n",
    "Estabelecemos que a ordem entre os pontos de dados é importante em dados sequenciais, então precisamos encontrar uma maneira de aproveitar essas informações de pedido em um modelo de aprendizado de máquina. Ao longo das explicações, representaremos as sequências como $\\small \\left \\langle x^{(1)},x^{(2)},\\cdots, x^{(T)}  \\right \\rangle$ . Os índices sobrescritos indicam a ordem das instâncias e o comprimento da sequência é $\\small T$. Para um exemplo sensato de sequências, considere dados de séries temporais, onde cada ponto de exemplo, $\\small x^{(T)}$, pertence a um determinado tempo, $\\small t$. A figura a seguir mostra um exemplo de dados de série temporal em que os recursos de entrada ($\\small x$) e os rótulos de destino ($\\small y$) seguem naturalmente a ordem de acordo com seu eixo de tempo; portanto, ambos os `x` e `y` são sequências:\n",
    "\n",
    "![](imagens\\sequencias.PNG)\n",
    "\n",
    "Como já mencionamos, os modelos de rede neural padrão (*RN*) que abordamos até agora, como o *multilayer perceptron* (MLP) e as *CNNs* para dados de imagem, assumem que os exemplos de treinamento são independentes uns dos outros e, portanto, não incorporam **informação de ordenamento**. Podemos dizer que tais modelos não possuem **memória** de exemplos de treinamento vistos anteriormente. Por exemplo, as amostras são passadas pelas etapas de *feedforward* e *backpropagation* e os pesos são atualizados independentemente da ordem em que os exemplos de treinamento são processados. As *RNNs*, por outro lado, são projetadas para modelar sequências e são capazes de lembrar informações passadas e processar novos eventos de acordo, o que é uma clara vantagem ao trabalhar com dados de sequência.\n",
    "\n",
    "### As diferentes categorias de modelagem de sequência\n",
    "\n",
    "A modelagem de sequência tem muitas aplicações fascinantes, como tradução de idiomas (por exemplo, tradução de texto de inglês para alemão), legendas de imagens e geração de texto. No entanto, para escolher uma arquitetura e abordagem apropriadas, temos que entender e ser capazes de distinguir entre essas diferentes tarefas de modelagem de sequência. A figura a seguir, baseada nas explicações do excelente artigo The Unreasonable Effectiveness of Recurrent Neural Networks, de Andrej Karpathy (http://karpathy.github.io/2015/05/21/rnn-effectiveness/), resume a sequência mais comum tarefas de modelagem, que dependem das categorias de relacionamento de dados de entrada e saída:\n",
    "\n",
    "![](imagens\\modelagem_de_sequencia.PNG)\n",
    "\n",
    "Vamos discutir as diferentes categorias de relacionamento entre dados de entrada e saída, que foram descritas na figura anterior, com mais detalhes. Se nem os dados de entrada nem de saída representam sequências, então estamos lidando com dados padrão e podemos simplesmente usar um perceptron multicamada (ou outro modelo de classificação abordado anteriormente) para modelar esses dados. No entanto, se a entrada ou a saída for uma sequência, a tarefa de modelagem provavelmente se enquadra em uma destas categorias:\n",
    "* **Muitos para um**: Os dados de entrada são uma sequência, mas a saída é um vetor de tamanho fixo ou escalar, não uma sequência. Por exemplo, na análise de sentimentos, a entrada é baseada em texto (por exemplo, uma resenha de filme) e a saída é um rótulo de classe (por exemplo, um rótulo que indica se um revisor gostou do filme).\n",
    "\n",
    "* **Um para muitos**: Os dados de entrada estão no formato padrão e não em sequência, mas a saída é uma sequência. Um exemplo dessa categoria é a legendagem de imagens — a entrada é uma imagem e a saída é uma frase em inglês que resume o conteúdo dessa imagem.\n",
    "\n",
    "* **Muitos para muitos**: As matrizes de entrada e saída são sequências. Esta categoria pode ser dividida com base na sincronização da entrada e saída. Um exemplo de uma tarefa de modelagem sincronizada de muitos para muitos é a classificação de vídeo, onde cada quadro em um vídeo é rotulado. Um exemplo de uma tarefa de modelagem muitos-para-muitos *atrasada* seria traduzir uma linguagem para outra. Por exemplo, uma frase inteira em inglês deve ser lida e processada por uma máquina antes que sua tradução para o alemão seja produzida. \n",
    "\n",
    "Agora, depois de resumir as três grandes categorias de modelagem de sequência, podemos avançar para discutir a estrutura de uma RNN.\n",
    "\n",
    "### RNNs para modelagem de sequências \n",
    "Nesta seção, antes de começarmos a implementar *RNNs* no *TensorFlow*, discutiremos os principais conceitos de *RNNs*. Começaremos examinando a estrutura típica de uma *RNN*, que inclui um componente recursivo para modelar dados de sequência. Em seguida, examinaremos como as ativações dos neurônios são computadas em uma *RNN* típica. Isso criará um contexto para discutirmos os desafios comuns no treinamento de *RNNs* e, em seguida, discutiremos soluções para esses desafios, como *LSTM* e unidades recorrentes fechadas (*GRUs*).\n",
    "\n",
    "### Entendendo o mecanismo de loop *RNN*\n",
    "Vamos começar com a arquitetura de uma *RNN*. A figura a seguir mostra uma *RN* *feedforward* padrão e um *RNN* lado a lado para comparação:\n",
    "\n",
    "![](imagens\\mecanismo_loop_rnn.PNG)\n",
    "\n",
    "Ambas as redes têm apenas uma camada oculta. Nesta representação, as unidades não são exibidas, mas assumimos que a camada de entrada ($\\small x$), a camada oculta ($\\small h$) e a camada de saída ($\\small o$) são vetores que contêm muitas unidades.\n",
    "\n",
    "> ##### Determinando o tipo de saída de um RNN\n",
    "> Essa arquitetura RNN genérica pode corresponder às duas categorias de modelagem de sequência em que a entrada é uma sequência. Normalmente, uma camada recorrente pode retornar uma sequência como saída, $\\small \\left \\langle o^{(1)},o^{(2)},\\cdots, o^{(T)}  \\right \\rangle$, ou simplesmente retornar a última saída (em $\\small t = T$, ou seja, $\\small o^{(T)}$). Assim, pode ser muitos para muitos ou muitos para um se, por exemplo, usarmos apenas o último elemento, $\\small o^{(T)}$, como a saída final.\n",
    ">\n",
    "> Como você verá mais tarde, na *API TensorFlow Keras*, o comportamento de uma camada recorrente em relação ao retorno de uma sequência como saída ou simplesmente usar a última saída pode ser especificado definindo o argumento `return_sequences` como `True` ou `False`, respectivamente.\n",
    "\n",
    "Em uma rede *feedforward* padrão, as informações fluem da entrada para a camada oculta e, em seguida, da camada oculta para a camada de saída. Por outro lado, em uma *RNN*, a camada oculta recebe sua entrada tanto da camada de entrada da etapa de tempo atual quanto da camada oculta da etapa de tempo anterior.\n",
    "\n",
    "O fluxo de informações em etapas de tempo adjacentes na camada oculta permite que a rede tenha uma memória de eventos passados. Esse fluxo de informações geralmente é exibido como um *loop*, também conhecido como **recurrent edge** (borda recorrente) em notação de gráfico, que é como essa arquitetura *RNN* geral recebeu seu nome.\n",
    "\n",
    "Semelhante aos *perceptrons* multicamadas, as *RNN*s podem consistir em várias camadas ocultas. Observe que é uma convenção comum se referir às *RNNs* com uma camada oculta como uma *RNN* de camada única, que não deve ser confundido com as RNs de camada única sem uma camada oculta, como *Adaline* ou *regressão logística*. A figura a seguir ilustra uma *RNN* com uma camada oculta (superior) e uma *RNN* com duas camadas ocultas (inferior):\n",
    "\n",
    "![](imagens\\rnn_oculta.PNG)\n",
    "\n",
    "Para examinar a arquitetura das *RNNs* e o fluxo de informações, pode-se desdobrar uma representação compacta com uma aresta recorrente, que você pode ver na figura anterior.\n",
    "\n",
    "Como sabemos, cada unidade oculta em uma *RN* padrão recebe apenas uma entrada – a pré-ativação de rede associada à camada de entrada. Em contraste, cada unidade oculta em uma *RNN* recebe dois conjuntos distintos de entrada – a pré-ativação da camada de entrada e a ativação da mesma camada oculta da etapa de tempo anterior, $\\small t – 1$.\n",
    "\n",
    "Na primeira etapa de tempo, $\\small t = 0$, as unidades ocultas são inicializadas com zeros ou pequenos valores aleatórios. Então, em um passo de tempo em que $\\small t > 0$, as unidades ocultas recebem sua entrada do ponto de dados no momento atual, $\\small x^{(T)}$, e os valores anteriores das unidades ocultas em $\\small t – 1$, indicados como $\\small h^{(t-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, no caso de uma RNN multicamada, podemos resumir o fluxo de informações da seguinte forma:\n",
    "* $\\small layer$ = 1: Aqui, a camada oculta é representada como $h_1^{(t)}$ e recebe sua entrada do ponto de dados, $x^{(t)}$, e os valores ocultos na mesma camada, mas no passo de tempo anterior, $h_1^{(t - 1)}$.\n",
    "* $\\small layer$ = 2: A segunda camada oculta, $h_2^{(t)}$ , recebe suas entradas das saídas da camada abaixo na etapa de tempo atual $(o_1^{(t)})$ e seus próprios valores ocultos da etapa de tempo anterior, $h_2^{(t-1)}$.\n",
    "\n",
    "Como, neste caso, cada camada recorrente deve receber uma sequência como entrada, todas as camadas recorrentes, exceto a última, devem retornar uma sequência como saída (ou seja, `return_sequences=True`). O comportamento da última camada recorrente depende do tipo de problema.\n",
    "\n",
    "### Computando ativações em uma *RNN*\n",
    "Agora que você entende a estrutura e o fluxo geral de informações em uma *RNN*, vamos ser mais específicos e calcular as ativações reais das camadas ocultas, bem como a camada de saída. Para simplificar, consideraremos apenas uma única camada oculta; no entanto, o mesmo conceito se aplica às *RNNs* multicamadas.\n",
    "\n",
    "Cada aresta direcionada (as conexões entre caixas) na representação de uma *RNN* que acabamos de ver está associada a uma matriz de pesos. Esses pesos não dependem do tempo, $\\small t$; portanto, eles são compartilhados ao longo do eixo do tempo. As diferentes matrizes de peso em uma *RNN* de camada única são as seguintes:\n",
    "* $\\small \\textbf{W}_{xh}$: A matriz de peso entre a entrada, $\\small x^{(t)}$, e a camada oculta, $\\small \\textbf{h}$\n",
    "* $\\small \\textbf{W}_{hh}$: A matriz de peso associada à aresta recorrente\n",
    "* $\\small \\textbf{W}_{ho}$: A matriz de peso entre a camada oculta e a camada de saída\n",
    "\n",
    "Essas matrizes de peso são representadas na figura a seguir:\n",
    "\n",
    "![](imagens\\matriz_pesos.PNG)\n",
    "\n",
    "Em certas implementações, você pode observar que as matrizes de peso, $\\small \\textbf{W}_{xh}$ e $\\small \\textbf{W}_{hh}$, são concatenadas a uma matriz combinada, $\\small \\textbf{W}_{h} = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$. Mais adiante, faremos uso dessa notação também.\n",
    "\n",
    "A computação das ativações é muito semelhante aos *perceptrons* multicamadas padrão e outros tipos de *RNs* *feedforward*. Para a camada oculta, a entrada líquida, $\\small \\textbf{z}_ h$ (pré-ativação), é calculada através de uma combinação linear, ou seja, calculamos a soma das multiplicações das matrizes de peso com os vetores correspondentes e somamos a unidade de polarização:\n",
    "$$\n",
    "\\small \\textbf{z}_h^{(t)} = \\text{W}_{xh} \\textbf{x}^{(t)} + \\textbf{W}_{hh}\\textbf{h}^{(t-1)} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "Então, as ativações das unidades ocultas na etapa do tempo, $\\small t$, são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^{(t)} = \\phi_h (\\textbf{z}_h^{(t)}) = \\phi_h (\\textbf{W}_{xh}\\textbf{x}^{(t)} +  \\textbf{W}_{hh}\\textbf{x}^{(t-1)} + \\textbf{b}_h)\n",
    "$$\n",
    " \n",
    "Aqui, $\\small \\textbf{b}_h$ é o vetor de polarização para as unidades ocultas e $\\phi_h(\\cdot)$ é a função de ativação da camada oculta.\n",
    "\n",
    "Caso você queira usar a matriz de peso concatenada, $\\small \\textbf{W}_h = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$, a fórmula para calcular as unidades ocultas mudará da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\phi_h \\left ([\\textbf{W}_{xh}; \\textbf{W}_{hh}] \\begin{bmatrix}\n",
    "\\textbf{x}^{(t)}\\\\ \n",
    "\\textbf{h}^{t-1}\n",
    "\\end{bmatrix}       \n",
    "+ \\textbf{b}_h          \\right )\n",
    "$$\n",
    "\n",
    "Uma vez computadas as ativações das unidades ocultas no passo de tempo atual, serão computadas as ativações das unidades de saída, como segue:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{o}^{(t)} = \\phi_0 (\\textbf{W}_{ho}\\textbf{h}^{(t)} + \\textbf{b}_0)\n",
    "$$\n",
    "\n",
    "Para ajudar a esclarecer ainda mais, a figura a seguir mostra o processo de cálculo dessas ativações com ambas as formulações:\n",
    "\n",
    "![](imagens\\matriz_sequencial.PNG)\n",
    "\n",
    "> ##### Treinando RNNs usando retropropagação ao longo do tempo (BPTT)\n",
    "> O algoritmo de aprendizado para RNNs foi introduzido em 1990: Backpropagation Through Time: What It Does and How to Do It (Paul Werbos, Proceedings of IEEE, 78(10): 1550-1560, 1990). A derivada dos gradientes pode ser um pouco complicada, mas a ideia básica é que a perda total, $\\small L$, é a soma de todas as funções de perda nos momentos $\\small t = 1$ a $\\small t = T$:\n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^{(t)}\n",
    "$$\n",
    "\n",
    "Como a perda no tempo $\\small t$ depende das unidades ocultas em todos os passos de tempo anteriores $\\small 1 : t$, o gradiente será calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{W}_{hh}} = \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{o}^{(t)}} \\times \\dfrac{\\partial \\textbf{o}^{(t)}}{\\partial \\textbf{h}^{(t)}} \\times \\left ( \\sum^t_{k=1}\\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}} \\times \\dfrac{\\partial \\textbf{h}^{(k)}}{\\partial \\textbf{W}_{hh}} \\right )\n",
    "\n",
    "$$\n",
    "\n",
    "Aqui, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ é calculado como uma multiplicação de passos de tempo adjacentes:\n",
    "$$\n",
    "\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{k}} = \\prod ^t_{i=k+1} \\dfrac{\\partial \\textbf{h}^{(i)}}{\\partial \\textbf{h}^{(i-1)}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrência oculta versus recorrência de saída\n",
    "Até agora, você viu redes recorrentes nas quais a camada oculta tem a propriedade recorrente. No entanto, observe que existe um modelo alternativo em que a conexão recorrente vem da camada de saída. Nesse caso, as ativações líquidas da camada de saída na etapa de tempo anterior, $\\small \\textbf{o}^{(t-1)}$, podem ser adicionadas de duas maneiras:\n",
    "* Para a camada oculta no passo de tempo atual, $\\small \\textbf{h}^t$ (mostrado na figura a seguir como recorrência de saída para oculta)\n",
    "* Para a camada de saída no passo de tempo atual, $\\small \\textbf{o}^{t}$ (mostrado na figura a seguir como recorrência de saída para saída)\n",
    "\n",
    "![](imagens\\recorrencia_saida.PNG)\n",
    "\n",
    "Conforme mostrado na figura anterior, as diferenças entre essas arquiteturas podem ser vistas claramente nas conexões recorrentes. Seguindo nossa notação, os pesos associados à conexão recorrente serão denotados para a recorrência oculta para oculta por $\\small \\textbf{W}_{hh}$, para a recorrência saída para oculta por $\\small \\textbf{W}_{oh}$, e para a recorrência saída para saída por $\\small \\textbf{W}_{oo}$. Em alguns artigos da literatura, os pesos associados às conexões recorrentes também são denotados por $\\small \\textbf{W}_{rec}$.\n",
    "\n",
    "Para ver como isso funciona na prática, vamos calcular manualmente a passagem direta para um desses tipos recorrentes. Usando a *API TensorFlow Keras*, uma camada recorrente pode ser definida por meio da *SimpleRNN*, que é semelhante à recorrência de saída para saída. No código a seguir, criaremos uma camada recorrente do *SimpleRNN* e executaremos uma passagem direta em uma sequência de entrada de comprimento 3 para calcular a saída. Também calcularemos manualmente a passagem direta e compararemos os resultados com os do *SimpleRNN*. Primeiro, vamos criar a camada e atribuir os pesos para nossos cálculos manuais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: (5, 2)\n",
      "W_oo shape: (2, 2)\n",
      "b_h shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=2, use_bias=True, \n",
    "    return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_oo shape:', w_oo.shape)\n",
    "print('b_h shape:', b_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma de entrada para esta camada é `(None, None, 5)`, onde a primeira dimensão é a dimensão do lote (usando `None` para tamanho de lote variável), a segunda dimensão corresponde à sequência (usando `None` para o comprimento variável da sequência) e a última dimensão corresponde às características. Observe que definimos `return_sequences=True`, que, para uma sequência de entrada de comprimento 3, resultará na sequência de saída $\\small \\left \\langle \\textbf{o}^{(0)},\\textbf{o}^{(1)}, \\textbf{o}^{(2)} \\right \\rangle$. Caso contrário, ele retornaria apenas a saída final, $\\textbf{o}^{(2)}$.\n",
    "\n",
    "Agora, vamos chamar a passagem direta no `rnn_layer` e calcular manualmente as saídas em cada passo de tempo e compará-las:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[0.41464037 0.96012145]]\n",
      "   Output (manual) : [[0.39240566 0.74433106]]\n",
      "   SimpleRNN output: [0.39240566 0.74433106]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[0.82928073 1.9202429 ]]\n",
      "   Output (manual) : [[0.80116504 0.99129474]]\n",
      "   SimpleRNN output: [0.80116504 0.99129474]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[1.243921  2.8803644]]\n",
      "   Output (manual) : [[0.95468265 0.9993069 ]]\n",
      "   SimpleRNN output: [0.95468265 0.9993069 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor(\n",
    "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "\n",
    "## output of SimepleRNN:\n",
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(len(x_seq)):\n",
    "    xt = tf.reshape(x_seq[t], (1, 5))\n",
    "    print('Time step {} =>'.format(t))\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h    \n",
    "    print('   Hidden          :', ht.numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_o = out_man[t-1]\n",
    "    else:\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "        \n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.numpy())\n",
    "    print('   SimpleRNN output:'.format(t), output[0][t].numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso cálculo direto manual, usamos a função de ativação da tangente hiperbólica (tanh), uma vez que também é usada na `SimpleRNN` (a ativação padrão). Como você pode ver nos resultados impressos, as saídas dos cálculos de encaminhamento manual correspondem exatamente à saída da camada `SimpleRNN` em cada etapa de tempo. Espero que esta tarefa prática tenha esclarecido você sobre os mistérios das redes recorrentes.\n",
    "\n",
    "### Os desafios de aprender interações de longo prazo\n",
    "\n",
    "BPTT, que foi brevemente mencionado anteriormente, apresenta alguns novos desafios. Por causa do fator multiplicativo,$\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$, ao calcular os gradientes de uma função de perda, surgem os chamados problemas de **gradiente de fuga e explosão**. Esses problemas são explicados pelos exemplos na figura a seguir, que mostra uma RNN com apenas uma unidade oculta para simplificar:\n",
    "\n",
    "![](imagens\\gradiente_fuga_explosao.PNG)\n",
    "\n",
    "\n",
    "Basicamente, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ tem $\\small t – k$ multiplicações; portanto, multiplicar o peso, $\\small w$, por ele mesmo $\\small t – k$ vezes resulta em um fator, $\\small w^{t-w}$ . Como resultado, se $\\small |w|< 1$, esse fator se torna muito pequeno quando $\\small t – k$ é grande. Por outro lado, se o peso da aresta recorrente for $\\small |w|> 1$ , então $w^{t-k}$ se torna muito grande quando $\\small t – k$ é grande. Observe que $\\small t – k$ grande se refere a dependências de longo alcance. Podemos ver que uma solução ingênua para evitar gradientes de fuga ou explosão pode ser alcançada garantindo $\\small |w| = 1$.\n",
    "\n",
    "Na prática, existem pelo menos três soluções para este problema:\n",
    "* *Gradient clipping*\n",
    "* *TBPTT*\n",
    "* *LSTM*\n",
    "\n",
    "Usando o *Gradient clipping*, especificamos um valor de corte ou limite para os gradientes e atribuímos esse valor de corte aos valores de gradiente que excedem esse valor. Em contraste, o *TBPTT* simplesmente limita ao número de passos de tempo que o sinal pode retropropagar após cada passagem para frente. Por exemplo, mesmo que a sequência tenha 100 elementos ou etapas, podemos retropropagar apenas as 20 etapas de tempo mais recentes.\n",
    "\n",
    "Embora tanto o *Gradient clipping* quanto o *TBPTT* possam resolver o problema do **gradiente explosivo**, o truncamento limita o número de etapas que o gradiente pode efetivamente retornar e atualizar adequadamente os pesos. Por outro lado, o *LSTM*, projetado em 1997 por *Sepp Hochreiter* e *Jürgen Schmidhuber*, tem tido mais sucesso em eliminar e explodir problemas de gradiente ao modelar dependências de longo alcance por meio de\n",
    "o uso de células de memória. Vamos discutir *LSTM* com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Células de memória de longo prazo\n",
    "\n",
    "Como afirmado anteriormente, os *LSTMs* foram introduzidos pela primeira vez para superar o problema do gradiente de fuga. O bloco de construção de um *LSTM* é uma célula de memória, que essencialmente representa ou substitui a camada oculta de RNNs padrão.\n",
    "\n",
    "Em cada célula de memória, há uma aresta recorrente que tem o peso desejável, $\\small w = 1$, como discutimos, para superar os problemas de gradiente de fuga e explosão. Os valores associados a essa borda recorrente são chamados coletivamente de estado da célula. A estrutura desdobrada de uma célula *LSTM* moderna é mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\ltsm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o estado da célula do passo de tempo anterior, $\\small \\textbf{C}^{(t-1)}$, é modificado para obter o estado da célula no passo de tempo atual, $\\small \\textbf{C}^{(t)}$, sem ser multiplicado diretamente por nenhum fator de peso. O fluxo de informação nesta célula de memória é controlado por várias unidades de computação (frequentemente chamadas de portas) que serão descritas aqui. Na figura anterior,$\\small \\bigodot$ refere-se ao produto **elemento-a-elemento** (multiplicação elemento-a-elemento) e $\\small \\bigoplus$ significa s**oma-elemento** (adição elemento-a-elemento). Além disso, $\\small \\textbf{x}^{(t)}$ refere-se aos dados de entrada no tempo $\\small t$, e $\\small \\textbf{h}^{(t-1)}$ indica as unidades ocultas no tempo $\\small t – 1$. Quatro caixas são indicadas com uma função de ativação, seja a função sigmóide ($\\small \\sigma$) ou $\\small tanh$, e um conjunto de pesos; essas caixas aplicam uma combinação linear realizando multiplicações de matriz-vetor em suas entradas (que são $\\small \\textbf{h}^{(t-1)}$ e $\\small \\textbf{x}^{(t)}$). Essas unidades de computação com funções de ativação sigmóides, cujas unidades de saída são passadas por $\\small \\bigodot$, são chamadas de portas.\n",
    "\n",
    "Em uma célula *LSTM*, existem três tipos diferentes de portas, que são conhecidas como **porta de esquecimento**, porta de entrada e porta de saída:\n",
    "\n",
    "* A **porta de esquecimento** ($\\small f_t$) permite que a célula de memória redefina o estado da célula sem crescer indefinidamente. Na verdade, o portão de esquecimento decide quais informações podem passar e quais informações devem ser suprimidas. Agora, $\\small f_t$ é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small f_t = \\sigma(\\textbf{W}_{xf}\\textbf{x}^{(t)} + \\textbf{W}_{hf}\\textbf{h}^{(t-1)} + \\textbf{b}_f)\n",
    "$$\n",
    "\n",
    "\n",
    "* A **porta de entrada** ($\\small i_t$) e o **valor candidato** ($\\breve{C}_t$) são responsáveis ​​por atualizar o estado da célula. Eles são calculados da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small i_t = \\sigma ( \\textbf{W}_{xi}\\textbf{x}^{(t)} +  \\textbf{W}_{hi}\\textbf{h}^{(t-1)} + \\textbf{b}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\small \\breve{C}_t = tanh (\\textbf{W}_{xc}\\textbf{x}^{(t)} + \\textbf{W}_{hc}\\textbf{h}^{(t-1)} + \\textbf{b}_c)\n",
    "$$\n",
    "\n",
    "O estado da célula no tempo t é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "C^{t} = (C^{(t-1)} \\odot f_t) \\otimes (i_t \\odot \\breve{C}_t)\n",
    "$$\n",
    "\n",
    "A porta de saída ($\\small \\textbf{o}_t$) decide como atualizar os valores das unidades ocultas:\n",
    "\n",
    "$$\n",
    "\\small o_t = \\sigma (\\textbf{W}_{xo}\\textbf{x}^{(t)} + \\textbf{W}_{ho}\\textbf{h}^{(t-1)} + \\textbf{b}_o)\n",
    "$$\n",
    "\n",
    "Dado isso, as unidades ocultas no passo de tempo atual são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\textbf{o}_t \\odot \\: tanh (\\textbf{C}^{(t)})\n",
    "$$\n",
    "\n",
    "A estrutura de uma célula *LSTM* e seus cálculos subjacentes podem parecer muito complexos e difíceis de implementar. No entanto, a boa notícia é que o *TensorFlow* já implementou tudo em funções otimizadas de wr*apper, o que nos permite definir nossas células *LSTM* de maneira fácil e eficiente. Aplicaremos RNNs e LSTMs a conjuntos de dados do mundo real posteriormente.\n",
    "\n",
    "> #### Outros modelos RNN avançados\n",
    "> *LSTMs* fornecem uma abordagem básica para modelar dependências de longo alcance em sequências. No entanto, é importante notar que existem muitas variações de *LSTMs* descritas na literatura (An Empirical Exploration of Recurrent Network Architectures, Rafal Jozefowicz, Wojciech Zaremba e Ilya Sutskever, Proceedings of ICML, 2342-2350, 2015).\n",
    ">\n",
    "> Também merece destaque uma abordagem mais recente, *Gated Recurrent Unit* (GRU), proposta em 2014. As GRUs possuem uma arquitetura mais simples que as *LSTMs*; portanto, eles são computacionalmente mais eficientes, enquanto seu desempenho em algumas tarefas, como modelagem de música polifônica, é comparável aos *LSTMs*.\n",
    "\n",
    "### Implementando RNNs para modelagem de sequência no TensorFlow\n",
    "\n",
    "Agora que abordamos a teoria subjacente por trás das *RNNs*, estamos prontos para passar para a parte mais prática: implementar *RNNs* no TensorFlow. Durante o restante deste capítulo, aplicaremos *RNNs* a duas tarefas problemáticas comuns:\n",
    "1. Análise de sentimentos\n",
    "2. Modelagem de linguagem\n",
    "\n",
    "Esses dois projetos, que apresentaremos, são fascinantes e envolventes. Assim, em vez de fornecer o código de uma só vez, dividiremos a implementação em várias etapas e discutiremos o código em detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Projeto um – prevendo o sentimento das críticas de filmes do IMDb**\n",
    "\n",
    "Nesta seção e nas subseções seguintes, implementaremos uma *RNN* multicamada para análise de sentimentos usando uma arquitetura muitos-para-um.\n",
    "\n",
    "Na próxima seção, implementaremos um *RNN* muitos-para-muitos para uma aplicação de modelagem de linguagem. Embora os exemplos escolhidos sejam propositadamente simples para introduzir os principais conceitos de *RNNs*, a modelagem de linguagem tem uma ampla gama de aplicações interessantes, como a construção de chatbots – dando aos computadores a capacidade de conversar e interagir diretamente com humanos.\n",
    "\n",
    "### Preparando os dados de revisão do filme\n",
    "Nas etapas de pré-processamento, criamos um conjunto de dados limpo chamado `movie_data.csv`, que usaremos novamente agora. Primeiro, importaremos os módulos necessários e leremos os dados em um `DataFrame` pandas, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se de que esse quadro de dados, `df`, consiste em duas colunas, chamadas `'review'` e `'sentiment'`, onde `'review'` contém o texto das resenhas de filmes (os recursos de entrada) e `'sentiment'` representa o rótulo de destino que queremos prever (0 refere-se ao sentimento negativo e 1 refere-se ao sentimento positivo).\n",
    "\n",
    "O componente de texto dessas resenhas de filmes são sequências de palavras, e o modelo *RNN* classifica cada sequência como uma resenha positiva (1) ou negativa (0). No entanto, antes de podermos alimentar os dados em um modelo *RNN*, precisamos aplicar várias etapas de pré-processamento:\n",
    "1. Crie um objeto de conjunto de dados do *TensorFlow* e divida-o em partições separadas de treinamento, teste e validação.\n",
    "2. Identifique as palavras exclusivas no conjunto de dados de treinamento.\n",
    "3. Mapeie cada palavra exclusiva para um número inteiro exclusivo e codifique o texto da revisão em números inteiros codificados (um índice de cada palavra exclusiva).\n",
    "4. Divida o conjunto de dados em minilotes como entrada para o modelo.\n",
    "\n",
    "Vamos prosseguir com a primeira etapa: criar um conjunto de dados do *TensorFlow* a partir deste quadro de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In 1974, the teenager Martha Moxley (Maggie Grace)' 1\n",
      "b'OK... so... I really like Kris Kristofferson and h' 0\n",
      "b'***SPOILER*** Do not read this, if you think about' 0\n"
     ]
    }
   ],
   "source": [
    "# Passo 1: Criando um Dataset\n",
    "\n",
    "target = df.pop('sentiment')\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    (df.values, target.values))\n",
    "\n",
    "## inspection:\n",
    "for ex in ds_raw.take(3):\n",
    "    tf.print(ex[0].numpy()[0][:50], ex[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos dividi-lo em conjuntos de dados de treinamento, teste e validação. Todo o conjunto de dados contém 50.000 exemplos. Manteremos os primeiros 25.000 exemplos para avaliação (conjunto de dados de teste de retenção) e, em seguida, 20.000 exemplos serão usados para treinamento e 5.000 para validação. O código é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "ds_raw = ds_raw.shuffle(50000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(25000)                 # Separa 25.000 amostras das 50.000\n",
    "ds_raw_train_valid = ds_raw.skip(25000)          # Pula as 25.000 amostras do conjunto anterior e salva em uma variável \n",
    "ds_raw_train = ds_raw_train_valid.take(20000)    # Da variável anterior, vamos pegas 20.000 e salvar numa variável.\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)    # Vamos pular 20.000 amostras e salvar numa variável. Que será somente 5.000 (que sobrou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preparar os dados para entrada em uma RN, precisamos codificá-los em valores numéricos, conforme mencionado nas etapas 2 e 3. Para fazer isso, primeiro encontraremos as palavras exclusivas (*tokens*) no conjunto de dados de treinamento. Embora encontrar *tokens* exclusivos seja um processo para o qual podemos usar conjuntos de dados *Python*, pode ser mais eficiente usar a classe `Counter` do pacote `collections`, que faz parte da biblioteca padrão do *Python*.\n",
    "\n",
    "No código a seguir, instanciaremos um novo objeto `Counter` (`token_counts`) que coletará as frequências de palavras exclusivas. Observe que neste aplicativo específico (e em contraste com o modelo *bag-of-words*), estamos interessados apenas no conjunto de palavras únicas e não exigiremos a contagem de palavras, que é criada como um produto secundário. Para dividir o texto em palavras (ou *tokens*), o pacote `tensorflow_datasets` fornece uma classe `Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 87007\n"
     ]
    }
   ],
   "source": [
    "## Passo 2: Encontrar tokens (palavras) unicas\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in ds_raw_train:\n",
    "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos mapear cada palavra única para um inteiro único. Isso pode ser feito manualmente usando um dicionário Python, onde as chaves são os tokens exclusivos (palavras) e o valor associado a cada chave é um inteiro único. No entanto, o pacote `tensorflow_datasets` já fornece uma classe, `TokenTextEncoder`, que podemos usar para criar esse mapeamento e codificar todo o conjunto de dados. Primeiro, criaremos um objeto codificador da classe `TokenTextEncoder` passando os tokens exclusivos (`token_counts` contém os tokens e suas contagens, embora aqui, suas contagens não sejam necessárias, portanto, serão ignoradas). Chamar o método `encoder.encode()` converterá seu texto de entrada em uma lista de valores inteiros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232, 9, 35, 1123]\n"
     ]
    }
   ],
   "source": [
    "## Passo 3: Encode tokens únicos (palavras) para inteiros\n",
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "example_str = 'This is a example!'\n",
    "print(encoder.encode(example_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que pode haver alguns *tokens* nos dados de validação ou teste que não estão presentes nos dados de treinamento e, portanto, não estão incluídos no mapeamento. Se tivermos $\\small q$ *tokens* (que é o tamanho de `token_counts` passado para o `TokenTextEncoder`, que neste caso é 87.007), todos os *tokens* que não foram vistos antes e, portanto, não estão incluídos em `token_counts`, receberão o inteiro $\\small q + 1$ (que será 87.008 no nosso caso). Em outras palavras, o índice $\\small q + 1$ é reservado para palavras desconhecidas.\n",
    "\n",
    "Outro valor reservado é o inteiro 0, que serve como espaço reservado para ajustar o comprimento da sequência. Mais tarde, quando estivermos construindo um modelo *RNN* no *TensorFlow*, consideraremos esses dois espaços reservados, 0 e $\\small q + 1$, com mais detalhes.\n",
    "\n",
    "Podemos usar o método `map()` dos objetos do conjunto de dados para transformar cada texto no conjunto de dados de acordo, assim como aplicaríamos qualquer outra transformação a um conjunto de dados. No entanto, há um pequeno problema: aqui, os dados de texto são incluídos em objetos tensores, que podemos acessar chamando o método `numpy()` em um tensor no modo de execução antecipada. Mas durante as transformações pelo método `map()`, a execução antecipada será desabilitada. Para resolver este problema, podemos definir duas funções. A primeira função tratará os tensores de entrada como se o modo de execução ansioso estivesse habilitado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passo 3-A: defina uma função para a tranformação\n",
    "\n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na segunda função, envolveremos a primeira função usando `tf.py_function` para convertê-la em um operador *TensorFlow*, que pode ser usado por meio de seu método `map()`. Este processo de codificação de texto em uma lista de inteiros pode ser realizado usando o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da sequência: (24,)\n",
      "Tamanho da sequência: (179,)\n",
      "Tamanho da sequência: (262,)\n",
      "Tamanho da sequência: (535,)\n",
      "Tamanho da sequência: (130,)\n"
     ]
    }
   ],
   "source": [
    "## Passo 3-B: Empacotar a Função encode na Tf. Op\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label],\n",
    "    Tout=(tf.int64, tf.int64))\n",
    "\n",
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "#  Checando o Shape de alguns exemplos\n",
    "tf.random.set_seed(1)\n",
    "for example in ds_train.shuffle(1000).take(5):\n",
    "    print(f\"Tamanho da sequência: {example[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora, convertemos sequências de palavras em sequências de inteiros. No entanto, há um problema que ainda precisamos resolver — as sequências atualmente têm comprimentos diferentes (como mostrado no resultado da execução do código anterior para cinco exemplos escolhidos aleatoriamente). Embora, em geral, as RNNs possam lidar com sequências com comprimentos diferentes, ainda precisamos garantir que todas as sequências em um minilote tenham o mesmo comprimento para armazená-las de forma eficiente em um tensor.\n",
    "\n",
    "Para dividir um conjunto de dados que possui elementos com formas diferentes em mini-lotes, o *TensorFlow* fornece um método diferente, *padded_batch()* (em vez de `batch()`), que preencherá automaticamente os elementos consecutivos que devem ser combinados em um lote com valores de espaço reservado (0s) para que todas as sequências dentro de um lote tenham a mesma forma. Para ilustrar isso com um exemplo prático, vamos pegar um pequeno subconjunto de tamanho 8 do conjunto de dados de treinamento, `ds_train`, e aplicar o método *padded_batch()* a esse subconjunto com `batch_size=4`. Também imprimiremos os tamanhos dos elementos individuais antes de combiná-los em minilotes, bem como as dimensões dos minilotes resultantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho individual: (119,)\n",
      "Tamanho individual: (688,)\n",
      "Tamanho individual: (308,)\n",
      "Tamanho individual: (204,)\n",
      "Tamanho individual: (326,)\n",
      "Tamanho individual: (240,)\n",
      "Tamanho individual: (127,)\n",
      "Tamanho individual: (453,)\n"
     ]
    }
   ],
   "source": [
    "## Selecionar um pequeno conjunto de dados:\n",
    "\n",
    "ds_subset = ds_train.take(8)\n",
    "for example in ds_subset:\n",
    "    print(f\"Tamanho individual: {example[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dimension: (4, 688)\n",
      "Batch dimension: (4, 453)\n"
     ]
    }
   ],
   "source": [
    "## Loteando o conjunto de dados\n",
    "\n",
    "ds_batch = ds_subset.padded_batch(4, padded_shapes=([-1], []))\n",
    "\n",
    "for batch in ds_batch:\n",
    "    print(f\"Batch dimension: {batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode observar nas formas de tensor impressas, o número de colunas (ou seja, `.shape[1]`) no primeiro lote é `688`, resultado da combinação dos quatro primeiros exemplos em um único lote e do uso do tamanho máximo desses exemplos. Isso significa que os outros três exemplos neste lote são preenchidos o quanto for necessário para corresponder a esse tamanho.\n",
    "\n",
    "Da mesma forma, o segundo lote mantém o tamanho máximo de seus quatro exemplos individuais, que é `453`, e preenche os outros exemplos para que seu comprimento seja menor que o comprimento máximo.\n",
    "\n",
    "Vamos dividir todos os três conjuntos de dados em minilotes com um tamanho de lote de 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds_train.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "valid_data = ds_valid.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = ds_test.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, os dados estão em um formato adequado para um modelo RNN, que vamos implementar. No próximo tópico, no entanto, discutiremos primeiro a incorporação de recursos, que é uma etapa de pré-processamento opcional, mas altamente recomendada, usada para reduzir a dimensionalidade dos vetores de palavras.\n",
    "\n",
    "### Incorporando camadas para codificação de frases\n",
    "Durante a preparação dos dados na etapa anterior, geramos sequências de mesmo comprimento. Os elementos dessas sequências eram números inteiros que correspondiam aos `índices` de palavras únicas. Esses índices de palavras podem ser convertidos em recursos de entrada de várias maneiras diferentes. Uma maneira ingênua é aplicar a codificação *one-hot* para converter os índices em vetores de zeros e uns. Em seguida, cada palavra será mapeada para um vetor cujo tamanho é o número de palavras únicas em todo o conjunto de dados. Dado que o número de palavras únicas (o tamanho do vocabulário) pode ser da ordem de $\\small 10^4 - 10^5$, que também será o número de nossos recursos de entrada, um modelo treinado em tais recursos pode sofrer a **maldição da dimensionalidade**. Além disso, esses recursos são muito esparsos, pois todos são zero, exceto um.\n",
    "\n",
    "Uma abordagem mais elegante é mapear cada palavra para um vetor de tamanho fixo com elementos de valor real (não necessariamente inteiros). Em contraste com os vetores codificados *one-hot*, podemos usar vetores de tamanho finito para representar um número infinito de números reais. (Em teoria, podemos extrair infinitos números reais de um determinado intervalo, por exemplo [–1, 1].)\n",
    "\n",
    "Essa é a ideia por trás da incorporação (**embedding**), que é uma técnica de aprendizado de recursos que podemos utilizar aqui para aprender automaticamente os recursos importantes para representar as palavras em nosso conjunto de dados. Dado o número de palavras únicas, $\\small n_{words}$, podemos selecionar o tamanho dos vetores de incorporação (também conhecido como, a dimensão de incorporação) para ser muito menor que o número de palavras únicas ($\\small embedding_{dims} << n_{words}$) para representar todo o vocabulário como recursos de entrada.\n",
    "\n",
    "As vantagens da incorporação sobre a codificação *one-hot* são as seguintes:\n",
    "• Uma redução na dimensionalidade do espaço de recursos para diminuir o efeito da maldição da dimensionalidade\n",
    "• A extração de recursos salientes desde a camada de incorporação em uma RN pode ser otimizada (ou aprendida)\n",
    "\n",
    "A representação esquemática a seguir mostra como a incorporação funciona mapeando índices de token para uma matriz de incorporação treinável:\n",
    "\n",
    "![](imagens\\embedding.PNG)\n",
    "\n",
    "\n",
    "Dado um conjunto de tokens de tamanho $\\small n + 2$ ($\\small n$ é o tamanho do conjunto de *tokens*, mais o índice 0 é reservado para o preenchimento e $\\small n + 1$ é para as palavras não presentes no conjunto de *tokens*), uma matriz de incorporação de tamanho $\\small (n + 2) \\times embedding-dim$ será criado onde cada linha desta matriz representa recursos numéricos associados a um *token*.\n",
    "\n",
    "Portanto, quando um índice inteiro, $\\small i$, for fornecido como entrada para a incorporação, ele procurará a linha correspondente da matriz no índice $\\small i$ e retornará os recursos numéricos. A matriz de incorporação serve como camada de entrada para nossos modelos RN.\n",
    "\n",
    "Na prática, a criação de uma camada de incorporação pode ser feita simplesmente usando `tf.keras.layers.Embedding`. Vamos ver um exemplo onde vamos criar um modelo e adicionar uma camada de embedding, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, 20, 6)             600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600\n",
      "Trainable params: 600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=100,\n",
    "                    output_dim=6,\n",
    "                    input_length=20,\n",
    "                    name=\"embed-layer\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrada para este modelo (camada de incorporação) deve ter a classificação 2 com dimensionalidade $\\small batchsize \\times input\\_length$, onde $\\small input\\_length$ é o comprimento das seqüências (no caso aqui, definida como 20 através do argumento `input_length`). Por exemplo, uma sequência de entrada no minilote pode ser $\\small \\left \\langle  14,43,52, 1,8,19,67,83,10,7,42,87,56,18,94,17,67,90, 6,39 \\right \\rangle$, onde cada elemento desta sequência é o índice das palavras únicas. A saída terá dimensionalidade $\\small batchsize \\times input\\_length \\times embedding\\_dim$, onde $\\small embedding\\_dim$ é o tamanho dos recursos de incorporação (aqui, definido como 6 via `output_dim`). O outro argumento fornecido à camada de incorporação, `input_dim`, corresponde aos valores inteiros exclusivos que o modelo receberá como entrada (por exemplo, `n + 2`, definido aqui como `100`). Portanto, a matriz de incorporação neste caso tem o tamanho `100 × 6`.\n",
    "\n",
    "> ##### Lidando com comprimentos de sequência variáveis\n",
    "> Observe que o argumento `input_length` não é necessário e podemos usar `None` para casos em que os comprimentos das sequências de entrada variam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo um modelo *RNN*\n",
    "Agora estamos prontos para construir um modelo *RNN*. Usando a classe Keras `Sequential`, podemos combinar a camada de incorporação (*embedding*), as camadas recorrentes da *RNN* e as camadas não recorrentes totalmente conectadas. Para as camadas recorrentes, podemos usar qualquer uma das seguintes implementações:\n",
    "* `SimpleRNN`: uma camada *RNN* regular, ou seja, uma camada recorrente totalmente conectada\n",
    "* `LSTM`: uma *RNN* de memória de curto prazo longo, que é útil para capturar as dependências de longo prazo\n",
    "* `GRU`: uma camada recorrente com uma unidade recorrente fechada, como alternativa aos `LSTMs`\n",
    "\n",
    "Para ver como um modelo *RNN* multicamada pode ser construído usando uma dessas camadas recorrentes, no exemplo a seguir, criaremos um modelo *RNN*, começando com uma camada de incorporação com `input_dim=1000` e `output_dim=32`. Em seguida, serão adicionadas duas camadas recorrentes do tipo `SimpleRNN`. Por fim, adicionaremos uma camada totalmente conectada não recorrente como camada de saída, que retornará um único valor de saída como previsão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          32000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,193\n",
      "Trainable params: 36,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 32)          8320      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,673\n",
      "Trainable params: 336,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Usando uma RNN com LTSM \n",
    "from tensorflow.keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 32)          6336      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 332,705\n",
      "Trainable params: 332,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Um exemplo com uma camada GRU\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, construir um modelo *RNN* usando essas camadas recorrentes é bastante simples. Na próxima subseção, voltaremos à nossa tarefa de análise de sentimentos e construiremos um modelo *RNN* para resolver isso.\n",
    "\n",
    "### Construindo um modelo *RNN* para a tarefa de análise de sentimento\n",
    "\n",
    "Como temos sequências muito longas, usaremos uma camada *LSTM* para contabilizar os efeitos de longo prazo. Além disso, colocaremos a camada *LSTM* dentro de um wrapper `Bidirectional`, que fará com que as camadas recorrentes passem pelas sequências de entrada de ambas as direções, do início ao fim, bem como no sentido inverso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1740180   \n",
      "                                                                 \n",
      " bidir-lstm (Bidirectional)  (None, 128)               43520     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,792,021\n",
      "Trainable params: 1,792,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 55s 85ms/step - loss: 0.5274 - accuracy: 0.7254 - val_loss: 0.4357 - val_accuracy: 0.8254\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 52s 83ms/step - loss: 0.3021 - accuracy: 0.8830 - val_loss: 0.4381 - val_accuracy: 0.8108\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 52s 83ms/step - loss: 0.1574 - accuracy: 0.9441 - val_loss: 0.4198 - val_accuracy: 0.8310\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 53s 84ms/step - loss: 0.0890 - accuracy: 0.9724 - val_loss: 0.5052 - val_accuracy: 0.8448\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 52s 84ms/step - loss: 0.0568 - accuracy: 0.9830 - val_loss: 0.5721 - val_accuracy: 0.8252\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 53s 84ms/step - loss: 0.0357 - accuracy: 0.9891 - val_loss: 0.6123 - val_accuracy: 0.8278\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 53s 85ms/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.7861 - val_accuracy: 0.7994\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 53s 84ms/step - loss: 0.0351 - accuracy: 0.9884 - val_loss: 0.7229 - val_accuracy: 0.8184\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 53s 85ms/step - loss: 0.0423 - accuracy: 0.9865 - val_loss: 0.8662 - val_accuracy: 0.8028\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 53s 85ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.8292 - val_accuracy: 0.8342\n",
      "782/782 [==============================] - 32s 41ms/step - loss: 0.8170 - accuracy: 0.8368\n",
      "Test Acc.: 83.68%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "## Construindo o modelo \n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        name='embed-layer'),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
    "        name='bidir-lstm'), \n",
    "\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "## Compilando e treinando:\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = bi_lstm_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)\n",
    "\n",
    "## Avaliando os dados\n",
    "test_results= bi_lstm_model.evaluate(test_data)\n",
    "print('Test Acc.: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após treinar este modelo por 10 épocas, a avaliação nos dados de teste mostra **83,68%** de precisão. (Observe que esse resultado não é o melhor quando comparado aos métodos de última geração usados no conjunto de dados do IMDb. O objetivo era simplesmente mostrar como o RNN funciona.)\n",
    "\n",
    ">##### Mais sobre o RNN bidirecional\n",
    "> O wrapper `Bidirectional` faz duas passagens em cada sequência de entrada: uma passagem para frente e uma passagem reversa ou para trás (observe que isso não deve ser confundido com as passagens para frente e para trás no contexto de retropropagação). Os resultados dessas passagens para frente e para trás serão concatenados por padrão. Mas se você quiser mudar esse comportamento, você pode definir o argumento `merge_mode` para `'sum'` (para soma), `'mul'` (para multiplicar os resultados das duas passagens), `'ave'` (para tirar a média das duas) , `'concat'` (que é o padrão) ou `None`, que retorna os dois tensores em uma lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos tentar outros tipos de camadas recorrentes, como `SimpleRNN`. No entanto, como se vê, um modelo construído com camadas recorrentes regulares não será capaz de alcançar um bom desempenho preditivo (mesmo nos dados de treinamento). Por exemplo, se você tentar substituir a camada *LSTM* bidirecional no código anterior por uma camada `SimpleRNN` unidirecional e treinar o modelo em sequências completas, poderá observar que a perda nem diminuirá durante o treinamento. **A razão é que as sequências neste conjunto de dados são muito longas**, portanto, um modelo com uma camada `SimpleRNN` não pode aprender as dependências de longo prazo e pode sofrer com problemas de **gradiente de fuga ou explosão**.\n",
    "\n",
    "Para obter um desempenho preditivo razoável neste conjunto de dados usando um `SimpleRNN`, podemos truncar as sequências. Além disso, utilizando nosso \"conhecimento de domínio\", podemos supor que os últimos parágrafos de uma crítica de filme podem conter a maioria das informações sobre seu sentimento. Portanto, podemos nos concentrar apenas na última parte de cada revisão. Para fazer isso, vamos definir uma função auxiliar, `preprocess_datasets()`, para combinar as etapas de pré-processamento `2-4`. Um argumento opcional para esta função é `max_seq_length`, que determina quantos *tokens* de cada revisão devem ser usados. Por exemplo, se definirmos `max_seq_length=100` e um comentário tiver mais de 100 *tokens*, apenas os últimos 100 *tokens* serão usados. Se `max_seq_length` for definido como `None`, as sequências de comprimento total serão usadas como antes. Tentar valores diferentes para `max_seq_length` nos dará mais informações sobre a capacidade de diferentes modelos de RNN para lidar com sequências longas.\n",
    "\n",
    "O código para a função `preprocess_datasets()` é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def preprocess_datasets(\n",
    "    ds_raw_train, \n",
    "    ds_raw_valid, \n",
    "    ds_raw_test,\n",
    "    max_seq_length=None,\n",
    "    batch_size=32):\n",
    "    \n",
    "    ## Passo 1: (Já feito => Cria o DataSet)\n",
    "    ## Passo 2: (Encontra os tokens únicos (palavras)\n",
    "    tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for example in ds_raw_train:\n",
    "        tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "        if max_seq_length is not None:\n",
    "            tokens = tokens[-max_seq_length:]\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "    print('Vocab-size:', len(token_counts))\n",
    "\n",
    "\n",
    "    ## Passo 3: Encode os textos\n",
    "    encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "    def encode(text_tensor, label):\n",
    "        text = text_tensor.numpy()[0]\n",
    "        encoded_text = encoder.encode(text)\n",
    "        if max_seq_length is not None:\n",
    "            encoded_text = encoded_text[-max_seq_length:]\n",
    "        return encoded_text, label\n",
    "\n",
    "    def encode_map_fn(text, label):\n",
    "        return tf.py_function(encode, inp=[text, label], \n",
    "                              Tout=(tf.int64, tf.int64))\n",
    "\n",
    "    ds_train = ds_raw_train.map(encode_map_fn)\n",
    "    ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "    ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "    ## Passo 4: Lotea o DataSet\n",
    "    train_data = ds_train.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    valid_data = ds_valid.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    test_data = ds_test.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    return (train_data, valid_data, \n",
    "            test_data, len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos definir outra função auxiliar, `build_rnn_model()`, para construir modelos com diferentes arquiteturas de forma mais conveniente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "def build_rnn_model(embedding_dim, vocab_size,\n",
    "                    recurrent_type='SimpleRNN',\n",
    "                    n_recurrent_units=64,\n",
    "                    n_recurrent_layers=1,\n",
    "                    bidirectional=True):\n",
    "\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # Constrói o modelo\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            name='embed-layer')\n",
    "    )\n",
    "    \n",
    "    for i in range(n_recurrent_layers):\n",
    "        return_sequences = (i < n_recurrent_layers-1)\n",
    "            \n",
    "        if recurrent_type == 'SimpleRNN':\n",
    "            recurrent_layer = SimpleRNN(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='simprnn-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            recurrent_layer = LSTM(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='lstm-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'GRU':\n",
    "            recurrent_layer = GRU(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='gru-layer-{}'.format(i))\n",
    "        \n",
    "        if bidirectional:\n",
    "            recurrent_layer = Bidirectional(\n",
    "                recurrent_layer, name='bidir-'+recurrent_layer.name)\n",
    "            \n",
    "        model.add(recurrent_layer)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, usando essas duas funções auxiliares bastante gerais, mas convenientes, podemos comparar prontamente diferentes modelos RNN com diferentes comprimentos de sequência de entrada. Como exemplo, no código a seguir, tentaremos um modelo com uma única camada recorrente do tipo `SimpleRNN` enquanto truncamos as sequências para um comprimento máximo de 100 *tokens*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 58063\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1161300   \n",
      "                                                                 \n",
      " bidir-simprnn-layer-0 (Bidi  (None, 128)              10880     \n",
      " rectional)                                                      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,180,501\n",
      "Trainable params: 1,180,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dim = 20\n",
    "max_seq_length = 100\n",
    "\n",
    "train_data, valid_data, test_data, n = preprocess_datasets(\n",
    "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
    "    max_seq_length=max_seq_length, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "vocab_size = n + 2\n",
    "\n",
    "rnn_model = build_rnn_model(\n",
    "    embedding_dim, vocab_size,\n",
    "    recurrent_type='SimpleRNN', \n",
    "    n_recurrent_units=64,\n",
    "    n_recurrent_layers=1,\n",
    "    bidirectional=True)\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 164s 261ms/step - loss: 0.7034 - accuracy: 0.5006 - val_loss: 0.7298 - val_accuracy: 0.4900\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 163s 260ms/step - loss: 0.6791 - accuracy: 0.5579 - val_loss: 0.6731 - val_accuracy: 0.5918\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 164s 263ms/step - loss: 0.5653 - accuracy: 0.7165 - val_loss: 0.5194 - val_accuracy: 0.7768\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 163s 261ms/step - loss: 0.4021 - accuracy: 0.8244 - val_loss: 0.5606 - val_accuracy: 0.7862\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 164s 263ms/step - loss: 0.3131 - accuracy: 0.8776 - val_loss: 0.5140 - val_accuracy: 0.7976\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 0.2458 - accuracy: 0.9118 - val_loss: 0.5852 - val_accuracy: 0.7928\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 162s 259ms/step - loss: 0.1962 - accuracy: 0.9335 - val_loss: 0.6190 - val_accuracy: 0.7994\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 160s 256ms/step - loss: 0.1603 - accuracy: 0.9453 - val_loss: 0.6776 - val_accuracy: 0.7818\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 159s 255ms/step - loss: 0.0889 - accuracy: 0.9697 - val_loss: 0.7926 - val_accuracy: 0.7694\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 161s 257ms/step - loss: 0.0473 - accuracy: 0.9858 - val_loss: 0.8514 - val_accuracy: 0.7908\n"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, truncar as sequências para 100 tokens e usar uma camada `SimpleRNN` bidirecional resultou em **79%** de precisão de classificação. Embora a previsão seja um pouco menor quando comparada ao modelo *LSTM* bidirecional anterior (**83,68%** de precisão no conjunto de dados de teste), o desempenho nessas sequências truncadas é muito melhor do que o desempenho que poderíamos alcançar com um `SimpleRNN` em resenhas de filmes completos. Como exercício opcional, você pode verificar isso usando as duas funções auxiliares que já definimos. Experimente com `max_seq_length=None` e defina o argumento bidirecional dentro da função auxiliar `build_rnn_model()` como `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Projeto dois – modelagem de linguagem em nível de caractere no *TensorFlow***\n",
    "\n",
    "A modelagem de linguagem é uma aplicatição fascinante que permite que as máquinas executem tarefas relacionadas à linguagem humana, como gerar frases em inglês.\n",
    "\n",
    "No modelo que construiremos agora, a entrada é um documento de texto, e nosso objetivo é desenvolver um modelo que possa gerar um novo texto com estilo semelhante ao documento de entrada. Exemplos de tal entrada são um livro ou um programa de computador em uma linguagem de programação específica.\n",
    "\n",
    "Na modelagem de linguagem em nível de caractere, a entrada é dividida em uma sequência de caracteres que são inseridos em nossa rede, um caractere por vez. A rede processará cada novo personagem em conjunto com a memória dos personagens vistos anteriormente para prever o próximo. A figura a seguir mostra um exemplo de modelagem de linguagem em nível de caractere (observe que EOS significa *End of Sequence* (fim de sequência)):\n",
    "![](imagens\\modelagem_nivel_caractere.PNG)\n",
    "\n",
    "Podemos dividir essa implementação em três etapas separadas:\n",
    "1. preparar os dados, construir o modelo RNN e realizar previsão, e\n",
    "2. amostragem do próximo caractere para gerar novo texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento do conjunto de dados\n",
    "Nesta seção, prepararemos os dados para modelagem de linguagem em nível de caractere.\n",
    "\n",
    "Para obter os dados de entrada, visite o site do Project Gutenberg em https://www.gutenberg.org/, que fornece milhares de e-books gratuitos. Para o nosso exemplo, você pode baixar o livro A Ilha Misteriosa, de Júlio Verne (publicado em 1874) em formato de texto simples de http://www.gutenberg.org/files/1268/1268-0.txt.\n",
    "\n",
    "Depois de baixar o conjunto de dados, podemos lê-lo em uma sessão do Python como texto simples. Usando o código a seguir, vamos ler o texto diretamente do arquivo baixado e remover partes do início e do fim (estes contêm certas descrições do projeto Gutenberg). Em seguida, criaremos uma variável *Python*, `char_set`, que representa o conjunto de caracteres `unique` observados neste texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download finalizado. Arquivo salvo em: 1268-0.txt\n",
      "\n",
      "Tamanho total: 1112350\n",
      "Caracteres únicos: 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "url = \"https://www.gutenberg.org/files/1268/1268-0.txt\"\n",
    "resposta = requests.get(url=url)\n",
    "endereco = os.path.basename(url.split('?')[0])\n",
    "if resposta.status_code == requests.codes.OK:\n",
    "    with open(endereco, 'wb') as arquivo:\n",
    "        arquivo.write(resposta.content)\n",
    "    print('Download finalizado. Arquivo salvo em: {}'.format(endereco))\n",
    "\n",
    "with open('1268-0.txt', 'r', encoding='utf8') as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "print()\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Tamanho total:', len(text))\n",
    "print('Caracteres únicos:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o download e pré-processamento do texto, temos uma sequência composta por 1.112.350 caracteres no total e 80 caracteres únicos. No entanto, a maioria das bibliotecas NN e implementações RNN não podem lidar com dados de entrada em formato de *string*, e é por isso que temos que converter o texto em um formato numérico. Para fazer isso, vamos criar um dicionário Python simples que mapeia cada caractere para um inteiro, `char2int`. Também precisaremos de um mapeamento reverso para converter os resultados do nosso modelo de volta em texto.\n",
    "\n",
    "Embora o inverso possa ser feito usando um dicionário que associa chaves inteiras a valores de caracteres, usar um array *NumPy* e indexar o array para mapear índices para esses caracteres exclusivos é mais eficiente. A figura a seguir mostra um exemplo de conversão de caracteres em inteiros e o inverso para as palavras \"Hello\" e \"world\":\n",
    "\n",
    "![](imagens\\mapeamento_reverso.PNG)\n",
    "\n",
    "A construção do dicionário para mapear caracteres para inteiros e o mapeamento reverso por meio da indexação de um array *NumPy*, conforme mostrado na figura anterior, é a seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Encode Shape: (1112350,)\n",
      "THE MYSTERIOUS  == Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28] == Reverse ==> ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)                        # Organizando o Conjunto ÚNICO da dados\n",
    "char2int = {ch:i for i, ch in enumerate(chars_sorted)} # Criando os índices para o Conjunto organizado em um Dict\n",
    "char_array = np.array(chars_sorted)                    # Criando um Array com o Conjunto ordenado.\n",
    "\n",
    "text_encoded = np.array(                               # Aplicando o mapeamento no texto de Júlio Verne\n",
    "                        [char2int[ch] for ch in text],\n",
    "                        dtype=np.int32   \n",
    "                       )              \n",
    "print(f'Text Encode Shape: {text_encoded.shape}')       \n",
    "\n",
    "print(text[:15], '== Encoding ==>', text_encoded[:15]) # Aplicando o Encode nos primeiros 15 caracteres\n",
    "print(text_encoded[15:21], '== Reverse ==>',''.join(char_array[text_encoded[15:21]])) # Aplicando o decode no caracteres ISLAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz NumPy `text_encoded` contém os valores codificados para todos os caracteres no texto. Agora, vamos criar um conjunto de dados do *TensorFlow* a partir deste array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 ===> T\n",
      "32 ===> H\n",
      "29 ===> E\n",
      "1 ===>  \n",
      "37 ===> M\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for ex in ds_text_encoded.take(5):\n",
    "    print(f'{ex.numpy()} ===> {char_array[ex.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora, criamos um objeto Dataset iterável para obter caracteres na ordem em que aparecem no texto. Agora, vamos dar um passo atrás e olhar para o quadro geral do que estamos tentando fazer. Para a tarefa de geração de texto, podemos formular o problema como uma tarefa de classificação.\n",
    "\n",
    "Suponha que tenhamos um conjunto de sequências de caracteres de texto incompletos, conforme mostrado na figura a seguir:\n",
    "\n",
    "![](imagens\\sequencias_targets.PNG)\n",
    "\n",
    "Na figura anterior, podemos considerar as sequências mostradas na caixa à esquerda como sendo a entrada. Para gerar um novo texto, nosso objetivo é projetar um modelo que possa prever o **próximo caractere** de uma determinada sequência de entrada, onde a sequência de entrada representa um texto incompleto. Por exemplo, depois de ver `\"Deep Learn\"`, o modelo deve prever `\"i\"` como o próximo caractere. Dado que temos 80 caracteres únicos, este problema torna-se uma tarefa de classificação multiclasse.\n",
    "\n",
    "Começando com uma sequência de comprimento 1 (ou seja, uma única letra), podemos gerar iterativamente novo texto com base nessa abordagem de classificação multiclasse, conforme ilustrado na figura a seguir:\n",
    "\n",
    "![](imagens\\sequencias_classificacao.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar a tarefa de geração de texto no *TensorFlow*, primeiro vamos cortar o comprimento da sequência para 40. Isso significa que o tensor de entrada, x, consiste em 40 tokens. **Na prática, o comprimento da sequência impacta na qualidade do texto gerado**. Sequências mais longas podem resultar em frases mais significativas. Para sequências mais curtas, no entanto, o modelo pode se concentrar em capturar palavras individuais corretamente, ignorando o contexto na maior parte.\n",
    "\n",
    "Embora sequências mais longas geralmente resultem em sentenças mais significativas, como mencionado, para sequências longas, o modelo RNN terá problemas para capturar dependências de longo prazo. Assim, na prática, encontrar um ponto ideal e um bom valor para o comprimento da sequência é um problema de otimização de hiperparâmetros, que temos que avaliar empiricamente. Aqui, vamos escolher 40, pois oferece uma boa troca.\n",
    "\n",
    "Como você pode ver na figura anterior, as entradas, $\\small \\textbf{x}$, e os destinos, $\\small \\textbf{y}$, são deslocados por um caractere. Assim, dividiremos o texto em pedaços de tamanho 41: os primeiros 40 caracteres formarão a sequência de entrada, $\\small \\textbf{x}$, e os últimos 40 elementos formarão a sequência alvo, $\\small \\textbf{y}$.\n",
    "\n",
    "Já armazenamos todo o texto codificado em sua ordem original em um objeto `Dataset`, `ds_text_encoded`. Usando as técnicas relativas à transformação de conjuntos de dados que já abordamos, você pode pensar em uma maneira de obter a entrada, $\\small \\textbf{x}$, e o destino, $\\small \\textbf{y}$, como foi mostrado na figura anterior? A resposta é muito simples: primeiro usaremos o método `batch()` para criar blocos de texto com 41 caracteres cada. Isso significa que definiremos `batch_size=41`. Vamos nos livrar ainda mais do último lote se for menor que 41 caracteres. Como resultado, o novo conjunto de dados fragmentado, denominado `ds_chunks`, sempre conterá sequências de tamanho 41. Os fragmentos de 41 caracteres serão usados ​​para construir a sequência $\\small \\textbf{x}$ (ou seja, a entrada), bem como a sequência $\\small \\textbf{y}$ (que é, o alvo), ambos terão 40 elementos. Por exemplo, a sequência x consistirá dos elementos com índices [0, 1,...,39]. Além disso, como a sequência $\\small \\textbf{y}$ será deslocada de uma posição em relação a $\\small \\textbf{x}$, seus índices correspondentes serão [1, 2,..., 40]. Em seguida, aplicaremos uma função de transformação usando o método `map()` para separar as sequências $\\small \\textbf{x}$ e $\\small \\textbf{y}$ de acordo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "ds_chuncks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
    "\n",
    "# Definir uma Função que divide X e Y\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq\n",
    "\n",
    "# Aplicar o map() em todo conjunto com a função\n",
    "\n",
    "ds_sequences = ds_chuncks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver alguns exemplos deste dataset transformado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input  (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "  Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      "  Input  (x): ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
      "  Target (y): 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in ds_sequences.take(2):\n",
    "    print(f\"  Input  (x): {repr(''.join(char_array[example[0].numpy()]))}\" )\n",
    "    print(f\"  Target (y): {repr(''.join(char_array[example[1].numpy()]))}\" )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, a última etapa na preparação do conjunto de dados é dividir esse conjunto de dados em minilotes. Durante a primeira etapa de pré-processamento para dividir o conjunto de dados em lotes, criamos pedaços de frases. Cada pedaço representa uma frase, que corresponde a um exemplo de treinamento. Agora, vamos embaralhar os exemplos de treinamento e dividir as entradas em mini-lotes novamente; no entanto, desta vez, cada lote conterá vários exemplos de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 40), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(None, 40), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo um modelo RNN em nível de personagem\n",
    "Agora que o conjunto de dados está pronto, a construção do modelo será relativamente simples. Para reutilização de código, escreveremos uma função, `build_model`, que define um modelo RNN usando a classe Keras `Sequential`. Então, podemos especificar os parâmetros de treinamento e chamar essa função para obter um modelo RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 256)         20480     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 512)         1574912   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, None, 80)          41040     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,636,432\n",
      "Trainable params: 1,636,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = charset_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a camada *LSTM* neste modelo tem o formato de saída (None, None, 512), o que significa que a saída de *LSTM* é de *rank* 3. A primeira dimensão representa o número de lotes, a segunda dimensão o comprimento da sequência de saída e a última dimensão corresponde ao número de unidades ocultas. A razão para ter a saída de *rank* 3 da camada *LSTM* é porque especificamos `return_sequences=True` ao definir nossa camada *LSTM*. Uma camada totalmente conectada (`Dense`) recebe a saída da célula *LSTM* e calcula os logits para cada elemento das sequências de saída. Como resultado, a saída final do modelo também será um tensor de *rank* 3.\n",
    "Além disso, especificamos `activation=None` para a camada final totalmente conectada. A razão para isso é que precisaremos ter os logits como saídas do modelo para que possamos amostrar as previsões do modelo para gerar um novo texto. Chegaremos a esta parte de amostragem mais tarde. Por enquanto, vamos treinar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "424/424 [==============================] - 13s 28ms/step - loss: 2.3203\n",
      "Epoch 2/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.7480\n",
      "Epoch 3/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.5487\n",
      "Epoch 4/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.4342\n",
      "Epoch 5/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.3608\n",
      "Epoch 6/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.3098\n",
      "Epoch 7/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.2713\n",
      "Epoch 8/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.2407\n",
      "Epoch 9/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.2142\n",
      "Epoch 10/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.1926\n",
      "Epoch 11/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.1729\n",
      "Epoch 12/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.1551\n",
      "Epoch 13/20\n",
      "424/424 [==============================] - 12s 28ms/step - loss: 1.1389\n",
      "Epoch 14/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.1242\n",
      "Epoch 15/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.1095\n",
      "Epoch 16/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.0961\n",
      "Epoch 17/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.0829\n",
      "Epoch 18/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.0701\n",
      "Epoch 19/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.0580\n",
      "Epoch 20/20\n",
      "424/424 [==============================] - 12s 27ms/step - loss: 1.0457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1be999b8bb0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True\n",
    "    ))\n",
    "\n",
    "model.fit(ds, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos avaliar o modelo para gerar um novo texto, começando com uma determinada *string* curta. Na próxima seção, definiremos uma função para avaliar o modelo treinado.\n",
    "\n",
    "### Fase de avaliação – geração de novas passagens de texto\n",
    "\n",
    "O modelo RNN que treinamos na seção anterior retorna os logits de tamanho 80 para cada caractere único. Esses logits podem ser facilmente convertidos em probabilidades, por meio da função *softmax*, de que um determinado caractere seja encontrado como o próximo caractere. Para prever o próximo caractere na sequência, basta selecionar o elemento com o valor logit máximo, o que equivale a selecionar o caractere com maior probabilidade.\n",
    "\n",
    "No entanto, em vez de sempre selecionar o caractere com a maior probabilidade, **queremos amostrar (aleatoriamente) das saídas**; caso contrário, o modelo sempre produzirá **o mesmo texto**. O *TensorFlow* já fornece uma função, `tf.random.categorical()`, que podemos usar para extrair amostras aleatórias de uma distribuição categórica. Para ver como isso funciona, vamos gerar algumas amostras aleatórias de três categorias [0, 1, 2], com logits de entrada [1, 1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades: [0.33333334 0.33333334 0.33333334]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 1.0]]\n",
    "\n",
    "print(f\"Probabilidades: {tf.math.softmax(logits=logits).numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 2, 0, 1, 0, 1, 1, 2, 1, 1]], dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "samples = tf.random.categorical(logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, com os logits dados, as categorias têm as mesmas probabilidades (ou seja, categorias equiprováveis). Portanto, se usarmos um tamanho de amostra grande ($\\small num\\_samples \\rightarrow \\infty)$, esperaríamos que o número de ocorrências de cada categoria chegasse a $\\small \\approx \\dfrac{1}{3}$ do tamanho da amostra. Se alterarmos os logits para [1, 1, 3], esperaríamos observar mais ocorrências para a categoria 2 (quando um número muito grande de exemplos é extraído dessa distribuição):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades: [0.10650698 0.10650698 0.78698605]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "logits = [[1.0, 1.0, 3.0]]\n",
    "\n",
    "print(f\"Probabilidades: {tf.math.softmax(logits=logits).numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2, 2, 0, 2, 2, 2, 2, 2, 1, 2]], dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "samples = tf.random.categorical(logits=logits, num_samples=10)\n",
    "tf.print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando `tf.random.categorical`, podemos gerar exemplos com base nos logits calculados pelo nosso modelo. Definimos uma função, `sample()`, que recebe uma *string* inicial curta, `starting_str`, e gera uma nova *string*, `generated_str`, que é inicialmente definida como a *string* de entrada. Em seguida, uma *string* de tamanho `max_input_length` é retirada do final de `generated_str` e codificada em uma sequência de inteiros, `encoded_input`. O `encoded_input` é passado para o modelo RNN para calcular os logits. Observe que a saída do modelo RNN é uma sequência de logits com o mesmo comprimento da sequência de entrada, pois especificamos `return_sequences=True` para a última camada recorrente de nosso modelo RNN. Portanto, cada elemento na saída do modelo RNN representa os logits (aqui, um vetor de tamanho 80, que é o número total de caracteres) para o próximo caractere após observar a sequência de entrada pelo modelo.\n",
    "\n",
    "Aqui, usamos apenas o último elemento dos logits de saída (ou seja, $\\small \\textbf{o}^{(T)}$ ), que é passado para a função `tf.random.categorical()` para gerar uma nova amostra. Essa nova amostra é convertida em um caractere, que é anexado ao final da *string* gerada, `generated_text`, aumentando seu comprimento em 1. Em seguida, esse processo é repetido, pegando o último número de caracteres `max_input_length` do final do `generated_str`, e usando isso para gerar um novo caractere até que o comprimento da *string* gerada atinja o valor desejado. O processo de consumir a sequência gerada como entrada para geração de novos elementos é chamado de `auto-regression` (**auto-regressão**).\n",
    "\n",
    "> #### Retornando sequências como saída\n",
    "> \n",
    "> Você pode se perguntar por que usamos `return_sequences=True` quando usamos apenas o último caractere para amostrar um novo caractere e ignoramos o restante da saída. Embora essa pergunta faça todo o sentido, você não deve esquecer que usamos toda a sequência de saída para treinamento. A perda é calculada com base em cada previsão na saída e não apenas na última.\n",
    "\n",
    "O código para a função `sample()` é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           max_input_length=40,\n",
    "           scale_factor=1.0):\n",
    "    encoded_input = [char2int[s] for s in starting_str]\n",
    "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input)\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "\n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_indx = tf.random.categorical(\n",
    "            scaled_logits, num_samples=1)\n",
    "        \n",
    "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()    \n",
    "\n",
    "        generated_str += str(char_array[new_char_indx])\n",
    "        \n",
    "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
    "        encoded_input = tf.concat(\n",
    "            [encoded_input, new_char_indx],\n",
    "            axis=1)\n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora gerar algum texto novo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was extremely stick.”\n",
      "\n",
      "It was somewhat lasted if Cyrus Harding, tried. Sometimes even doubtless--act, so as to save\n",
      "that startline misnough to see her as Australia. While earth mingled with more vapor at the ear.\n",
      "\n",
      "An\n",
      "hour very speedy,” said the engineer; “I will disappear a man’s\n",
      "friends?” asked Pencroft; “let us wasly, untilly\n",
      "then allowed some escaped by Pencroft. Perhaps men in a whire as heart, in lay our quantity of a desert seconds, we may not see that protection and down the\n",
      "name\n",
      "of the \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, o modelo gera principalmente palavras corretas e, em alguns casos, as frases são **parcialmente significativas**. Você pode ajustar ainda mais os parâmetros de treinamento, como o comprimento das sequências de entrada para treinamento, a arquitetura do modelo e os parâmetros de amostragem (como `max_input_length`).\n",
    "\n",
    "Além disso, para controlar a previsibilidade das amostras geradas (ou seja, gerar texto seguindo os padrões aprendidos do texto de treinamento versus adicionar mais aleatoriedade), os logits calculados pelo modelo RNN podem ser dimensionados antes de serem passados para `tf.random.categorical()` para amostragem. O fator de escala, $\\small \\alpha$ , pode ser interpretado como o inverso da temperatura na física. Temperaturas mais altas resultam em mais aleatoriedade versus comportamento mais previsível em temperaturas mais baixas. Ao escalonar os logits com $\\small \\alpha < 1$, as probabilidades calculadas pela função `softmax` se tornam mais uniformes, conforme mostrado no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades antes de dimensionar:           [0.10650698 0.10650698 0.78698604]\n",
      "Probabilidades depois de dimensionar com 0.5:  [0.21194156 0.21194156 0.57611688]\n",
      "Probabilidades depois de dimensionar com 0.1:  [0.31042377 0.31042377 0.37915245]\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([[1.0, 1.0, 3.0]])\n",
    "print(f\"Probabilidades antes de dimensionar:           {tf.math.softmax(logits).numpy()[0]}\")\n",
    "print(f\"Probabilidades depois de dimensionar com 0.5:  {tf.math.softmax(0.5*logits).numpy()[0]}\")\n",
    "print(f\"Probabilidades depois de dimensionar com 0.1:  {tf.math.softmax(0.1*logits).numpy()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, dimensionar os logits por $\\small \\alpha = 0.1$ resulta em probabilidades quase uniformes [0.31, 0.31, 0.38]. Agora, podemos comparar o texto gerado com $\\small \\alpha = 2.0$ e $\\small \\alpha = 0.5$, conforme mostrado nos seguintes pontos:\n",
    "\n",
    "* $\\small \\alpha = 2.0 \\rightarrow mais\\: previsível$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was extremely stick. The wall was obliged to conceal that they were to be feared that the car before the work had at the same time a patience, since the engineer had been saved an hour of carbonize with lost in the corral which were some death the stranger was going on the coast, and the colonists had at the forest, and it was became somewhat even a sailor in a signal of the colonists, and some accomplished with\n",
      "her course and exploring even destruction. The prosper rapidly left the corral and \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\small \\alpha = 0.5 \\rightarrow mais\\: aleatório$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was egely slept information floated new, “saying Curst. no,\n",
      "my, Harding,--“To case to do?” Conf-blaes. Copse. he Evidopoh gaz;\n",
      "unfem into try,\n",
      "sin heart; Ayrtony, Jup’,-sieally me of Mircla!”\n",
      "\n",
      "The anziatles--thress couple of fury:\n",
      "oxion five fires, recies againg.\n",
      "Haster\n",
      "Neb! Therefore, nized had no freechtellors in? little obscut,\n",
      "of feel? Very meniaga wand Affilitify every mater-yaryly woubly.\n",
      "Acasivilay puwprately become any equal furn, acgo! three\n",
      "furth, wibly swar. It emlish, somber\n",
      "to Purg\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados mostram que dimensionar os logits com $\\small \\alpha = 0.5$ (aumentando a temperatura) gera mais texto aleatório. Há uma compensação entre a novidade do texto gerado e sua correção.\n",
    "\n",
    "Nesta seção, trabalhamos com geração de texto em nível de caractere, que é uma tarefa de modelagem de sequência a sequência (seq2seq). Embora este exemplo possa não ser muito útil por si só, é fácil pensar em várias aplicações úteis para esses tipos de modelos; por exemplo, um modelo RNN semelhante pode ser treinado como um *chatbot* para auxiliar os usuários com consultas simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendendo a linguagem com o modelo *Transformer*\n",
    "\n",
    "Resolvemos até agora dois problemas de modelagem de sequência usando NNs baseados em RNN. No entanto, surgiu recentemente uma nova arquitetura que demonstrou superar os modelos seq2seq baseados em RNN em várias tarefas de PNL.\n",
    "\n",
    "É chamada de arquitetura *Transformer*, capaz de modelar dependências globais entre sequências de entrada e saída, e foi introduzida em 2017 por Ashish Vaswani, et. al., no artigo do NeurIPS Attention Is All You Need (disponível online em http://papers.nips.cc/paper/7181-attention-is-all-you-need).\n",
    "A arquitetura do *Transformer* é baseada em um conceito chamado atenção e, mais especificamente, **no mecanismo de autoatenção**. Para isso, vamos considerar a tarefa de análise de sentimentos que abordamos anteriormente. Nesse caso, usar o mecanismo de atenção significaria que nosso modelo seria capaz de aprender a se concentrar nas partes de uma sequência de entrada que <u>são mais relevantes para o sentimento</u>.\n",
    "\n",
    "### Entendendo o mecanismo de autoatenção\n",
    "Esta seção explicará o mecanismo de autoatenção e como ele ajuda um modelo *Transformer* a se concentrar em partes importantes de uma sequência para NLP. A primeira subseção cobrirá uma forma muito básica de autoatenção para ilustrar a ideia geral por trás das representações de texto de aprendizagem. Em seguida, adicionaremos diferentes parâmetros de peso para chegarmos ao mecanismo de autoatenção que é comumente usado em modelos de *Transformer*.\n",
    "\n",
    "### Uma versão básica de auto-atenção\n",
    "Para introduzir a ideia básica por trás da autoatenção, vamos supor que temos uma sequência de entrada de comprimento $\\small T$, $\\small \\textbf{x}^{(0)}, \\textbf{x}^{(1)},\\cdots , \\textbf{x}^{(T)}$, bem como uma sequência de saída, $\\small \\textbf{o}^{(0)}, \\textbf{o}^{(1)}, \\cdots,\\textbf{o}^{(T)}$. Cada elemento dessas sequências, $\\small \\textbf{x}^{(T)}$ e $\\small \\textbf{o}^{(T)}$ , são vetores de tamanho `d` (ou seja, $\\small \\textbf{x}^{(T)} \\in R^d $).\n",
    "Então, para uma tarefa seq2seq, o objetivo da autoatenção é modelar as dependências de cada elemento na sequência de saída para os elementos de entrada. Para isso, os mecanismos de atenção são compostos por três etapas. Em primeiro lugar, derivamos pesos de importância com base na semelhança entre o elemento atual e todos os outros elementos na sequência. Em segundo lugar, normalizamos os pesos, o que geralmente envolve o uso da já familiar função *softmax*. Em terceiro lugar, usamos esses pesos em combinação com os elementos de sequência correspondentes para calcular o valor de atenção.\n",
    "\n",
    "Mais formalmente, a saída da autoatenção é a soma ponderada de todas as sequências de entrada. Por exemplo, para o elemento de entrada $\\small ith$, o valor de saída correspondente é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{o}^{(i)} = \\sum^T_{j=0}\\textbf{W}_{ij}\\textbf{x}^{(j)}\n",
    "$$\n",
    "\n",
    "\n",
    "Aqui, os pesos, $\\small W_{ij}$, são calculados com base na semelhança entre o elemento de entrada atual, $\\small \\textbf{x}^{(i)}$ e todos os outros elementos na sequência de entrada. Mais concretamente, essa semelhança é calculada como o produto escalar entre o elemento de entrada atual, $\\small \\textbf{x}^{(i)}$ , e outro elemento na sequência de entrada, $\\small \\textbf{x}^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\small \\omega_{ij} = \\textbf{x}^{(i)^T}  \\textbf{x}^{(j)}\n",
    "$$\n",
    "\n",
    "Depois de calcular esses pesos baseados em similaridade para a entrada $\\small i$ e todas as entradas na sequência ($\\small \\textbf{x}^{(i)}$ a $\\small \\textbf{x}^{(T)}$), os pesos \"brutos\" ($\\omega_{i0}$ a $\\omega_{iT}$) são então normalizados usando a função softmax familiar, como segue :\n",
    "\n",
    "$$\n",
    "\\small W_{ij} = \\dfrac{exp(\\omega_{ij})}{\\sum ^T_{j=0 } exp(\\omega_{ij})} = softmax([\\omega_{ij}]_{j=0\\cdots T})\n",
    "$$\n",
    "\n",
    "Observe que, como consequência da aplicação da função softmax, os pesos serão somados para 1 após essa normalização, ou seja,\n",
    "$$\n",
    "\\small \\sum^T_{j=0}W_{ij} = 1\n",
    "$$\n",
    "\n",
    "Para recapitular, vamos resumir as três principais etapas por trás da operação de autoatenção:\n",
    "1. Para um dado elemento de entrada, $\\small \\textbf{x}^{(i)}$, e cada $\\small j$ésimo elemento no intervalo [0, T], calcule o produto escalar, $\\small \\textbf{x}^{(i)^T}$, $\\small \\textbf{x}^{(j)}$\n",
    "2. Obtenha o peso, $\\small W_{ij}$, normalizando os produtos escalares usando a função *softmax*\n",
    "3. Calcule a saída, $\\small \\textbf{o}^{(i)}$ , como a soma ponderada sobre toda a entrada sequência:\n",
    "$$\n",
    "\\small \\textbf{o}^{(i)} = \\sum^T_{j=0} W_{ij}\\textbf{x}^{(j)}\n",
    "$$\n",
    "\n",
    "Essas etapas são melhor ilustradas na figura a seguir:\n",
    "\n",
    "![](imagens\\mecanismo_atencao.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrização do mecanismo de autoatenção com pesos de consulta, chave e valor\n",
    "Agora que você foi apresentado ao conceito básico por trás da autoatenção, esta subseção resume o mecanismo de autoatenção mais avançado usado no modelo *Transformer*. Observe que na subseção anterior, não envolvemos nenhum parâmetro de aprendizagem ao calcular as saídas. Portanto, se quisermos aprender um modelo de linguagem e quisermos alterar os valores de atenção para otimizar um objetivo, como minimizar o erro de classificação, precisaremos alterar os *embeddings* de palavras (ou seja, vetores de entrada) subjacentes a cada elemento de entrada, $x^{(i)}$. Em outras palavras, usando o mecanismo básico de autoatenção introduzido anteriormente, o modelo *Transformer* é bastante limitado em relação a como ele pode atualizar ou alterar os valores de atenção durante a otimização do modelo para uma determinada sequência. Para tornar o mecanismo de autoatenção mais flexível e passível de otimização do modelo, apresentaremos três matrizes de peso adicionais que podem ser ajustadas como parâmetros do modelo durante o treinamento do modelo. Denotamos essas três matrizes de peso como $\\small  U_q$, $\\small  U_k$ e $\\small  U_v$. Eles são usados ​​para projetar as entradas em elementos de sequência de $\\small query$ (consulta), $\\small key$ (chave) e $\\small value$(valor):\n",
    "* Sequência de $\\small query$: $\\small q^{(i)} = U_qx^{(i)} \\: para \\: i \\in [0,T]$,\n",
    "* Sequência de $\\small key$: $\\small k^{(i)} = U_kx^{(i)}  \\: para \\: i \\in [0,T]$,\n",
    "* Sequência de $\\small value$: $\\small v^{(i)} = U_vx^{(i)}  \\: para \\: i \\in [0,T]$\n",
    "\n",
    "Aqui, ambos $\\small q^{(i)}$ e $\\small k^{(i)}$ são vetores de tamanho $\\small d_{k}$ . Portanto, as matrizes de projeção $\\small U_q$ e $\\small U_k$ têm a forma $\\small d_k \\times d$, enquanto $\\small U_v$ tem a forma $\\small d_v \\times d$. Por simplicidade, podemos projetar esses vetores para ter a mesma forma, por exemplo, usando $\\small m = d_k = d_v$. Agora, em vez de calcular o peso não normalizado como o produto escalar de pares entre o elemento de sequência de entrada fornecido, $\\small x^{(i)}$, e o elemento de sequência $\\small j$-ésimo, $\\small x^{(j)}$ , podemos calcular o produto escalar entre a consulta e a chave:\n",
    "\n",
    "$$\n",
    "\\small \\omega_{ij} = q^{(i)^T} k^{(j)}\n",
    "$$\n",
    "\n",
    "Podemos então usar `m`, ou, mais precisamente, $\\small 1 / \\sqrt{m}$,  para escalar $\\small \\omega_{ij}$ antes de normalizá-lo através da função *softmax*, como segue:\n",
    "\n",
    "$$\n",
    "\\small W_{ij} = softmax\\left ( \\dfrac{\\omega_{ij}}{\\sqrt{m}} \\right )\n",
    "$$\n",
    "\n",
    "Observe que dimensionar $\\small \\omega_{ij}$  por $\\small 1/\\sqrt{m}$ garantirá que o comprimento euclidiano dos vetores de peso esteja aproximadamente no mesmo intervalo.\n",
    "\n",
    "### Atenção multi-cabeças e o bloco *Transformer*\n",
    "Outro truque que melhora muito o poder discriminatório do mecanismo de autoatenção é a **multi-head atention** (atenção multicabeças)(MHA), que combina várias operações de autoatenção. Nesse caso, cada mecanismo de autoatenção é chamado de *head*, que pode ser calculada em paralelo. Usando `r` *head* paralelas, cada *head* resulta em um vetor, `h`, de tamanho *m*. Esses vetores são então concatenados para obter um vetor, *z*, com a forma $r \\times m$. Finalmente, o vetor concatenado é projetado usando a matriz de saída $W^o$ para obter a saída final, como segue:\n",
    "$$\n",
    "\\small o^{(i)} = W^o_{ij}z\n",
    "$$\n",
    "\n",
    "A arquitetura de um bloco Transformer é mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\transformer_block.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que na arquitetura do *Transformer* mostrada na figura anterior, adicionamos dois componentes adicionais que ainda não discutimos. Um desses componentes é a *residual conection* (conexão residual), que adiciona a saída de uma camada (ou mesmo um grupo de camadas) à sua entrada, ou seja, **x** + camada(**x**). O bloco que consiste em uma camada (ou várias camadas) com essa conexão residual é chamado de bloco residual. O bloco *Transformer* mostrado na figura anterior possui dois blocos residuais.\n",
    "\n",
    "O outro novo componente é a ***layer normalization*** (normalização de camada), indicada na figura anterior como \"Norma de camada\". Há uma família de camadas de normalização, incluindo normalização em lote. Por enquanto, você pode pensar na normalização de camada como uma maneira sofisticada ou mais avançada de normalizar ou dimensionar as entradas e ativações NN em cada camada.\n",
    "\n",
    "Voltando à ilustração do modelo *Transformer* na figura anterior, vamos agora discutir como esse modelo funciona. Primeiro, a sequência de entrada é passada para as camadas *MHA*, que são baseadas no mecanismo de autoatenção que discutimos anteriormente. Além disso, as sequências de entrada são adicionadas à saída das camadas *MHA* por meio das conexões residuais - isso garante que as camadas anteriores recebam sinais de gradiente suficientes durante o treinamento, que é um truque comum usado para melhorar a velocidade e a convergência do treinamento. Depois que as sequências de entrada são adicionadas à saída das camadas *MHA*, as saídas são normalizadas por meio da normalização da camada. Esses sinais normalizados passam então por uma série de camadas *MLP* (ou seja, totalmente conectadas), que também possuem uma conexão residual. Finalmente, a saída do bloco residual é normalizada novamente e retornada como a sequência de saída, que pode ser usada para classificação ou geração de sequência.\n",
    "\n",
    "As instruções para implementação e treinamento de modelos *Transformer* foram omitidas para economizar espaço. No entanto, o leitor interessado pode encontrar uma excelente implementação e passo a passo na documentação oficial do TensorFlow em https://www.tensorflow.org/tutorials/text/transformer."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1870f1194fb5b3d43b6d32e845741389586dbe9c4e1e45e17e0f6602cfe22778"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
