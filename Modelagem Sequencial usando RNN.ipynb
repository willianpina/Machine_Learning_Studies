{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelagem de dados sequenciais usando redes neurais recorrentes</h1>\n",
    "\n",
    "<p align=center><img src=https://datascience.eu/wp-content/uploads/2020/05/image-513-1024x347.png></p>\n",
    "\n",
    "Tivemos oportunidade de focar em redes neurais convolucionais (*CNNs*), de forma a cobrir os blocos de construÃ§Ã£o das arquiteturas *CNN* e como implementar *CNNs* profundas no *TensorFlow*. Por fim, vocÃª aprendeu a usar *CNNs* para classificaÃ§Ã£o de imagens.\n",
    "\n",
    "Aqui, exploraremos as redes neurais recorrentes (*RNNs*) e veremos sua aplicaÃ§Ã£o na modelagem de dados sequenciais.\n",
    "\n",
    "### Apresentando dados sequenciais\n",
    "Vamos comeÃ§ar nossa discussÃ£o sobre *RNNs* observando a natureza dos dados sequenciais, que sÃ£o mais comumente conhecidos como dados de sequÃªncia ou **sequÃªncias**. Vamos dar uma olhada nas propriedades Ãºnicas das sequÃªncias que as tornam diferentes de outros tipos de dados. Veremos entÃ£o como podemos representar dados sequenciais e explorar as vÃ¡rias categorias de modelos para dados sequenciais, que sÃ£o baseados na entrada e saÃ­da de um modelo. Isso nos ajudarÃ¡ a explorar a relaÃ§Ã£o entre *RNNs* e sequÃªncias.\n",
    "\n",
    "### Modelagem de dados sequenciais - a ordem importa\n",
    "\n",
    "O que torna as sequÃªncias Ãºnicas, em comparaÃ§Ã£o com outros tipos de dados, Ã© que os elementos em uma sequÃªncia aparecem em uma determinada ordem e nÃ£o sÃ£o independentes uns dos outros. Algoritmos tÃ­picos de aprendizado de mÃ¡quina para aprendizado supervisionado pressupÃµem que a entrada de dados Ã© **independente e e distribuÃ­da de forma idÃªntica (IID)**, o que significa que os exemplos de treinamento sÃ£o `mutuamente independentes` e tÃªm a mesma distribuiÃ§Ã£o subjacente.\n",
    "\n",
    "Nesse sentido, com base na suposiÃ§Ã£o de independÃªncia mÃºtua, a ordem em que os exemplos de treinamento sÃ£o dados ao modelo Ã© **irrelevante**. Por exemplo, se tivermos uma amostra composta por n exemplos de treinamento, $\\small x^{(1)}, x^{(2)},\\cdots, x^{(n)} $, a ordem em que usamos os dados para treinar nosso algoritmo de aprendizado de mÃ¡quina nÃ£o importa. Um exemplo desse cenÃ¡rio seria o conjunto de dados Iris, muito conhecido. No conjunto de dados Iris, cada flor foi medida independentemente e as medidas de uma flor nÃ£o influenciam as medidas de outra flor.\n",
    "\n",
    "No entanto, essa suposiÃ§Ã£o nÃ£o Ã© vÃ¡lida quando lidamos com sequÃªncias â€“ por definiÃ§Ã£o, **a ordem Ã© importante**. Prever o valor de mercado de uma determinada aÃ§Ã£o seria um exemplo desse cenÃ¡rio. Por exemplo, suponha que temos uma amostra de n exemplos de treinamento, onde cada exemplo de treinamento representa o valor de mercado de uma determinada aÃ§Ã£o em um determinado dia. Se nossa tarefa Ã© prever o valor do mercado de aÃ§Ãµes para os prÃ³ximos trÃªs dias, faria sentido considerar os preÃ§os das aÃ§Ãµes anteriores em uma ordem de data para derivar tendÃªncias, em vez de utilizar esses exemplos de treinamento em uma ordem aleatÃ³ria.\n",
    "\n",
    "> #### Dados sequenciais versus dados de sÃ©ries temporais\n",
    "> Os dados de sÃ©rie temporal sÃ£o um tipo especial de dados sequenciais, em que cada exemplo estÃ¡ associado a uma dimensÃ£o de tempo. Em dados de sÃ©ries temporais, as amostras sÃ£o coletadas em *timestamps* sucessivos e, portanto, a dimensÃ£o de tempo determina a ordem entre os pontos de dados. Por exemplo, preÃ§os de aÃ§Ãµes e registros de voz ou fala sÃ£o dados de sÃ©ries temporais.\n",
    ">\n",
    "> Por outro lado, nem todos os dados sequenciais tÃªm a dimensÃ£o temporal, por exemplo, dados de texto ou sequÃªncias de DNA, onde os exemplos sÃ£o ordenados, mas nÃ£o se qualificam como dados de sÃ©ries temporais. Como vocÃª verÃ¡, abordaremos alguns exemplos de Processamento de Linguagem Natural (NLP) e modelagem de texto que nÃ£o sÃ£o dados de sÃ©rie temporal, mas observe que as *RNNs* tambÃ©m podem ser usados para dados de sÃ©rie temporal.\n",
    "\n",
    "### Representando sequÃªncias\n",
    "\n",
    "Estabelecemos que a ordem entre os pontos de dados Ã© importante em dados sequenciais, entÃ£o precisamos encontrar uma maneira de aproveitar essas informaÃ§Ãµes de pedido em um modelo de aprendizado de mÃ¡quina. Ao longo das explicaÃ§Ãµes, representaremos as sequÃªncias como $\\small \\left \\langle x^{(1)},x^{(2)},\\cdots, x^{(T)}  \\right \\rangle$ . Os Ã­ndices sobrescritos indicam a ordem das instÃ¢ncias e o comprimento da sequÃªncia Ã© $\\small T$. Para um exemplo sensato de sequÃªncias, considere dados de sÃ©ries temporais, onde cada ponto de exemplo, $\\small x^{(T)}$, pertence a um determinado tempo, $\\small t$. A figura a seguir mostra um exemplo de dados de sÃ©rie temporal em que os recursos de entrada ($\\small x$) e os rÃ³tulos de destino ($\\small y$) seguem naturalmente a ordem de acordo com seu eixo de tempo; portanto, ambos os `x` e `y` sÃ£o sequÃªncias:\n",
    "\n",
    "![](imagens\\sequencias.PNG)\n",
    "\n",
    "Como jÃ¡ mencionamos, os modelos de rede neural padrÃ£o (*RN*) que abordamos atÃ© agora, como o *multilayer perceptron* (MLP) e as *CNNs* para dados de imagem, assumem que os exemplos de treinamento sÃ£o independentes uns dos outros e, portanto, nÃ£o incorporam **informaÃ§Ã£o de ordenamento**. Podemos dizer que tais modelos nÃ£o possuem **memÃ³ria** de exemplos de treinamento vistos anteriormente. Por exemplo, as amostras sÃ£o passadas pelas etapas de *feedforward* e *backpropagation* e os pesos sÃ£o atualizados independentemente da ordem em que os exemplos de treinamento sÃ£o processados. As *RNNs*, por outro lado, sÃ£o projetadas para modelar sequÃªncias e sÃ£o capazes de lembrar informaÃ§Ãµes passadas e processar novos eventos de acordo, o que Ã© uma clara vantagem ao trabalhar com dados de sequÃªncia.\n",
    "\n",
    "### As diferentes categorias de modelagem de sequÃªncia\n",
    "\n",
    "A modelagem de sequÃªncia tem muitas aplicaÃ§Ãµes fascinantes, como traduÃ§Ã£o de idiomas (por exemplo, traduÃ§Ã£o de texto de inglÃªs para alemÃ£o), legendas de imagens e geraÃ§Ã£o de texto. No entanto, para escolher uma arquitetura e abordagem apropriadas, temos que entender e ser capazes de distinguir entre essas diferentes tarefas de modelagem de sequÃªncia. A figura a seguir, baseada nas explicaÃ§Ãµes do excelente artigo The Unreasonable Effectiveness of Recurrent Neural Networks, de Andrej Karpathy (http://karpathy.github.io/2015/05/21/rnn-effectiveness/), resume a sequÃªncia mais comum tarefas de modelagem, que dependem das categorias de relacionamento de dados de entrada e saÃ­da:\n",
    "\n",
    "![](imagens\\modelagem_de_sequencia.PNG)\n",
    "\n",
    "Vamos discutir as diferentes categorias de relacionamento entre dados de entrada e saÃ­da, que foram descritas na figura anterior, com mais detalhes. Se nem os dados de entrada nem de saÃ­da representam sequÃªncias, entÃ£o estamos lidando com dados padrÃ£o e podemos simplesmente usar um perceptron multicamada (ou outro modelo de classificaÃ§Ã£o abordado anteriormente) para modelar esses dados. No entanto, se a entrada ou a saÃ­da for uma sequÃªncia, a tarefa de modelagem provavelmente se enquadra em uma destas categorias:\n",
    "* **Muitos para um**: Os dados de entrada sÃ£o uma sequÃªncia, mas a saÃ­da Ã© um vetor de tamanho fixo ou escalar, nÃ£o uma sequÃªncia. Por exemplo, na anÃ¡lise de sentimentos, a entrada Ã© baseada em texto (por exemplo, uma resenha de filme) e a saÃ­da Ã© um rÃ³tulo de classe (por exemplo, um rÃ³tulo que indica se um revisor gostou do filme).\n",
    "\n",
    "* **Um para muitos**: Os dados de entrada estÃ£o no formato padrÃ£o e nÃ£o em sequÃªncia, mas a saÃ­da Ã© uma sequÃªncia. Um exemplo dessa categoria Ã© a legendagem de imagens â€” a entrada Ã© uma imagem e a saÃ­da Ã© uma frase em inglÃªs que resume o conteÃºdo dessa imagem.\n",
    "\n",
    "* **Muitos para muitos**: As matrizes de entrada e saÃ­da sÃ£o sequÃªncias. Esta categoria pode ser dividida com base na sincronizaÃ§Ã£o da entrada e saÃ­da. Um exemplo de uma tarefa de modelagem sincronizada de muitos para muitos Ã© a classificaÃ§Ã£o de vÃ­deo, onde cada quadro em um vÃ­deo Ã© rotulado. Um exemplo de uma tarefa de modelagem muitos-para-muitos *atrasada* seria traduzir uma linguagem para outra. Por exemplo, uma frase inteira em inglÃªs deve ser lida e processada por uma mÃ¡quina antes que sua traduÃ§Ã£o para o alemÃ£o seja produzida. \n",
    "\n",
    "Agora, depois de resumir as trÃªs grandes categorias de modelagem de sequÃªncia, podemos avanÃ§ar para discutir a estrutura de uma RNN.\n",
    "\n",
    "### RNNs para modelagem de sequÃªncias \n",
    "Nesta seÃ§Ã£o, antes de comeÃ§armos a implementar *RNNs* no *TensorFlow*, discutiremos os principais conceitos de *RNNs*. ComeÃ§aremos examinando a estrutura tÃ­pica de uma *RNN*, que inclui um componente recursivo para modelar dados de sequÃªncia. Em seguida, examinaremos como as ativaÃ§Ãµes dos neurÃ´nios sÃ£o computadas em uma *RNN* tÃ­pica. Isso criarÃ¡ um contexto para discutirmos os desafios comuns no treinamento de *RNNs* e, em seguida, discutiremos soluÃ§Ãµes para esses desafios, como *LSTM* e unidades recorrentes fechadas (*GRUs*).\n",
    "\n",
    "### Entendendo o mecanismo de loop *RNN*\n",
    "Vamos comeÃ§ar com a arquitetura de uma *RNN*. A figura a seguir mostra uma *RN* *feedforward* padrÃ£o e um *RNN* lado a lado para comparaÃ§Ã£o:\n",
    "\n",
    "![](imagens\\mecanismo_loop_rnn.PNG)\n",
    "\n",
    "Ambas as redes tÃªm apenas uma camada oculta. Nesta representaÃ§Ã£o, as unidades nÃ£o sÃ£o exibidas, mas assumimos que a camada de entrada ($\\small x$), a camada oculta ($\\small h$) e a camada de saÃ­da ($\\small o$) sÃ£o vetores que contÃªm muitas unidades.\n",
    "\n",
    "> ##### Determinando o tipo de saÃ­da de um RNN\n",
    "> Essa arquitetura RNN genÃ©rica pode corresponder Ã s duas categorias de modelagem de sequÃªncia em que a entrada Ã© uma sequÃªncia. Normalmente, uma camada recorrente pode retornar uma sequÃªncia como saÃ­da, $\\small \\left \\langle o^{(1)},o^{(2)},\\cdots, o^{(T)}  \\right \\rangle$, ou simplesmente retornar a Ãºltima saÃ­da (em $\\small t = T$, ou seja, $\\small o^{(T)}$). Assim, pode ser muitos para muitos ou muitos para um se, por exemplo, usarmos apenas o Ãºltimo elemento, $\\small o^{(T)}$, como a saÃ­da final.\n",
    ">\n",
    "> Como vocÃª verÃ¡ mais tarde, na *API TensorFlow Keras*, o comportamento de uma camada recorrente em relaÃ§Ã£o ao retorno de uma sequÃªncia como saÃ­da ou simplesmente usar a Ãºltima saÃ­da pode ser especificado definindo o argumento `return_sequences` como `True` ou `False`, respectivamente.\n",
    "\n",
    "Em uma rede *feedforward* padrÃ£o, as informaÃ§Ãµes fluem da entrada para a camada oculta e, em seguida, da camada oculta para a camada de saÃ­da. Por outro lado, em uma *RNN*, a camada oculta recebe sua entrada tanto da camada de entrada da etapa de tempo atual quanto da camada oculta da etapa de tempo anterior.\n",
    "\n",
    "O fluxo de informaÃ§Ãµes em etapas de tempo adjacentes na camada oculta permite que a rede tenha uma memÃ³ria de eventos passados. Esse fluxo de informaÃ§Ãµes geralmente Ã© exibido como um *loop*, tambÃ©m conhecido como **recurrent edge** (borda recorrente) em notaÃ§Ã£o de grÃ¡fico, que Ã© como essa arquitetura *RNN* geral recebeu seu nome.\n",
    "\n",
    "Semelhante aos *perceptrons* multicamadas, as *RNN*s podem consistir em vÃ¡rias camadas ocultas. Observe que Ã© uma convenÃ§Ã£o comum se referir Ã s *RNNs* com uma camada oculta como uma *RNN* de camada Ãºnica, que nÃ£o deve ser confundido com as RNs de camada Ãºnica sem uma camada oculta, como *Adaline* ou *regressÃ£o logÃ­stica*. A figura a seguir ilustra uma *RNN* com uma camada oculta (superior) e uma *RNN* com duas camadas ocultas (inferior):\n",
    "\n",
    "![](imagens\\rnn_oculta.PNG)\n",
    "\n",
    "Para examinar a arquitetura das *RNNs* e o fluxo de informaÃ§Ãµes, pode-se desdobrar uma representaÃ§Ã£o compacta com uma aresta recorrente, que vocÃª pode ver na figura anterior.\n",
    "\n",
    "Como sabemos, cada unidade oculta em uma *RN* padrÃ£o recebe apenas uma entrada â€“ a prÃ©-ativaÃ§Ã£o de rede associada Ã  camada de entrada. Em contraste, cada unidade oculta em uma *RNN* recebe dois conjuntos distintos de entrada â€“ a prÃ©-ativaÃ§Ã£o da camada de entrada e a ativaÃ§Ã£o da mesma camada oculta da etapa de tempo anterior, $\\small t â€“ 1$.\n",
    "\n",
    "Na primeira etapa de tempo, $\\small t = 0$, as unidades ocultas sÃ£o inicializadas com zeros ou pequenos valores aleatÃ³rios. EntÃ£o, em um passo de tempo em que $\\small t > 0$, as unidades ocultas recebem sua entrada do ponto de dados no momento atual, $\\small x^{(T)}$, e os valores anteriores das unidades ocultas em $\\small t â€“ 1$, indicados como $\\small h^{(t-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, no caso de uma RNN multicamada, podemos resumir o fluxo de informaÃ§Ãµes da seguinte forma:\n",
    "* $\\small layer$ = 1: Aqui, a camada oculta Ã© representada como $h_1^{(t)}$ e recebe sua entrada do ponto de dados, $x^{(t)}$, e os valores ocultos na mesma camada, mas no passo de tempo anterior, $h_1^{(t - 1)}$.\n",
    "* $\\small layer$ = 2: A segunda camada oculta, $h_2^{(t)}$ , recebe suas entradas das saÃ­das da camada abaixo na etapa de tempo atual $(o_1^{(t)})$ e seus prÃ³prios valores ocultos da etapa de tempo anterior, $h_2^{(t-1)}$.\n",
    "\n",
    "Como, neste caso, cada camada recorrente deve receber uma sequÃªncia como entrada, todas as camadas recorrentes, exceto a Ãºltima, devem retornar uma sequÃªncia como saÃ­da (ou seja, `return_sequences=True`). O comportamento da Ãºltima camada recorrente depende do tipo de problema.\n",
    "\n",
    "### Computando ativaÃ§Ãµes em uma *RNN*\n",
    "Agora que vocÃª entende a estrutura e o fluxo geral de informaÃ§Ãµes em uma *RNN*, vamos ser mais especÃ­ficos e calcular as ativaÃ§Ãµes reais das camadas ocultas, bem como a camada de saÃ­da. Para simplificar, consideraremos apenas uma Ãºnica camada oculta; no entanto, o mesmo conceito se aplica Ã s *RNNs* multicamadas.\n",
    "\n",
    "Cada aresta direcionada (as conexÃµes entre caixas) na representaÃ§Ã£o de uma *RNN* que acabamos de ver estÃ¡ associada a uma matriz de pesos. Esses pesos nÃ£o dependem do tempo, $\\small t$; portanto, eles sÃ£o compartilhados ao longo do eixo do tempo. As diferentes matrizes de peso em uma *RNN* de camada Ãºnica sÃ£o as seguintes:\n",
    "* $\\small \\textbf{W}_{xh}$: A matriz de peso entre a entrada, $\\small x^{(t)}$, e a camada oculta, $\\small \\textbf{h}$\n",
    "* $\\small \\textbf{W}_{hh}$: A matriz de peso associada Ã  aresta recorrente\n",
    "* $\\small \\textbf{W}_{ho}$: A matriz de peso entre a camada oculta e a camada de saÃ­da\n",
    "\n",
    "Essas matrizes de peso sÃ£o representadas na figura a seguir:\n",
    "\n",
    "![](imagens\\matriz_pesos.PNG)\n",
    "\n",
    "Em certas implementaÃ§Ãµes, vocÃª pode observar que as matrizes de peso, $\\small \\textbf{W}_{xh}$ e $\\small \\textbf{W}_{hh}$, sÃ£o concatenadas a uma matriz combinada, $\\small \\textbf{W}_{h} = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$. Mais adiante, faremos uso dessa notaÃ§Ã£o tambÃ©m.\n",
    "\n",
    "A computaÃ§Ã£o das ativaÃ§Ãµes Ã© muito semelhante aos *perceptrons* multicamadas padrÃ£o e outros tipos de *RNs* *feedforward*. Para a camada oculta, a entrada lÃ­quida, $\\small \\textbf{z}_ h$ (prÃ©-ativaÃ§Ã£o), Ã© calculada atravÃ©s de uma combinaÃ§Ã£o linear, ou seja, calculamos a soma das multiplicaÃ§Ãµes das matrizes de peso com os vetores correspondentes e somamos a unidade de polarizaÃ§Ã£o:\n",
    "$$\n",
    "\\small \\textbf{z}_h^{(t)} = \\text{W}_{xh} \\textbf{x}^{(t)} + \\textbf{W}_{hh}\\textbf{h}^{(t-1)} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "EntÃ£o, as ativaÃ§Ãµes das unidades ocultas na etapa do tempo, $\\small t$, sÃ£o calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^{(t)} = \\phi_h (\\textbf{z}_h^{(t)}) = \\phi_h (\\textbf{W}_{xh}\\textbf{x}^{(t)} +  \\textbf{W}_{hh}\\textbf{x}^{(t-1)} + \\textbf{b}_h)\n",
    "$$\n",
    " \n",
    "Aqui, $\\small \\textbf{b}_h$ Ã© o vetor de polarizaÃ§Ã£o para as unidades ocultas e $\\phi_h(\\cdot)$ Ã© a funÃ§Ã£o de ativaÃ§Ã£o da camada oculta.\n",
    "\n",
    "Caso vocÃª queira usar a matriz de peso concatenada, $\\small \\textbf{W}_h = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$, a fÃ³rmula para calcular as unidades ocultas mudarÃ¡ da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\phi_h \\left ([\\textbf{W}_{xh}; \\textbf{W}_{hh}] \\begin{bmatrix}\n",
    "\\textbf{x}^{(t)}\\\\ \n",
    "\\textbf{h}^{t-1}\n",
    "\\end{bmatrix}       \n",
    "+ \\textbf{b}_h          \\right )\n",
    "$$\n",
    "\n",
    "Uma vez computadas as ativaÃ§Ãµes das unidades ocultas no passo de tempo atual, serÃ£o computadas as ativaÃ§Ãµes das unidades de saÃ­da, como segue:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{o}^{(t)} = \\phi_0 (\\textbf{W}_{ho}\\textbf{h}^{(t)} + \\textbf{b}_0)\n",
    "$$\n",
    "\n",
    "Para ajudar a esclarecer ainda mais, a figura a seguir mostra o processo de cÃ¡lculo dessas ativaÃ§Ãµes com ambas as formulaÃ§Ãµes:\n",
    "\n",
    "![](imagens\\matriz_sequencial.PNG)\n",
    "\n",
    "> ##### Treinando RNNs usando retropropagaÃ§Ã£o ao longo do tempo (BPTT)\n",
    "> O algoritmo de aprendizado para RNNs foi introduzido em 1990: Backpropagation Through Time: What It Does and How to Do It (Paul Werbos, Proceedings of IEEE, 78(10): 1550-1560, 1990). A derivada dos gradientes pode ser um pouco complicada, mas a ideia bÃ¡sica Ã© que a perda total, $\\small L$, Ã© a soma de todas as funÃ§Ãµes de perda nos momentos $\\small t = 1$ a $\\small t = T$:\n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^{(t)}\n",
    "$$\n",
    "\n",
    "Como a perda no tempo $\\small t$ depende das unidades ocultas em todos os passos de tempo anteriores $\\small 1 : t$, o gradiente serÃ¡ calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{W}_{hh}} = \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{o}^{(t)}} \\times \\dfrac{\\partial \\textbf{o}^{(t)}}{\\partial \\textbf{h}^{(t)}} \\times \\left ( \\sum^t_{k=1}\\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}} \\times \\dfrac{\\partial \\textbf{h}^{(k)}}{\\partial \\textbf{W}_{hh}} \\right )\n",
    "\n",
    "$$\n",
    "\n",
    "Aqui, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ Ã© calculado como uma multiplicaÃ§Ã£o de passos de tempo adjacentes:\n",
    "$$\n",
    "\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{k}} = \\prod ^t_{i=k+1} \\dfrac{\\partial \\textbf{h}^{(i)}}{\\partial \\textbf{h}^{(i-1)}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecorrÃªncia oculta versus recorrÃªncia de saÃ­da\n",
    "AtÃ© agora, vocÃª viu redes recorrentes nas quais a camada oculta tem a propriedade recorrente. No entanto, observe que existe um modelo alternativo em que a conexÃ£o recorrente vem da camada de saÃ­da. Nesse caso, as ativaÃ§Ãµes lÃ­quidas da camada de saÃ­da na etapa de tempo anterior, $\\small \\textbf{o}^{(t-1)}$, podem ser adicionadas de duas maneiras:\n",
    "* Para a camada oculta no passo de tempo atual, $\\small \\textbf{h}^t$ (mostrado na figura a seguir como recorrÃªncia de saÃ­da para oculta)\n",
    "* Para a camada de saÃ­da no passo de tempo atual, $\\small \\textbf{o}^{t}$ (mostrado na figura a seguir como recorrÃªncia de saÃ­da para saÃ­da)\n",
    "\n",
    "![](imagens\\recorrencia_saida.PNG)\n",
    "\n",
    "Conforme mostrado na figura anterior, as diferenÃ§as entre essas arquiteturas podem ser vistas claramente nas conexÃµes recorrentes. Seguindo nossa notaÃ§Ã£o, os pesos associados Ã  conexÃ£o recorrente serÃ£o denotados para a recorrÃªncia oculta para oculta por $\\small \\textbf{W}_{hh}$, para a recorrÃªncia saÃ­da para oculta por $\\small \\textbf{W}_{oh}$, e para a recorrÃªncia saÃ­da para saÃ­da por $\\small \\textbf{W}_{oo}$. Em alguns artigos da literatura, os pesos associados Ã s conexÃµes recorrentes tambÃ©m sÃ£o denotados por $\\small \\textbf{W}_{rec}$.\n",
    "\n",
    "Para ver como isso funciona na prÃ¡tica, vamos calcular manualmente a passagem direta para um desses tipos recorrentes. Usando a *API TensorFlow Keras*, uma camada recorrente pode ser definida por meio da *SimpleRNN*, que Ã© semelhante Ã  recorrÃªncia de saÃ­da para saÃ­da. No cÃ³digo a seguir, criaremos uma camada recorrente do *SimpleRNN* e executaremos uma passagem direta em uma sequÃªncia de entrada de comprimento 3 para calcular a saÃ­da. TambÃ©m calcularemos manualmente a passagem direta e compararemos os resultados com os do *SimpleRNN*. Primeiro, vamos criar a camada e atribuir os pesos para nossos cÃ¡lculos manuais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: (5, 2)\n",
      "W_oo shape: (2, 2)\n",
      "b_h shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=2, use_bias=True, \n",
    "    return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_oo shape:', w_oo.shape)\n",
    "print('b_h shape:', b_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma de entrada para esta camada Ã© `(None, None, 5)`, onde a primeira dimensÃ£o Ã© a dimensÃ£o do lote (usando `None` para tamanho de lote variÃ¡vel), a segunda dimensÃ£o corresponde Ã  sequÃªncia (usando `None` para o comprimento variÃ¡vel da sequÃªncia) e a Ãºltima dimensÃ£o corresponde Ã s caracterÃ­sticas. Observe que definimos `return_sequences=True`, que, para uma sequÃªncia de entrada de comprimento 3, resultarÃ¡ na sequÃªncia de saÃ­da $\\small \\left \\langle \\textbf{o}^{(0)},\\textbf{o}^{(1)}, \\textbf{o}^{(2)} \\right \\rangle$. Caso contrÃ¡rio, ele retornaria apenas a saÃ­da final, $\\textbf{o}^{(2)}$.\n",
    "\n",
    "Agora, vamos chamar a passagem direta no `rnn_layer` e calcular manualmente as saÃ­das em cada passo de tempo e comparÃ¡-las:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[0.41464037 0.96012145]]\n",
      "   Output (manual) : [[0.39240566 0.74433106]]\n",
      "   SimpleRNN output: [0.39240566 0.74433106]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[0.82928073 1.9202429 ]]\n",
      "   Output (manual) : [[0.80116504 0.9912947 ]]\n",
      "   SimpleRNN output: [0.80116504 0.9912947 ]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[1.243921  2.8803642]]\n",
      "   Output (manual) : [[0.95468265 0.9993069 ]]\n",
      "   SimpleRNN output: [0.95468265 0.9993069 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor(\n",
    "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "\n",
    "## output of SimepleRNN:\n",
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(len(x_seq)):\n",
    "    xt = tf.reshape(x_seq[t], (1, 5))\n",
    "    print('Time step {} =>'.format(t))\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h    \n",
    "    print('   Hidden          :', ht.numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_o = out_man[t-1]\n",
    "    else:\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "        \n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.numpy())\n",
    "    print('   SimpleRNN output:'.format(t), output[0][t].numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso cÃ¡lculo direto manual, usamos a funÃ§Ã£o de ativaÃ§Ã£o da tangente hiperbÃ³lica (tanh), uma vez que tambÃ©m Ã© usada na `SimpleRNN` (a ativaÃ§Ã£o padrÃ£o). Como vocÃª pode ver nos resultados impressos, as saÃ­das dos cÃ¡lculos de encaminhamento manual correspondem exatamente Ã  saÃ­da da camada `SimpleRNN` em cada etapa de tempo. Espero que esta tarefa prÃ¡tica tenha esclarecido vocÃª sobre os mistÃ©rios das redes recorrentes.\n",
    "\n",
    "### Os desafios de aprender interaÃ§Ãµes de longo prazo\n",
    "\n",
    "BPTT, que foi brevemente mencionado anteriormente, apresenta alguns novos desafios. Por causa do fator multiplicativo,$\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$, ao calcular os gradientes de uma funÃ§Ã£o de perda, surgem os chamados problemas de **gradiente de fuga e explosÃ£o**. Esses problemas sÃ£o explicados pelos exemplos na figura a seguir, que mostra uma RNN com apenas uma unidade oculta para simplificar:\n",
    "\n",
    "![](imagens\\gradiente_fuga_explosao.PNG)\n",
    "\n",
    "\n",
    "Basicamente, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ tem $\\small t â€“ k$ multiplicaÃ§Ãµes; portanto, multiplicar o peso, $\\small w$, por ele mesmo $\\small t â€“ k$ vezes resulta em um fator, $\\small w^{t-w}$ . Como resultado, se $\\small |w|< 1$, esse fator se torna muito pequeno quando $\\small t â€“ k$ Ã© grande. Por outro lado, se o peso da aresta recorrente for $\\small |w|> 1$ , entÃ£o $w^{t-k}$ se torna muito grande quando $\\small t â€“ k$ Ã© grande. Observe que $\\small t â€“ k$ grande se refere a dependÃªncias de longo alcance. Podemos ver que uma soluÃ§Ã£o ingÃªnua para evitar gradientes de fuga ou explosÃ£o pode ser alcanÃ§ada garantindo $\\small |w| = 1$.\n",
    "\n",
    "Na prÃ¡tica, existem pelo menos trÃªs soluÃ§Ãµes para este problema:\n",
    "* *Gradient clipping*\n",
    "* *TBPTT*\n",
    "* *LSTM*\n",
    "\n",
    "Usando o *Gradient clipping*, especificamos um valor de corte ou limite para os gradientes e atribuÃ­mos esse valor de corte aos valores de gradiente que excedem esse valor. Em contraste, o *TBPTT* simplesmente limita ao nÃºmero de passos de tempo que o sinal pode retropropagar apÃ³s cada passagem para frente. Por exemplo, mesmo que a sequÃªncia tenha 100 elementos ou etapas, podemos retropropagar apenas as 20 etapas de tempo mais recentes.\n",
    "\n",
    "Embora tanto o *Gradient clipping* quanto o *TBPTT* possam resolver o problema do **gradiente explosivo**, o truncamento limita o nÃºmero de etapas que o gradiente pode efetivamente retornar e atualizar adequadamente os pesos. Por outro lado, o *LSTM*, projetado em 1997 por *Sepp Hochreiter* e *JÃ¼rgen Schmidhuber*, tem tido mais sucesso em eliminar e explodir problemas de gradiente ao modelar dependÃªncias de longo alcance por meio de\n",
    "o uso de cÃ©lulas de memÃ³ria. Vamos discutir *LSTM* com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CÃ©lulas de memÃ³ria de longo prazo\n",
    "\n",
    "Como afirmado anteriormente, os *LSTMs* foram introduzidos pela primeira vez para superar o problema do gradiente de fuga. O bloco de construÃ§Ã£o de um *LSTM* Ã© uma cÃ©lula de memÃ³ria, que essencialmente representa ou substitui a camada oculta de RNNs padrÃ£o.\n",
    "\n",
    "Em cada cÃ©lula de memÃ³ria, hÃ¡ uma aresta recorrente que tem o peso desejÃ¡vel, $\\small w = 1$, como discutimos, para superar os problemas de gradiente de fuga e explosÃ£o. Os valores associados a essa borda recorrente sÃ£o chamados coletivamente de estado da cÃ©lula. A estrutura desdobrada de uma cÃ©lula *LSTM* moderna Ã© mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\ltsm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o estado da cÃ©lula do passo de tempo anterior, $\\small \\textbf{C}^{(t-1)}$, Ã© modificado para obter o estado da cÃ©lula no passo de tempo atual, $\\small \\textbf{C}^{(t)}$, sem ser multiplicado diretamente por nenhum fator de peso. O fluxo de informaÃ§Ã£o nesta cÃ©lula de memÃ³ria Ã© controlado por vÃ¡rias unidades de computaÃ§Ã£o (frequentemente chamadas de portas) que serÃ£o descritas aqui. Na figura anterior,$\\small \\bigodot$ refere-se ao produto **elemento-a-elemento** (multiplicaÃ§Ã£o elemento-a-elemento) e $\\small \\bigoplus$ significa s**oma-elemento** (adiÃ§Ã£o elemento-a-elemento). AlÃ©m disso, $\\small \\textbf{x}^{(t)}$ refere-se aos dados de entrada no tempo $\\small t$, e $\\small \\textbf{h}^{(t-1)}$ indica as unidades ocultas no tempo $\\small t â€“ 1$. Quatro caixas sÃ£o indicadas com uma funÃ§Ã£o de ativaÃ§Ã£o, seja a funÃ§Ã£o sigmÃ³ide ($\\small \\sigma$) ou $\\small tanh$, e um conjunto de pesos; essas caixas aplicam uma combinaÃ§Ã£o linear realizando multiplicaÃ§Ãµes de matriz-vetor em suas entradas (que sÃ£o $\\small \\textbf{h}^{(t-1)}$ e $\\small \\textbf{x}^{(t)}$). Essas unidades de computaÃ§Ã£o com funÃ§Ãµes de ativaÃ§Ã£o sigmÃ³ides, cujas unidades de saÃ­da sÃ£o passadas por $\\small \\bigodot$, sÃ£o chamadas de portas.\n",
    "\n",
    "Em uma cÃ©lula *LSTM*, existem trÃªs tipos diferentes de portas, que sÃ£o conhecidas como **porta de esquecimento**, porta de entrada e porta de saÃ­da:\n",
    "\n",
    "* A **porta de esquecimento** ($\\small f_t$) permite que a cÃ©lula de memÃ³ria redefina o estado da cÃ©lula sem crescer indefinidamente. Na verdade, o portÃ£o de esquecimento decide quais informaÃ§Ãµes podem passar e quais informaÃ§Ãµes devem ser suprimidas. Agora, $\\small f_t$ Ã© calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small f_t = \\sigma(\\textbf{W}_{xf}\\textbf{x}^{(t)} + \\textbf{W}_{hf}\\textbf{h}^{(t-1)} + \\textbf{b}_f)\n",
    "$$\n",
    "\n",
    "\n",
    "* A **porta de entrada** ($\\small i_t$) e o **valor candidato** ($\\breve{C}_t$) sÃ£o responsÃ¡veis â€‹â€‹por atualizar o estado da cÃ©lula. Eles sÃ£o calculados da seguinte forma:\n",
    "\n",
    "\n",
    "**PAREI AQUI**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ğ’Šğ’Šğ‘¡ğ‘¡ = ğœğœ(ğ‘¾ğ‘¾ğ‘¥ğ‘¥ğ‘¥ğ‘¥ğ’™ğ’™(ğ‘¡ğ‘¡) + ğ‘¾ğ‘¾â„ğ‘¥ğ‘¥ğ’‰ğ’‰(ğ‘¡ğ‘¡âˆ’1) + ğ’ƒğ’ƒğ‘‘\n",
    "ğ‘ªğ‘ªÌƒğ‘¡ğ‘¡ = tanh(ğ‘¾ğ‘¾ğ‘¥ğ‘¥ğ‘¥ğ‘¥ ğ’™ğ’™(ğ‘¡ğ‘¡) + ğ‘¾ğ‘¾â„ğ‘¥ğ‘¥ ğ’‰ğ’‰(ğ‘¡ğ‘¡âˆ’1) + ğ’ƒğ’ƒğ‘¥ğ‘¥\n",
    "O estado da cÃ©lula no tempo t Ã© calculado da seguinte forma:\n",
    "ğ‘ªğ‘ª(ğ‘¡ğ‘¡) = (ğ‘ªğ‘ª(ğ‘¡ğ‘¡âˆ’1) âŠ™ ğ’‡ğ’‡ğ‘¡ğ‘¡) â¨ (ğ’Šğ’Šğ‘¡ğ‘¡ âŠ™ ğ‘ªğ‘ªÌƒğ‘¡ğ‘¡)\n",
    "â€¢ A porta de saÃ­da (ğ’ğ’ğ‘¡ğ‘¡ ) decide como atualizar os valores das unidades ocultas:\n",
    "\n",
    "ğ’ğ’ğ‘¡ğ‘¡ = ğœğœ(ğ‘¾ğ‘¾ğ‘¥ğ‘¥ğ‘¥ğ‘¥ ğ’™ğ’™(ğ‘¡ğ‘¡) + ğ‘¾ğ‘¾â„ğ‘¥ğ‘¥ğ’‰ğ’‰(ğ‘¡ğ‘¡ğ‘¥) + ğ’ƒğ’ƒğ‘¡\n",
    "\n",
    "Dado isso, as unidades ocultas no passo de tempo atual sÃ£o calculadas da seguinte forma:\n",
    "ğ’‰ğ’‰(ğ‘¡ğ‘¡) = ğ’ğ’ğ‘¡ğ‘¡ âŠ™ tanh(ğ‘ªğ‘ª(ğ‘¡ğ‘¡))\n",
    "A estrutura de uma cÃ©lula LSTM e seus cÃ¡lculos subjacentes podem parecer muito complexos e difÃ­ceis de implementar. No entanto, a boa notÃ­cia Ã© que o TensorFlow jÃ¡ implementou tudo em funÃ§Ãµes otimizadas de wrapper, o que nos permite definir nossas cÃ©lulas LSTM de maneira fÃ¡cil e eficiente. Aplicaremos RNNs e LSTMs a conjuntos de dados do mundo real posteriormente neste capÃ­tulo.\n",
    "\n",
    "Outros modelos RNN avanÃ§ados\n",
    "\n",
    "LSTMs fornecem uma abordagem bÃ¡sica para modelar dependÃªncias de longo alcance em sequÃªncias. No entanto, Ã© importante notar que existem muitas variaÃ§Ãµes de LSTMs descritas na literatura (An Empirical Exploration of Recurrent Network Architectures, Rafal Jozefowicz, Wojciech Zaremba e Ilya Sutskever, Proceedings of ICML, 2342-2350, 2015). TambÃ©m merece destaque uma abordagem mais recente, Gated Recurrent Unit (GRU), proposta em 2014. As GRUs possuem uma arquitetura mais simples que as LSTMs; portanto, eles sÃ£o computacionalmente mais eficientes, enquanto seu desempenho em algumas tarefas, como modelagem de mÃºsica polifÃ´nica, Ã© comparÃ¡vel aos LSTMs. Se vocÃª estiver interessado em aprender mais sobre essas arquiteturas RNN modernas, consulte o artigo Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, de Junyoung Chung e outros, 014\n",
    "(https://arxiv.org/pdf/1412.3555v1.pdf)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c198a0bf143da720db8a1ed2fdc43342371f98b364fdf2ef3c0c72d17a471d7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
