{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelagem de dados sequenciais usando redes neurais recorrentes</h1>\n",
    "\n",
    "<p align=center><img src=https://datascience.eu/wp-content/uploads/2020/05/image-513-1024x347.png></p>\n",
    "\n",
    "Tivemos oportunidade de focar em redes neurais convolucionais (*CNNs*), de forma a cobrir os blocos de construção das arquiteturas *CNN* e como implementar *CNNs* profundas no *TensorFlow*. Por fim, você aprendeu a usar *CNNs* para classificação de imagens.\n",
    "\n",
    "Aqui, exploraremos as redes neurais recorrentes (*RNNs*) e veremos sua aplicação na modelagem de dados sequenciais.\n",
    "\n",
    "### Apresentando dados sequenciais\n",
    "Vamos começar nossa discussão sobre *RNNs* observando a natureza dos dados sequenciais, que são mais comumente conhecidos como dados de sequência ou **sequências**. Vamos dar uma olhada nas propriedades únicas das sequências que as tornam diferentes de outros tipos de dados. Veremos então como podemos representar dados sequenciais e explorar as várias categorias de modelos para dados sequenciais, que são baseados na entrada e saída de um modelo. Isso nos ajudará a explorar a relação entre *RNNs* e sequências.\n",
    "\n",
    "### Modelagem de dados sequenciais - a ordem importa\n",
    "\n",
    "O que torna as sequências únicas, em comparação com outros tipos de dados, é que os elementos em uma sequência aparecem em uma determinada ordem e não são independentes uns dos outros. Algoritmos típicos de aprendizado de máquina para aprendizado supervisionado pressupõem que a entrada de dados é **independente e e distribuída de forma idêntica (IID)**, o que significa que os exemplos de treinamento são `mutuamente independentes` e têm a mesma distribuição subjacente.\n",
    "\n",
    "Nesse sentido, com base na suposição de independência mútua, a ordem em que os exemplos de treinamento são dados ao modelo é **irrelevante**. Por exemplo, se tivermos uma amostra composta por n exemplos de treinamento, $\\small x^{(1)}, x^{(2)},\\cdots, x^{(n)} $, a ordem em que usamos os dados para treinar nosso algoritmo de aprendizado de máquina não importa. Um exemplo desse cenário seria o conjunto de dados Iris, muito conhecido. No conjunto de dados Iris, cada flor foi medida independentemente e as medidas de uma flor não influenciam as medidas de outra flor.\n",
    "\n",
    "No entanto, essa suposição não é válida quando lidamos com sequências – por definição, **a ordem é importante**. Prever o valor de mercado de uma determinada ação seria um exemplo desse cenário. Por exemplo, suponha que temos uma amostra de n exemplos de treinamento, onde cada exemplo de treinamento representa o valor de mercado de uma determinada ação em um determinado dia. Se nossa tarefa é prever o valor do mercado de ações para os próximos três dias, faria sentido considerar os preços das ações anteriores em uma ordem de data para derivar tendências, em vez de utilizar esses exemplos de treinamento em uma ordem aleatória.\n",
    "\n",
    "> #### Dados sequenciais versus dados de séries temporais\n",
    "> Os dados de série temporal são um tipo especial de dados sequenciais, em que cada exemplo está associado a uma dimensão de tempo. Em dados de séries temporais, as amostras são coletadas em *timestamps* sucessivos e, portanto, a dimensão de tempo determina a ordem entre os pontos de dados. Por exemplo, preços de ações e registros de voz ou fala são dados de séries temporais.\n",
    ">\n",
    "> Por outro lado, nem todos os dados sequenciais têm a dimensão temporal, por exemplo, dados de texto ou sequências de DNA, onde os exemplos são ordenados, mas não se qualificam como dados de séries temporais. Como você verá, abordaremos alguns exemplos de Processamento de Linguagem Natural (NLP) e modelagem de texto que não são dados de série temporal, mas observe que as *RNNs* também podem ser usados para dados de série temporal.\n",
    "\n",
    "### Representando sequências\n",
    "\n",
    "Estabelecemos que a ordem entre os pontos de dados é importante em dados sequenciais, então precisamos encontrar uma maneira de aproveitar essas informações de pedido em um modelo de aprendizado de máquina. Ao longo das explicações, representaremos as sequências como $\\small \\left \\langle x^{(1)},x^{(2)},\\cdots, x^{(T)}  \\right \\rangle$ . Os índices sobrescritos indicam a ordem das instâncias e o comprimento da sequência é $\\small T$. Para um exemplo sensato de sequências, considere dados de séries temporais, onde cada ponto de exemplo, $\\small x^{(T)}$, pertence a um determinado tempo, $\\small t$. A figura a seguir mostra um exemplo de dados de série temporal em que os recursos de entrada ($\\small x$) e os rótulos de destino ($\\small y$) seguem naturalmente a ordem de acordo com seu eixo de tempo; portanto, ambos os `x` e `y` são sequências:\n",
    "\n",
    "![](imagens\\sequencias.PNG)\n",
    "\n",
    "Como já mencionamos, os modelos de rede neural padrão (*RN*) que abordamos até agora, como o *multilayer perceptron* (MLP) e as *CNNs* para dados de imagem, assumem que os exemplos de treinamento são independentes uns dos outros e, portanto, não incorporam **informação de ordenamento**. Podemos dizer que tais modelos não possuem **memória** de exemplos de treinamento vistos anteriormente. Por exemplo, as amostras são passadas pelas etapas de *feedforward* e *backpropagation* e os pesos são atualizados independentemente da ordem em que os exemplos de treinamento são processados. As *RNNs*, por outro lado, são projetadas para modelar sequências e são capazes de lembrar informações passadas e processar novos eventos de acordo, o que é uma clara vantagem ao trabalhar com dados de sequência.\n",
    "\n",
    "### As diferentes categorias de modelagem de sequência\n",
    "\n",
    "A modelagem de sequência tem muitas aplicações fascinantes, como tradução de idiomas (por exemplo, tradução de texto de inglês para alemão), legendas de imagens e geração de texto. No entanto, para escolher uma arquitetura e abordagem apropriadas, temos que entender e ser capazes de distinguir entre essas diferentes tarefas de modelagem de sequência. A figura a seguir, baseada nas explicações do excelente artigo The Unreasonable Effectiveness of Recurrent Neural Networks, de Andrej Karpathy (http://karpathy.github.io/2015/05/21/rnn-effectiveness/), resume a sequência mais comum tarefas de modelagem, que dependem das categorias de relacionamento de dados de entrada e saída:\n",
    "\n",
    "![](imagens\\modelagem_de_sequencia.PNG)\n",
    "\n",
    "Vamos discutir as diferentes categorias de relacionamento entre dados de entrada e saída, que foram descritas na figura anterior, com mais detalhes. Se nem os dados de entrada nem de saída representam sequências, então estamos lidando com dados padrão e podemos simplesmente usar um perceptron multicamada (ou outro modelo de classificação abordado anteriormente) para modelar esses dados. No entanto, se a entrada ou a saída for uma sequência, a tarefa de modelagem provavelmente se enquadra em uma destas categorias:\n",
    "* **Muitos para um**: Os dados de entrada são uma sequência, mas a saída é um vetor de tamanho fixo ou escalar, não uma sequência. Por exemplo, na análise de sentimentos, a entrada é baseada em texto (por exemplo, uma resenha de filme) e a saída é um rótulo de classe (por exemplo, um rótulo que indica se um revisor gostou do filme).\n",
    "\n",
    "* **Um para muitos**: Os dados de entrada estão no formato padrão e não em sequência, mas a saída é uma sequência. Um exemplo dessa categoria é a legendagem de imagens — a entrada é uma imagem e a saída é uma frase em inglês que resume o conteúdo dessa imagem.\n",
    "\n",
    "* **Muitos para muitos**: As matrizes de entrada e saída são sequências. Esta categoria pode ser dividida com base na sincronização da entrada e saída. Um exemplo de uma tarefa de modelagem sincronizada de muitos para muitos é a classificação de vídeo, onde cada quadro em um vídeo é rotulado. Um exemplo de uma tarefa de modelagem muitos-para-muitos *atrasada* seria traduzir uma linguagem para outra. Por exemplo, uma frase inteira em inglês deve ser lida e processada por uma máquina antes que sua tradução para o alemão seja produzida. \n",
    "\n",
    "Agora, depois de resumir as três grandes categorias de modelagem de sequência, podemos avançar para discutir a estrutura de uma RNN.\n",
    "\n",
    "### RNNs para modelagem de sequências \n",
    "Nesta seção, antes de começarmos a implementar *RNNs* no *TensorFlow*, discutiremos os principais conceitos de *RNNs*. Começaremos examinando a estrutura típica de uma *RNN*, que inclui um componente recursivo para modelar dados de sequência. Em seguida, examinaremos como as ativações dos neurônios são computadas em uma *RNN* típica. Isso criará um contexto para discutirmos os desafios comuns no treinamento de *RNNs* e, em seguida, discutiremos soluções para esses desafios, como *LSTM* e unidades recorrentes fechadas (*GRUs*).\n",
    "\n",
    "### Entendendo o mecanismo de loop *RNN*\n",
    "Vamos começar com a arquitetura de uma *RNN*. A figura a seguir mostra uma *RN* *feedforward* padrão e um *RNN* lado a lado para comparação:\n",
    "\n",
    "![](imagens\\mecanismo_loop_rnn.PNG)\n",
    "\n",
    "Ambas as redes têm apenas uma camada oculta. Nesta representação, as unidades não são exibidas, mas assumimos que a camada de entrada ($\\small x$), a camada oculta ($\\small h$) e a camada de saída ($\\small o$) são vetores que contêm muitas unidades.\n",
    "\n",
    "> ##### Determinando o tipo de saída de um RNN\n",
    "> Essa arquitetura RNN genérica pode corresponder às duas categorias de modelagem de sequência em que a entrada é uma sequência. Normalmente, uma camada recorrente pode retornar uma sequência como saída, $\\small \\left \\langle o^{(1)},o^{(2)},\\cdots, o^{(T)}  \\right \\rangle$, ou simplesmente retornar a última saída (em $\\small t = T$, ou seja, $\\small o^{(T)}$). Assim, pode ser muitos para muitos ou muitos para um se, por exemplo, usarmos apenas o último elemento, $\\small o^{(T)}$, como a saída final.\n",
    ">\n",
    "> Como você verá mais tarde, na *API TensorFlow Keras*, o comportamento de uma camada recorrente em relação ao retorno de uma sequência como saída ou simplesmente usar a última saída pode ser especificado definindo o argumento `return_sequences` como `True` ou `False`, respectivamente.\n",
    "\n",
    "Em uma rede *feedforward* padrão, as informações fluem da entrada para a camada oculta e, em seguida, da camada oculta para a camada de saída. Por outro lado, em uma *RNN*, a camada oculta recebe sua entrada tanto da camada de entrada da etapa de tempo atual quanto da camada oculta da etapa de tempo anterior.\n",
    "\n",
    "O fluxo de informações em etapas de tempo adjacentes na camada oculta permite que a rede tenha uma memória de eventos passados. Esse fluxo de informações geralmente é exibido como um *loop*, também conhecido como **recurrent edge** (borda recorrente) em notação de gráfico, que é como essa arquitetura *RNN* geral recebeu seu nome.\n",
    "\n",
    "Semelhante aos *perceptrons* multicamadas, as *RNN*s podem consistir em várias camadas ocultas. Observe que é uma convenção comum se referir às *RNNs* com uma camada oculta como uma *RNN* de camada única, que não deve ser confundido com as RNs de camada única sem uma camada oculta, como *Adaline* ou *regressão logística*. A figura a seguir ilustra uma *RNN* com uma camada oculta (superior) e uma *RNN* com duas camadas ocultas (inferior):\n",
    "\n",
    "![](imagens\\rnn_oculta.PNG)\n",
    "\n",
    "Para examinar a arquitetura das *RNNs* e o fluxo de informações, pode-se desdobrar uma representação compacta com uma aresta recorrente, que você pode ver na figura anterior.\n",
    "\n",
    "Como sabemos, cada unidade oculta em uma *RN* padrão recebe apenas uma entrada – a pré-ativação de rede associada à camada de entrada. Em contraste, cada unidade oculta em uma *RNN* recebe dois conjuntos distintos de entrada – a pré-ativação da camada de entrada e a ativação da mesma camada oculta da etapa de tempo anterior, $\\small t – 1$.\n",
    "\n",
    "Na primeira etapa de tempo, $\\small t = 0$, as unidades ocultas são inicializadas com zeros ou pequenos valores aleatórios. Então, em um passo de tempo em que $\\small t > 0$, as unidades ocultas recebem sua entrada do ponto de dados no momento atual, $\\small x^{(T)}$, e os valores anteriores das unidades ocultas em $\\small t – 1$, indicados como $\\small h^{(t-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, no caso de uma RNN multicamada, podemos resumir o fluxo de informações da seguinte forma:\n",
    "* $\\small layer$ = 1: Aqui, a camada oculta é representada como $h_1^{(t)}$ e recebe sua entrada do ponto de dados, $x^{(t)}$, e os valores ocultos na mesma camada, mas no passo de tempo anterior, $h_1^{(t - 1)}$.\n",
    "* $\\small layer$ = 2: A segunda camada oculta, $h_2^{(t)}$ , recebe suas entradas das saídas da camada abaixo na etapa de tempo atual $(o_1^{(t)})$ e seus próprios valores ocultos da etapa de tempo anterior, $h_2^{(t-1)}$.\n",
    "\n",
    "Como, neste caso, cada camada recorrente deve receber uma sequência como entrada, todas as camadas recorrentes, exceto a última, devem retornar uma sequência como saída (ou seja, `return_sequences=True`). O comportamento da última camada recorrente depende do tipo de problema.\n",
    "\n",
    "### Computando ativações em uma *RNN*\n",
    "Agora que você entende a estrutura e o fluxo geral de informações em uma *RNN*, vamos ser mais específicos e calcular as ativações reais das camadas ocultas, bem como a camada de saída. Para simplificar, consideraremos apenas uma única camada oculta; no entanto, o mesmo conceito se aplica às *RNNs* multicamadas.\n",
    "\n",
    "Cada aresta direcionada (as conexões entre caixas) na representação de uma *RNN* que acabamos de ver está associada a uma matriz de pesos. Esses pesos não dependem do tempo, $\\small t$; portanto, eles são compartilhados ao longo do eixo do tempo. As diferentes matrizes de peso em uma *RNN* de camada única são as seguintes:\n",
    "* $\\small \\textbf{W}_{xh}$: A matriz de peso entre a entrada, $\\small x^{(t)}$, e a camada oculta, $\\small \\textbf{h}$\n",
    "* $\\small \\textbf{W}_{hh}$: A matriz de peso associada à aresta recorrente\n",
    "* $\\small \\textbf{W}_{ho}$: A matriz de peso entre a camada oculta e a camada de saída\n",
    "\n",
    "Essas matrizes de peso são representadas na figura a seguir:\n",
    "\n",
    "![](imagens\\matriz_pesos.PNG)\n",
    "\n",
    "Em certas implementações, você pode observar que as matrizes de peso, $\\small \\textbf{W}_{xh}$ e $\\small \\textbf{W}_{hh}$, são concatenadas a uma matriz combinada, $\\small \\textbf{W}_{h} = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$. Mais adiante, faremos uso dessa notação também.\n",
    "\n",
    "A computação das ativações é muito semelhante aos *perceptrons* multicamadas padrão e outros tipos de *RNs* *feedforward*. Para a camada oculta, a entrada líquida, $\\small \\textbf{z}_ h$ (pré-ativação), é calculada através de uma combinação linear, ou seja, calculamos a soma das multiplicações das matrizes de peso com os vetores correspondentes e somamos a unidade de polarização:\n",
    "$$\n",
    "\\small \\textbf{z}_h^{(t)} = \\text{W}_{xh} \\textbf{x}^{(t)} + \\textbf{W}_{hh}\\textbf{h}^{(t-1)} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "Então, as ativações das unidades ocultas na etapa do tempo, $\\small t$, são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^{(t)} = \\phi_h (\\textbf{z}_h^{(t)}) = \\phi_h (\\textbf{W}_{xh}\\textbf{x}^{(t)} +  \\textbf{W}_{hh}\\textbf{x}^{(t-1)} + \\textbf{b}_h)\n",
    "$$\n",
    " \n",
    "Aqui, $\\small \\textbf{b}_h$ é o vetor de polarização para as unidades ocultas e $\\phi_h(\\cdot)$ é a função de ativação da camada oculta.\n",
    "\n",
    "Caso você queira usar a matriz de peso concatenada, $\\small \\textbf{W}_h = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$, a fórmula para calcular as unidades ocultas mudará da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\phi_h \\left ([\\textbf{W}_{xh}; \\textbf{W}_{hh}] \\begin{bmatrix}\n",
    "\\textbf{x}^{(t)}\\\\ \n",
    "\\textbf{h}^{t-1}\n",
    "\\end{bmatrix}       \n",
    "+ \\textbf{b}_h          \\right )\n",
    "$$\n",
    "\n",
    "Uma vez computadas as ativações das unidades ocultas no passo de tempo atual, serão computadas as ativações das unidades de saída, como segue:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{o}^{(t)} = \\phi_0 (\\textbf{W}_{ho}\\textbf{h}^{(t)} + \\textbf{b}_0)\n",
    "$$\n",
    "\n",
    "Para ajudar a esclarecer ainda mais, a figura a seguir mostra o processo de cálculo dessas ativações com ambas as formulações:\n",
    "\n",
    "![](imagens\\matriz_sequencial.PNG)\n",
    "\n",
    "> ##### Treinando RNNs usando retropropagação ao longo do tempo (BPTT)\n",
    "> O algoritmo de aprendizado para RNNs foi introduzido em 1990: Backpropagation Through Time: What It Does and How to Do It (Paul Werbos, Proceedings of IEEE, 78(10): 1550-1560, 1990). A derivada dos gradientes pode ser um pouco complicada, mas a ideia básica é que a perda total, $\\small L$, é a soma de todas as funções de perda nos momentos $\\small t = 1$ a $\\small t = T$:\n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^{(t)}\n",
    "$$\n",
    "\n",
    "Como a perda no tempo $\\small t$ depende das unidades ocultas em todos os passos de tempo anteriores $\\small 1 : t$, o gradiente será calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{W}_{hh}} = \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{o}^{(t)}} \\times \\dfrac{\\partial \\textbf{o}^{(t)}}{\\partial \\textbf{h}^{(t)}} \\times \\left ( \\sum^t_{k=1}\\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}} \\times \\dfrac{\\partial \\textbf{h}^{(k)}}{\\partial \\textbf{W}_{hh}} \\right )\n",
    "\n",
    "$$\n",
    "\n",
    "Aqui, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ é calculado como uma multiplicação de passos de tempo adjacentes:\n",
    "$$\n",
    "\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{k}} = \\prod ^t_{i=k+1} \\dfrac{\\partial \\textbf{h}^{(i)}}{\\partial \\textbf{h}^{(i-1)}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrência oculta versus recorrência de saída\n",
    "Até agora, você viu redes recorrentes nas quais a camada oculta tem a propriedade recorrente. No entanto, observe que existe um modelo alternativo em que a conexão recorrente vem da camada de saída. Nesse caso, as ativações líquidas da camada de saída na etapa de tempo anterior, $\\small \\textbf{o}^{(t-1)}$, podem ser adicionadas de duas maneiras:\n",
    "* Para a camada oculta no passo de tempo atual, $\\small \\textbf{h}^t$ (mostrado na figura a seguir como recorrência de saída para oculta)\n",
    "* Para a camada de saída no passo de tempo atual, $\\small \\textbf{o}^{t}$ (mostrado na figura a seguir como recorrência de saída para saída)\n",
    "\n",
    "![](imagens\\recorrencia_saida.PNG)\n",
    "\n",
    "Conforme mostrado na figura anterior, as diferenças entre essas arquiteturas podem ser vistas claramente nas conexões recorrentes. Seguindo nossa notação, os pesos associados à conexão recorrente serão denotados para a recorrência oculta para oculta por $\\small \\textbf{W}_{hh}$, para a recorrência saída para oculta por $\\small \\textbf{W}_{oh}$, e para a recorrência saída para saída por $\\small \\textbf{W}_{oo}$. Em alguns artigos da literatura, os pesos associados às conexões recorrentes também são denotados por $\\small \\textbf{W}_{rec}$.\n",
    "\n",
    "Para ver como isso funciona na prática, vamos calcular manualmente a passagem direta para um desses tipos recorrentes. Usando a *API TensorFlow Keras*, uma camada recorrente pode ser definida por meio da *SimpleRNN*, que é semelhante à recorrência de saída para saída. No código a seguir, criaremos uma camada recorrente do *SimpleRNN* e executaremos uma passagem direta em uma sequência de entrada de comprimento 3 para calcular a saída. Também calcularemos manualmente a passagem direta e compararemos os resultados com os do *SimpleRNN*. Primeiro, vamos criar a camada e atribuir os pesos para nossos cálculos manuais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: (5, 2)\n",
      "W_oo shape: (2, 2)\n",
      "b_h shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=2, use_bias=True, \n",
    "    return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_oo shape:', w_oo.shape)\n",
    "print('b_h shape:', b_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma de entrada para esta camada é `(None, None, 5)`, onde a primeira dimensão é a dimensão do lote (usando `None` para tamanho de lote variável), a segunda dimensão corresponde à sequência (usando `None` para o comprimento variável da sequência) e a última dimensão corresponde às características. Observe que definimos `return_sequences=True`, que, para uma sequência de entrada de comprimento 3, resultará na sequência de saída $\\small \\left \\langle \\textbf{o}^{(0)},\\textbf{o}^{(1)}, \\textbf{o}^{(2)} \\right \\rangle$. Caso contrário, ele retornaria apenas a saída final, $\\textbf{o}^{(2)}$.\n",
    "\n",
    "Agora, vamos chamar a passagem direta no `rnn_layer` e calcular manualmente as saídas em cada passo de tempo e compará-las:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[0.41464037 0.96012145]]\n",
      "   Output (manual) : [[0.39240566 0.74433106]]\n",
      "   SimpleRNN output: [0.39240566 0.74433106]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[0.82928073 1.9202429 ]]\n",
      "   Output (manual) : [[0.80116504 0.99129474]]\n",
      "   SimpleRNN output: [0.80116504 0.99129474]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[1.243921  2.8803644]]\n",
      "   Output (manual) : [[0.95468265 0.9993069 ]]\n",
      "   SimpleRNN output: [0.95468265 0.9993069 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor(\n",
    "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "\n",
    "## output of SimepleRNN:\n",
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(len(x_seq)):\n",
    "    xt = tf.reshape(x_seq[t], (1, 5))\n",
    "    print('Time step {} =>'.format(t))\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h    \n",
    "    print('   Hidden          :', ht.numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_o = out_man[t-1]\n",
    "    else:\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "        \n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.numpy())\n",
    "    print('   SimpleRNN output:'.format(t), output[0][t].numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso cálculo direto manual, usamos a função de ativação da tangente hiperbólica (tanh), uma vez que também é usada na `SimpleRNN` (a ativação padrão). Como você pode ver nos resultados impressos, as saídas dos cálculos de encaminhamento manual correspondem exatamente à saída da camada `SimpleRNN` em cada etapa de tempo. Espero que esta tarefa prática tenha esclarecido você sobre os mistérios das redes recorrentes.\n",
    "\n",
    "### Os desafios de aprender interações de longo prazo\n",
    "\n",
    "BPTT, que foi brevemente mencionado anteriormente, apresenta alguns novos desafios. Por causa do fator multiplicativo,$\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$, ao calcular os gradientes de uma função de perda, surgem os chamados problemas de **gradiente de fuga e explosão**. Esses problemas são explicados pelos exemplos na figura a seguir, que mostra uma RNN com apenas uma unidade oculta para simplificar:\n",
    "\n",
    "![](imagens\\gradiente_fuga_explosao.PNG)\n",
    "\n",
    "\n",
    "Basicamente, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ tem $\\small t – k$ multiplicações; portanto, multiplicar o peso, $\\small w$, por ele mesmo $\\small t – k$ vezes resulta em um fator, $\\small w^{t-w}$ . Como resultado, se $\\small |w|< 1$, esse fator se torna muito pequeno quando $\\small t – k$ é grande. Por outro lado, se o peso da aresta recorrente for $\\small |w|> 1$ , então $w^{t-k}$ se torna muito grande quando $\\small t – k$ é grande. Observe que $\\small t – k$ grande se refere a dependências de longo alcance. Podemos ver que uma solução ingênua para evitar gradientes de fuga ou explosão pode ser alcançada garantindo $\\small |w| = 1$.\n",
    "\n",
    "Na prática, existem pelo menos três soluções para este problema:\n",
    "* *Gradient clipping*\n",
    "* *TBPTT*\n",
    "* *LSTM*\n",
    "\n",
    "Usando o *Gradient clipping*, especificamos um valor de corte ou limite para os gradientes e atribuímos esse valor de corte aos valores de gradiente que excedem esse valor. Em contraste, o *TBPTT* simplesmente limita ao número de passos de tempo que o sinal pode retropropagar após cada passagem para frente. Por exemplo, mesmo que a sequência tenha 100 elementos ou etapas, podemos retropropagar apenas as 20 etapas de tempo mais recentes.\n",
    "\n",
    "Embora tanto o *Gradient clipping* quanto o *TBPTT* possam resolver o problema do **gradiente explosivo**, o truncamento limita o número de etapas que o gradiente pode efetivamente retornar e atualizar adequadamente os pesos. Por outro lado, o *LSTM*, projetado em 1997 por *Sepp Hochreiter* e *Jürgen Schmidhuber*, tem tido mais sucesso em eliminar e explodir problemas de gradiente ao modelar dependências de longo alcance por meio de\n",
    "o uso de células de memória. Vamos discutir *LSTM* com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Células de memória de longo prazo\n",
    "\n",
    "Como afirmado anteriormente, os *LSTMs* foram introduzidos pela primeira vez para superar o problema do gradiente de fuga. O bloco de construção de um *LSTM* é uma célula de memória, que essencialmente representa ou substitui a camada oculta de RNNs padrão.\n",
    "\n",
    "Em cada célula de memória, há uma aresta recorrente que tem o peso desejável, $\\small w = 1$, como discutimos, para superar os problemas de gradiente de fuga e explosão. Os valores associados a essa borda recorrente são chamados coletivamente de estado da célula. A estrutura desdobrada de uma célula *LSTM* moderna é mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\ltsm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o estado da célula do passo de tempo anterior, $\\small \\textbf{C}^{(t-1)}$, é modificado para obter o estado da célula no passo de tempo atual, $\\small \\textbf{C}^{(t)}$, sem ser multiplicado diretamente por nenhum fator de peso. O fluxo de informação nesta célula de memória é controlado por várias unidades de computação (frequentemente chamadas de portas) que serão descritas aqui. Na figura anterior,$\\small \\bigodot$ refere-se ao produto **elemento-a-elemento** (multiplicação elemento-a-elemento) e $\\small \\bigoplus$ significa s**oma-elemento** (adição elemento-a-elemento). Além disso, $\\small \\textbf{x}^{(t)}$ refere-se aos dados de entrada no tempo $\\small t$, e $\\small \\textbf{h}^{(t-1)}$ indica as unidades ocultas no tempo $\\small t – 1$. Quatro caixas são indicadas com uma função de ativação, seja a função sigmóide ($\\small \\sigma$) ou $\\small tanh$, e um conjunto de pesos; essas caixas aplicam uma combinação linear realizando multiplicações de matriz-vetor em suas entradas (que são $\\small \\textbf{h}^{(t-1)}$ e $\\small \\textbf{x}^{(t)}$). Essas unidades de computação com funções de ativação sigmóides, cujas unidades de saída são passadas por $\\small \\bigodot$, são chamadas de portas.\n",
    "\n",
    "Em uma célula *LSTM*, existem três tipos diferentes de portas, que são conhecidas como **porta de esquecimento**, porta de entrada e porta de saída:\n",
    "\n",
    "* A **porta de esquecimento** ($\\small f_t$) permite que a célula de memória redefina o estado da célula sem crescer indefinidamente. Na verdade, o portão de esquecimento decide quais informações podem passar e quais informações devem ser suprimidas. Agora, $\\small f_t$ é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small f_t = \\sigma(\\textbf{W}_{xf}\\textbf{x}^{(t)} + \\textbf{W}_{hf}\\textbf{h}^{(t-1)} + \\textbf{b}_f)\n",
    "$$\n",
    "\n",
    "\n",
    "* A **porta de entrada** ($\\small i_t$) e o **valor candidato** ($\\breve{C}_t$) são responsáveis ​​por atualizar o estado da célula. Eles são calculados da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small i_t = \\sigma ( \\textbf{W}_{xi}\\textbf{x}^{(t)} +  \\textbf{W}_{hi}\\textbf{h}^{(t-1)} + \\textbf{b}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\small \\breve{C}_t = tanh (\\textbf{W}_{xc}\\textbf{x}^{(t)} + \\textbf{W}_{hc}\\textbf{h}^{(t-1)} + \\textbf{b}_c)\n",
    "$$\n",
    "\n",
    "O estado da célula no tempo t é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "C^{t} = (C^{(t-1)} \\odot f_t) \\otimes (i_t \\odot \\breve{C}_t)\n",
    "$$\n",
    "\n",
    "A porta de saída ($\\small \\textbf{o}_t$) decide como atualizar os valores das unidades ocultas:\n",
    "\n",
    "$$\n",
    "\\small o_t = \\sigma (\\textbf{W}_{xo}\\textbf{x}^{(t)} + \\textbf{W}_{ho}\\textbf{h}^{(t-1)} + \\textbf{b}_o)\n",
    "$$\n",
    "\n",
    "Dado isso, as unidades ocultas no passo de tempo atual são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\textbf{o}_t \\odot \\: tanh (\\textbf{C}^{(t)})\n",
    "$$\n",
    "\n",
    "A estrutura de uma célula *LSTM* e seus cálculos subjacentes podem parecer muito complexos e difíceis de implementar. No entanto, a boa notícia é que o *TensorFlow* já implementou tudo em funções otimizadas de wr*apper, o que nos permite definir nossas células *LSTM* de maneira fácil e eficiente. Aplicaremos RNNs e LSTMs a conjuntos de dados do mundo real posteriormente.\n",
    "\n",
    "> #### Outros modelos RNN avançados\n",
    "> *LSTMs* fornecem uma abordagem básica para modelar dependências de longo alcance em sequências. No entanto, é importante notar que existem muitas variações de *LSTMs* descritas na literatura (An Empirical Exploration of Recurrent Network Architectures, Rafal Jozefowicz, Wojciech Zaremba e Ilya Sutskever, Proceedings of ICML, 2342-2350, 2015).\n",
    ">\n",
    "> Também merece destaque uma abordagem mais recente, *Gated Recurrent Unit* (GRU), proposta em 2014. As GRUs possuem uma arquitetura mais simples que as *LSTMs*; portanto, eles são computacionalmente mais eficientes, enquanto seu desempenho em algumas tarefas, como modelagem de música polifônica, é comparável aos *LSTMs*.\n",
    "\n",
    "### Implementando RNNs para modelagem de sequência no TensorFlow\n",
    "\n",
    "Agora que abordamos a teoria subjacente por trás das *RNNs*, estamos prontos para passar para a parte mais prática: implementar *RNNs* no TensorFlow. Durante o restante deste capítulo, aplicaremos *RNNs* a duas tarefas problemáticas comuns:\n",
    "1. Análise de sentimentos\n",
    "2. Modelagem de linguagem\n",
    "\n",
    "Esses dois projetos, que apresentaremos, são fascinantes e envolventes. Assim, em vez de fornecer o código de uma só vez, dividiremos a implementação em várias etapas e discutiremos o código em detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto um – prevendo o sentimento das críticas de filmes do IMDb\n",
    "\n",
    "Nesta seção e nas subseções seguintes, implementaremos uma *RNN* multicamada para análise de sentimentos usando uma arquitetura muitos-para-um.\n",
    "\n",
    "Na próxima seção, implementaremos um *RNN* muitos-para-muitos para uma aplicação de modelagem de linguagem. Embora os exemplos escolhidos sejam propositadamente simples para introduzir os principais conceitos de *RNNs*, a modelagem de linguagem tem uma ampla gama de aplicações interessantes, como a construção de chatbots – dando aos computadores a capacidade de conversar e interagir diretamente com humanos.\n",
    "\n",
    "### Preparando os dados de revisão do filme\n",
    "Nas etapas de pré-processamento, criamos um conjunto de dados limpo chamado `movie_data.csv`, que usaremos novamente agora. Primeiro, importaremos os módulos necessários e leremos os dados em um `DataFrame` pandas, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se de que esse quadro de dados, `df`, consiste em duas colunas, chamadas `'review'` e `'sentiment'`, onde `'review'` contém o texto das resenhas de filmes (os recursos de entrada) e `'sentiment'` representa o rótulo de destino que queremos prever (0 refere-se ao sentimento negativo e 1 refere-se ao sentimento positivo).\n",
    "\n",
    "O componente de texto dessas resenhas de filmes são sequências de palavras, e o modelo *RNN* classifica cada sequência como uma resenha positiva (1) ou negativa (0). No entanto, antes de podermos alimentar os dados em um modelo *RNN*, precisamos aplicar várias etapas de pré-processamento:\n",
    "1. Crie um objeto de conjunto de dados do *TensorFlow* e divida-o em partições separadas de treinamento, teste e validação.\n",
    "2. Identifique as palavras exclusivas no conjunto de dados de treinamento.\n",
    "3. Mapeie cada palavra exclusiva para um número inteiro exclusivo e codifique o texto da revisão em números inteiros codificados (um índice de cada palavra exclusiva).\n",
    "4. Divida o conjunto de dados em minilotes como entrada para o modelo.\n",
    "\n",
    "Vamos prosseguir com a primeira etapa: criar um conjunto de dados do *TensorFlow* a partir deste quadro de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In 1974, the teenager Martha Moxley (Maggie Grace)' 1\n",
      "b'OK... so... I really like Kris Kristofferson and h' 0\n",
      "b'***SPOILER*** Do not read this, if you think about' 0\n"
     ]
    }
   ],
   "source": [
    "# Passo 1: Criando um Dataset\n",
    "\n",
    "target = df.pop('sentiment')\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    (df.values, target.values))\n",
    "\n",
    "## inspection:\n",
    "for ex in ds_raw.take(3):\n",
    "    tf.print(ex[0].numpy()[0][:50], ex[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos dividi-lo em conjuntos de dados de treinamento, teste e validação. Todo o conjunto de dados contém 50.000 exemplos. Manteremos os primeiros 25.000 exemplos para avaliação (conjunto de dados de teste de retenção) e, em seguida, 20.000 exemplos serão usados para treinamento e 5.000 para validação. O código é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "ds_raw = ds_raw.shuffle(50000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(25000)                 # Separa 25.000 amostras das 50.000\n",
    "ds_raw_train_valid = ds_raw.skip(25000)          # Pula as 25.000 amostras do conjunto anterior e salva em uma variável \n",
    "ds_raw_train = ds_raw_train_valid.take(20000)    # Da variável anterior, vamos pegas 20.000 e salvar numa variável.\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)    # Vamos pular 20.000 amostras e salvar numa variável. Que será somente 5.000 (que sobrou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preparar os dados para entrada em uma RN, precisamos codificá-los em valores numéricos, conforme mencionado nas etapas 2 e 3. Para fazer isso, primeiro encontraremos as palavras exclusivas (*tokens*) no conjunto de dados de treinamento. Embora encontrar *tokens* exclusivos seja um processo para o qual podemos usar conjuntos de dados *Python*, pode ser mais eficiente usar a classe `Counter` do pacote `collections`, que faz parte da biblioteca padrão do *Python*.\n",
    "\n",
    "No código a seguir, instanciaremos um novo objeto `Counter` (`token_counts`) que coletará as frequências de palavras exclusivas. Observe que neste aplicativo específico (e em contraste com o modelo *bag-of-words*), estamos interessados apenas no conjunto de palavras únicas e não exigiremos a contagem de palavras, que é criada como um produto secundário. Para dividir o texto em palavras (ou *tokens*), o pacote `tensorflow_datasets` fornece uma classe `Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 87007\n"
     ]
    }
   ],
   "source": [
    "## Passo 2: Encontrar tokens (palavras) unicas\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in ds_raw_train:\n",
    "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos mapear cada palavra única para um inteiro único. Isso pode ser feito manualmente usando um dicionário Python, onde as chaves são os tokens exclusivos (palavras) e o valor associado a cada chave é um inteiro único. No entanto, o pacote `tensorflow_datasets` já fornece uma classe, `TokenTextEncoder`, que podemos usar para criar esse mapeamento e codificar todo o conjunto de dados. Primeiro, criaremos um objeto codificador da classe `TokenTextEncoder` passando os tokens exclusivos (`token_counts` contém os tokens e suas contagens, embora aqui, suas contagens não sejam necessárias, portanto, serão ignoradas). Chamar o método `encoder.encode()` converterá seu texto de entrada em uma lista de valores inteiros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232, 9, 35, 1123]\n"
     ]
    }
   ],
   "source": [
    "## Passo 3: Encode tokens únicos (palavras) para inteiros\n",
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "example_str = 'This is a example!'\n",
    "print(encoder.encode(example_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que pode haver alguns *tokens* nos dados de validação ou teste que não estão presentes nos dados de treinamento e, portanto, não estão incluídos no mapeamento. Se tivermos $\\small q$ *tokens* (que é o tamanho de `token_counts` passado para o `TokenTextEncoder`, que neste caso é 87.007), todos os *tokens* que não foram vistos antes e, portanto, não estão incluídos em `token_counts`, receberão o inteiro $\\small q + 1$ (que será 87.008 no nosso caso). Em outras palavras, o índice $\\small q + 1$ é reservado para palavras desconhecidas.\n",
    "\n",
    "Outro valor reservado é o inteiro 0, que serve como espaço reservado para ajustar o comprimento da sequência. Mais tarde, quando estivermos construindo um modelo *RNN* no *TensorFlow*, consideraremos esses dois espaços reservados, 0 e $\\small q + 1$, com mais detalhes.\n",
    "\n",
    "Podemos usar o método `map()` dos objetos do conjunto de dados para transformar cada texto no conjunto de dados de acordo, assim como aplicaríamos qualquer outra transformação a um conjunto de dados. No entanto, há um pequeno problema: aqui, os dados de texto são incluídos em objetos tensores, que podemos acessar chamando o método `numpy()` em um tensor no modo de execução antecipada. Mas durante as transformações pelo método `map()`, a execução antecipada será desabilitada. Para resolver este problema, podemos definir duas funções. A primeira função tratará os tensores de entrada como se o modo de execução ansioso estivesse habilitado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passo 3-A: defina uma função para a tranformação\n",
    "\n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na segunda função, envolveremos a primeira função usando `tf.py_function` para convertê-la em um operador *TensorFlow*, que pode ser usado por meio de seu método `map()`. Este processo de codificação de texto em uma lista de inteiros pode ser realizado usando o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da sequência: (24,)\n",
      "Tamanho da sequência: (179,)\n",
      "Tamanho da sequência: (262,)\n",
      "Tamanho da sequência: (535,)\n",
      "Tamanho da sequência: (130,)\n"
     ]
    }
   ],
   "source": [
    "## Passo 3-B: Empacotar a Função encode na Tf. Op\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label],\n",
    "    Tout=(tf.int64, tf.int64))\n",
    "\n",
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "#  Checando o Shape de alguns exemplos\n",
    "tf.random.set_seed(1)\n",
    "for example in ds_train.shuffle(1000).take(5):\n",
    "    print(f\"Tamanho da sequência: {example[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora, convertemos sequências de palavras em sequências de inteiros. No entanto, há um problema que ainda precisamos resolver — as sequências atualmente têm comprimentos diferentes (como mostrado no resultado da execução do código anterior para cinco exemplos escolhidos aleatoriamente). Embora, em geral, as RNNs possam lidar com sequências com comprimentos diferentes, ainda precisamos garantir que todas as sequências em um minilote tenham o mesmo comprimento para armazená-las de forma eficiente em um tensor.\n",
    "\n",
    "Para dividir um conjunto de dados que possui elementos com formas diferentes em mini-lotes, o *TensorFlow* fornece um método diferente, *padded_batch()* (em vez de `batch()`), que preencherá automaticamente os elementos consecutivos que devem ser combinados em um lote com valores de espaço reservado (0s) para que todas as sequências dentro de um lote tenham a mesma forma. Para ilustrar isso com um exemplo prático, vamos pegar um pequeno subconjunto de tamanho 8 do conjunto de dados de treinamento, `ds_train`, e aplicar o método *padded_batch()* a esse subconjunto com `batch_size=4`. Também imprimiremos os tamanhos dos elementos individuais antes de combiná-los em minilotes, bem como as dimensões dos minilotes resultantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho individual: (119,)\n",
      "Tamanho individual: (688,)\n",
      "Tamanho individual: (308,)\n",
      "Tamanho individual: (204,)\n",
      "Tamanho individual: (326,)\n",
      "Tamanho individual: (240,)\n",
      "Tamanho individual: (127,)\n",
      "Tamanho individual: (453,)\n"
     ]
    }
   ],
   "source": [
    "## Selecionar um pequeno conjunto de dados:\n",
    "\n",
    "ds_subset = ds_train.take(8)\n",
    "for example in ds_subset:\n",
    "    print(f\"Tamanho individual: {example[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dimension: (4, 688)\n",
      "Batch dimension: (4, 453)\n"
     ]
    }
   ],
   "source": [
    "## Loteando o conjunto de dados\n",
    "\n",
    "ds_batch = ds_subset.padded_batch(4, padded_shapes=([-1], []))\n",
    "\n",
    "for batch in ds_batch:\n",
    "    print(f\"Batch dimension: {batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode observar nas formas de tensor impressas, o número de colunas (ou seja, `.shape[1]`) no primeiro lote é `688`, resultado da combinação dos quatro primeiros exemplos em um único lote e do uso do tamanho máximo desses exemplos. Isso significa que os outros três exemplos neste lote são preenchidos o quanto for necessário para corresponder a esse tamanho.\n",
    "\n",
    "Da mesma forma, o segundo lote mantém o tamanho máximo de seus quatro exemplos individuais, que é `453`, e preenche os outros exemplos para que seu comprimento seja menor que o comprimento máximo.\n",
    "\n",
    "Vamos dividir todos os três conjuntos de dados em minilotes com um tamanho de lote de 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds_train.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "valid_data = ds_valid.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = ds_test.padded_batch(\n",
    "    32, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, os dados estão em um formato adequado para um modelo RNN, que vamos implementar. No próximo tópico, no entanto, discutiremos primeiro a incorporação de recursos, que é uma etapa de pré-processamento opcional, mas altamente recomendada, usada para reduzir a dimensionalidade dos vetores de palavras.\n",
    "\n",
    "### Incorporando camadas para codificação de frases\n",
    "Durante a preparação dos dados na etapa anterior, geramos sequências de mesmo comprimento. Os elementos dessas sequências eram números inteiros que correspondiam aos `índices` de palavras únicas. Esses índices de palavras podem ser convertidos em recursos de entrada de várias maneiras diferentes. Uma maneira ingênua é aplicar a codificação *one-hot* para converter os índices em vetores de zeros e uns. Em seguida, cada palavra será mapeada para um vetor cujo tamanho é o número de palavras únicas em todo o conjunto de dados. Dado que o número de palavras únicas (o tamanho do vocabulário) pode ser da ordem de $\\small 10^4 - 10^5$, que também será o número de nossos recursos de entrada, um modelo treinado em tais recursos pode sofrer a **maldição da dimensionalidade**. Além disso, esses recursos são muito esparsos, pois todos são zero, exceto um.\n",
    "\n",
    "Uma abordagem mais elegante é mapear cada palavra para um vetor de tamanho fixo com elementos de valor real (não necessariamente inteiros). Em contraste com os vetores codificados *one-hot*, podemos usar vetores de tamanho finito para representar um número infinito de números reais. (Em teoria, podemos extrair infinitos números reais de um determinado intervalo, por exemplo [–1, 1].)\n",
    "\n",
    "Essa é a ideia por trás da incorporação (**embedding**), que é uma técnica de aprendizado de recursos que podemos utilizar aqui para aprender automaticamente os recursos importantes para representar as palavras em nosso conjunto de dados. Dado o número de palavras únicas, $\\small n_{words}$, podemos selecionar o tamanho dos vetores de incorporação (também conhecido como, a dimensão de incorporação) para ser muito menor que o número de palavras únicas ($\\small embedding_{dims} << n_{words}$) para representar todo o vocabulário como recursos de entrada.\n",
    "\n",
    "As vantagens da incorporação sobre a codificação *one-hot* são as seguintes:\n",
    "• Uma redução na dimensionalidade do espaço de recursos para diminuir o efeito da maldição da dimensionalidade\n",
    "• A extração de recursos salientes desde a camada de incorporação em uma RN pode ser otimizada (ou aprendida)\n",
    "\n",
    "A representação esquemática a seguir mostra como a incorporação funciona mapeando índices de token para uma matriz de incorporação treinável:\n",
    "\n",
    "![](imagens\\embedding.PNG)\n",
    "\n",
    "\n",
    "Dado um conjunto de tokens de tamanho $\\small n + 2$ ($\\small n$ é o tamanho do conjunto de *tokens*, mais o índice 0 é reservado para o preenchimento e $\\small n + 1$ é para as palavras não presentes no conjunto de *tokens*), uma matriz de incorporação de tamanho $\\small (n + 2) \\times embedding-dim$ será criado onde cada linha desta matriz representa recursos numéricos associados a um *token*.\n",
    "\n",
    "Portanto, quando um índice inteiro, $\\small i$, for fornecido como entrada para a incorporação, ele procurará a linha correspondente da matriz no índice $\\small i$ e retornará os recursos numéricos. A matriz de incorporação serve como camada de entrada para nossos modelos RN.\n",
    "\n",
    "Na prática, a criação de uma camada de incorporação pode ser feita simplesmente usando `tf.keras.layers.Embedding`. Vamos ver um exemplo onde vamos criar um modelo e adicionar uma camada de embedding, como segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, 20, 6)             600       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600\n",
      "Trainable params: 600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=100,\n",
    "                    output_dim=6,\n",
    "                    input_length=20,\n",
    "                    name=\"embed-layer\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrada para este modelo (camada de incorporação) deve ter a classificação 2 com dimensionalidade $\\small batchsize \\times input\\_length$, onde $\\small input\\_length$ é o comprimento das seqüências (no caso aqui, definida como 20 através do argumento `input_length`). Por exemplo, uma sequência de entrada no minilote pode ser $\\small \\left \\langle  14,43,52, 1,8,19,67,83,10,7,42,87,56,18,94,17,67,90, 6,39 \\right \\rangle$, onde cada elemento desta sequência é o índice das palavras únicas. A saída terá dimensionalidade $\\small batchsize \\times input\\_length \\times embedding\\_dim$, onde $\\small embedding\\_dim$ é o tamanho dos recursos de incorporação (aqui, definido como 6 via `output_dim`). O outro argumento fornecido à camada de incorporação, `input_dim`, corresponde aos valores inteiros exclusivos que o modelo receberá como entrada (por exemplo, `n + 2`, definido aqui como `100`). Portanto, a matriz de incorporação neste caso tem o tamanho `100 × 6`.\n",
    "\n",
    "> ##### Lidando com comprimentos de sequência variáveis\n",
    "> Observe que o argumento `input_length` não é necessário e podemos usar `None` para casos em que os comprimentos das sequências de entrada variam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo um modelo *RNN*\n",
    "Agora estamos prontos para construir um modelo *RNN*. Usando a classe Keras `Sequential`, podemos combinar a camada de incorporação (*embedding*), as camadas recorrentes da *RNN* e as camadas não recorrentes totalmente conectadas. Para as camadas recorrentes, podemos usar qualquer uma das seguintes implementações:\n",
    "* `SimpleRNN`: uma camada *RNN* regular, ou seja, uma camada recorrente totalmente conectada\n",
    "* `LSTM`: uma *RNN* de memória de curto prazo longo, que é útil para capturar as dependências de longo prazo\n",
    "* `GRU`: uma camada recorrente com uma unidade recorrente fechada, como alternativa aos `LSTMs`\n",
    "\n",
    "Para ver como um modelo *RNN* multicamada pode ser construído usando uma dessas camadas recorrentes, no exemplo a seguir, criaremos um modelo *RNN*, começando com uma camada de incorporação com `input_dim=1000` e `output_dim=32`. Em seguida, serão adicionadas duas camadas recorrentes do tipo `SimpleRNN`. Por fim, adicionaremos uma camada totalmente conectada não recorrente como camada de saída, que retornará um único valor de saída como previsão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          32000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,193\n",
      "Trainable params: 36,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 32)          8320      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,673\n",
      "Trainable params: 336,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Usando uma RNN com LTSM \n",
    "from tensorflow.keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 32)          6336      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 332,705\n",
      "Trainable params: 332,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Um exemplo com uma camada GRU\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, construir um modelo *RNN* usando essas camadas recorrentes é bastante simples. Na próxima subseção, voltaremos à nossa tarefa de análise de sentimentos e construiremos um modelo *RNN* para resolver isso.\n",
    "\n",
    "### Construindo um modelo *RNN* para a tarefa de análise de sentimento\n",
    "\n",
    "Como temos sequências muito longas, usaremos uma camada *LSTM* para contabilizar os efeitos de longo prazo. Além disso, colocaremos a camada *LSTM* dentro de um wrapper `Bidirectional`, que fará com que as camadas recorrentes passem pelas sequências de entrada de ambas as direções, do início ao fim, bem como no sentido inverso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1740180   \n",
      "                                                                 \n",
      " bidir-lstm (Bidirectional)  (None, 128)               43520     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,792,021\n",
      "Trainable params: 1,792,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 59s 90ms/step - loss: 0.5012 - accuracy: 0.7455 - val_loss: 0.3960 - val_accuracy: 0.8252\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 55s 89ms/step - loss: 0.2501 - accuracy: 0.9054 - val_loss: 0.3445 - val_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.1257 - accuracy: 0.9593 - val_loss: 0.4547 - val_accuracy: 0.8578\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 56s 90ms/step - loss: 0.0747 - accuracy: 0.9766 - val_loss: 0.4966 - val_accuracy: 0.8270\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 56s 89ms/step - loss: 0.0911 - accuracy: 0.9696 - val_loss: 0.5535 - val_accuracy: 0.8516\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 56s 89ms/step - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.6706 - val_accuracy: 0.8294\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 57s 90ms/step - loss: 0.0320 - accuracy: 0.9900 - val_loss: 0.6230 - val_accuracy: 0.8250\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 55s 88ms/step - loss: 0.0378 - accuracy: 0.9894 - val_loss: 0.7325 - val_accuracy: 0.8010\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 56s 89ms/step - loss: 0.0326 - accuracy: 0.9905 - val_loss: 0.6865 - val_accuracy: 0.8454\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.6340 - val_accuracy: 0.8150\n",
      "782/782 [==============================] - 32s 41ms/step - loss: 0.6488 - accuracy: 0.8149\n",
      "Test Acc.: 81.49%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "## Construindo o modelo \n",
    "bi_lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        name='embed-layer'),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, name='lstm-layer'),\n",
    "        name='bidir-lstm'), \n",
    "\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "## Compilando e treinando:\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = bi_lstm_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)\n",
    "\n",
    "## Avaliando os dados\n",
    "test_results= bi_lstm_model.evaluate(test_data)\n",
    "print('Test Acc.: {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após treinar este modelo por 10 épocas, a avaliação nos dados de teste mostra **81%** de precisão. (Observe que esse resultado não é o melhor quando comparado aos métodos de última geração usados no conjunto de dados do IMDb. O objetivo era simplesmente mostrar como o RNN funciona.)\n",
    "\n",
    ">##### Mais sobre o RNN bidirecional\n",
    "> O wrapper `Bidirectional` faz duas passagens em cada sequência de entrada: uma passagem para frente e uma passagem reversa ou para trás (observe que isso não deve ser confundido com as passagens para frente e para trás no contexto de retropropagação). Os resultados dessas passagens para frente e para trás serão concatenados por padrão. Mas se você quiser mudar esse comportamento, você pode definir o argumento `merge_mode` para `'sum'` (para soma), `'mul'` (para multiplicar os resultados das duas passagens), `'ave'` (para tirar a média das duas) , `'concat'` (que é o padrão) ou `None`, que retorna os dois tensores em uma lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos tentar outros tipos de camadas recorrentes, como `SimpleRNN`. No entanto, como se vê, um modelo construído com camadas recorrentes regulares não será capaz de alcançar um bom desempenho preditivo (mesmo nos dados de treinamento). Por exemplo, se você tentar substituir a camada *LSTM* bidirecional no código anterior por uma camada `SimpleRNN` unidirecional e treinar o modelo em sequências completas, poderá observar que a perda nem diminuirá durante o treinamento. **A razão é que as sequências neste conjunto de dados são muito longas**, portanto, um modelo com uma camada `SimpleRNN` não pode aprender as dependências de longo prazo e pode sofrer com problemas de **gradiente de fuga ou explosão**.\n",
    "\n",
    "Para obter um desempenho preditivo razoável neste conjunto de dados usando um `SimpleRNN`, podemos truncar as sequências. Além disso, utilizando nosso \"conhecimento de domínio\", podemos supor que os últimos parágrafos de uma crítica de filme podem conter a maioria das informações sobre seu sentimento. Portanto, podemos nos concentrar apenas na última parte de cada revisão. Para fazer isso, vamos definir uma função auxiliar, `preprocess_datasets()`, para combinar as etapas de pré-processamento `2-4`. Um argumento opcional para esta função é `max_seq_length`, que determina quantos *tokens* de cada revisão devem ser usados. Por exemplo, se definirmos `max_seq_length=100` e um comentário tiver mais de 100 *tokens*, apenas os últimos 100 *tokens* serão usados. Se `max_seq_length` for definido como `None`, as sequências de comprimento total serão usadas como antes. Tentar valores diferentes para `max_seq_length` nos dará mais informações sobre a capacidade de diferentes modelos de RNN para lidar com sequências longas.\n",
    "\n",
    "O código para a função `preprocess_datasets()` é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def preprocess_datasets(\n",
    "    ds_raw_train, \n",
    "    ds_raw_valid, \n",
    "    ds_raw_test,\n",
    "    max_seq_length=None,\n",
    "    batch_size=32):\n",
    "    \n",
    "    ## Passo 1: (Já feito => Cria o DataSet)\n",
    "    ## Passo 2: (Encontra os tokens únicos (palavras)\n",
    "    tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for example in ds_raw_train:\n",
    "        tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "        if max_seq_length is not None:\n",
    "            tokens = tokens[-max_seq_length:]\n",
    "        token_counts.update(tokens)\n",
    "\n",
    "    print('Vocab-size:', len(token_counts))\n",
    "\n",
    "\n",
    "    ## Passo 3: Encode os textos\n",
    "    encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "    def encode(text_tensor, label):\n",
    "        text = text_tensor.numpy()[0]\n",
    "        encoded_text = encoder.encode(text)\n",
    "        if max_seq_length is not None:\n",
    "            encoded_text = encoded_text[-max_seq_length:]\n",
    "        return encoded_text, label\n",
    "\n",
    "    def encode_map_fn(text, label):\n",
    "        return tf.py_function(encode, inp=[text, label], \n",
    "                              Tout=(tf.int64, tf.int64))\n",
    "\n",
    "    ds_train = ds_raw_train.map(encode_map_fn)\n",
    "    ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "    ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "    ## Passo 4: Lotea o DataSet\n",
    "    train_data = ds_train.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    valid_data = ds_valid.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    test_data = ds_test.padded_batch(\n",
    "        batch_size, padded_shapes=([-1],[]))\n",
    "\n",
    "    return (train_data, valid_data, \n",
    "            test_data, len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos definir outra função auxiliar, `build_rnn_model()`, para construir modelos com diferentes arquiteturas de forma mais conveniente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "def build_rnn_model(embedding_dim, vocab_size,\n",
    "                    recurrent_type='SimpleRNN',\n",
    "                    n_recurrent_units=64,\n",
    "                    n_recurrent_layers=1,\n",
    "                    bidirectional=True):\n",
    "\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # Constrói o modelo\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            name='embed-layer')\n",
    "    )\n",
    "    \n",
    "    for i in range(n_recurrent_layers):\n",
    "        return_sequences = (i < n_recurrent_layers-1)\n",
    "            \n",
    "        if recurrent_type == 'SimpleRNN':\n",
    "            recurrent_layer = SimpleRNN(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='simprnn-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            recurrent_layer = LSTM(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='lstm-layer-{}'.format(i))\n",
    "        elif recurrent_type == 'GRU':\n",
    "            recurrent_layer = GRU(\n",
    "                units=n_recurrent_units, \n",
    "                return_sequences=return_sequences,\n",
    "                name='gru-layer-{}'.format(i))\n",
    "        \n",
    "        if bidirectional:\n",
    "            recurrent_layer = Bidirectional(\n",
    "                recurrent_layer, name='bidir-'+recurrent_layer.name)\n",
    "            \n",
    "        model.add(recurrent_layer)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, usando essas duas funções auxiliares bastante gerais, mas convenientes, podemos comparar prontamente diferentes modelos RNN com diferentes comprimentos de sequência de entrada. Como exemplo, no código a seguir, tentaremos um modelo com uma única camada recorrente do tipo `SimpleRNN` enquanto truncamos as sequências para um comprimento máximo de 100 *tokens*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 58063\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embed-layer (Embedding)     (None, None, 20)          1161300   \n",
      "                                                                 \n",
      " bidir-simprnn-layer-0 (Bidi  (None, 128)              10880     \n",
      " rectional)                                                      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,180,501\n",
      "Trainable params: 1,180,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dim = 20\n",
    "max_seq_length = 100\n",
    "\n",
    "train_data, valid_data, test_data, n = preprocess_datasets(\n",
    "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
    "    max_seq_length=max_seq_length, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "vocab_size = n + 2\n",
    "\n",
    "rnn_model = build_rnn_model(\n",
    "    embedding_dim, vocab_size,\n",
    "    recurrent_type='SimpleRNN', \n",
    "    n_recurrent_units=64,\n",
    "    n_recurrent_layers=1,\n",
    "    bidirectional=True)\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 173s 275ms/step - loss: 0.7013 - accuracy: 0.5030 - val_loss: 0.6968 - val_accuracy: 0.5040\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 177s 282ms/step - loss: 0.6858 - accuracy: 0.5490 - val_loss: 0.6403 - val_accuracy: 0.6286\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 176s 281ms/step - loss: 0.4813 - accuracy: 0.7696 - val_loss: 0.4830 - val_accuracy: 0.7886\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.2786 - accuracy: 0.8864 - val_loss: 0.5534 - val_accuracy: 0.7630\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.1624 - accuracy: 0.9371 - val_loss: 0.6887 - val_accuracy: 0.7518\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 176s 281ms/step - loss: 0.0504 - accuracy: 0.9829 - val_loss: 1.0899 - val_accuracy: 0.7372\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 178s 284ms/step - loss: 0.0285 - accuracy: 0.9915 - val_loss: 1.3421 - val_accuracy: 0.7308\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 176s 281ms/step - loss: 0.0559 - accuracy: 0.9803 - val_loss: 1.3597 - val_accuracy: 0.7172\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.0391 - accuracy: 0.9857 - val_loss: 1.3970 - val_accuracy: 0.6420\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 1.3560 - val_accuracy: 0.7044\n"
     ]
    }
   ],
   "source": [
    "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    train_data, \n",
    "    validation_data=valid_data, \n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, truncar as sequências para 100 tokens e usar uma camada `SimpleRNN` bidirecional resultou em **80%** de precisão de classificação. Embora a previsão seja um pouco menor quando comparada ao modelo *LSTM* bidirecional anterior (**85,15%** de precisão no conjunto de dados de teste), o desempenho nessas sequências truncadas é muito melhor do que o desempenho que poderíamos alcançar com um `SimpleRNN` em resenhas de filmes completos. Como exercício opcional, você pode verificar isso usando as duas funções auxiliares que já definimos. Experimente com `max_seq_length=None` e defina o argumento bidirecional dentro da função auxiliar `build_rnn_model()` como `False`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1870f1194fb5b3d43b6d32e845741389586dbe9c4e1e45e17e0f6602cfe22778"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
