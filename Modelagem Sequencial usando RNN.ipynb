{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modelagem de dados sequenciais usando redes neurais recorrentes</h1>\n",
    "\n",
    "<p align=center><img src=https://datascience.eu/wp-content/uploads/2020/05/image-513-1024x347.png></p>\n",
    "\n",
    "Tivemos oportunidade de focar em redes neurais convolucionais (*CNNs*), de forma a cobrir os blocos de construção das arquiteturas *CNN* e como implementar *CNNs* profundas no *TensorFlow*. Por fim, você aprendeu a usar *CNNs* para classificação de imagens.\n",
    "\n",
    "Aqui, exploraremos as redes neurais recorrentes (*RNNs*) e veremos sua aplicação na modelagem de dados sequenciais.\n",
    "\n",
    "### Apresentando dados sequenciais\n",
    "Vamos começar nossa discussão sobre *RNNs* observando a natureza dos dados sequenciais, que são mais comumente conhecidos como dados de sequência ou **sequências**. Vamos dar uma olhada nas propriedades únicas das sequências que as tornam diferentes de outros tipos de dados. Veremos então como podemos representar dados sequenciais e explorar as várias categorias de modelos para dados sequenciais, que são baseados na entrada e saída de um modelo. Isso nos ajudará a explorar a relação entre *RNNs* e sequências.\n",
    "\n",
    "### Modelagem de dados sequenciais - a ordem importa\n",
    "\n",
    "O que torna as sequências únicas, em comparação com outros tipos de dados, é que os elementos em uma sequência aparecem em uma determinada ordem e não são independentes uns dos outros. Algoritmos típicos de aprendizado de máquina para aprendizado supervisionado pressupõem que a entrada de dados é **independente e e distribuída de forma idêntica (IID)**, o que significa que os exemplos de treinamento são `mutuamente independentes` e têm a mesma distribuição subjacente.\n",
    "\n",
    "Nesse sentido, com base na suposição de independência mútua, a ordem em que os exemplos de treinamento são dados ao modelo é **irrelevante**. Por exemplo, se tivermos uma amostra composta por n exemplos de treinamento, $\\small x^{(1)}, x^{(2)},\\cdots, x^{(n)} $, a ordem em que usamos os dados para treinar nosso algoritmo de aprendizado de máquina não importa. Um exemplo desse cenário seria o conjunto de dados Iris, muito conhecido. No conjunto de dados Iris, cada flor foi medida independentemente e as medidas de uma flor não influenciam as medidas de outra flor.\n",
    "\n",
    "No entanto, essa suposição não é válida quando lidamos com sequências – por definição, **a ordem é importante**. Prever o valor de mercado de uma determinada ação seria um exemplo desse cenário. Por exemplo, suponha que temos uma amostra de n exemplos de treinamento, onde cada exemplo de treinamento representa o valor de mercado de uma determinada ação em um determinado dia. Se nossa tarefa é prever o valor do mercado de ações para os próximos três dias, faria sentido considerar os preços das ações anteriores em uma ordem de data para derivar tendências, em vez de utilizar esses exemplos de treinamento em uma ordem aleatória.\n",
    "\n",
    "> #### Dados sequenciais versus dados de séries temporais\n",
    "> Os dados de série temporal são um tipo especial de dados sequenciais, em que cada exemplo está associado a uma dimensão de tempo. Em dados de séries temporais, as amostras são coletadas em *timestamps* sucessivos e, portanto, a dimensão de tempo determina a ordem entre os pontos de dados. Por exemplo, preços de ações e registros de voz ou fala são dados de séries temporais.\n",
    ">\n",
    "> Por outro lado, nem todos os dados sequenciais têm a dimensão temporal, por exemplo, dados de texto ou sequências de DNA, onde os exemplos são ordenados, mas não se qualificam como dados de séries temporais. Como você verá, abordaremos alguns exemplos de Processamento de Linguagem Natural (NLP) e modelagem de texto que não são dados de série temporal, mas observe que as *RNNs* também podem ser usados para dados de série temporal.\n",
    "\n",
    "### Representando sequências\n",
    "\n",
    "Estabelecemos que a ordem entre os pontos de dados é importante em dados sequenciais, então precisamos encontrar uma maneira de aproveitar essas informações de pedido em um modelo de aprendizado de máquina. Ao longo das explicações, representaremos as sequências como $\\small \\left \\langle x^{(1)},x^{(2)},\\cdots, x^{(T)}  \\right \\rangle$ . Os índices sobrescritos indicam a ordem das instâncias e o comprimento da sequência é $\\small T$. Para um exemplo sensato de sequências, considere dados de séries temporais, onde cada ponto de exemplo, $\\small x^{(T)}$, pertence a um determinado tempo, $\\small t$. A figura a seguir mostra um exemplo de dados de série temporal em que os recursos de entrada ($\\small x$) e os rótulos de destino ($\\small y$) seguem naturalmente a ordem de acordo com seu eixo de tempo; portanto, ambos os `x` e `y` são sequências:\n",
    "\n",
    "![](imagens\\sequencias.PNG)\n",
    "\n",
    "Como já mencionamos, os modelos de rede neural padrão (*RN*) que abordamos até agora, como o *multilayer perceptron* (MLP) e as *CNNs* para dados de imagem, assumem que os exemplos de treinamento são independentes uns dos outros e, portanto, não incorporam **informação de ordenamento**. Podemos dizer que tais modelos não possuem **memória** de exemplos de treinamento vistos anteriormente. Por exemplo, as amostras são passadas pelas etapas de *feedforward* e *backpropagation* e os pesos são atualizados independentemente da ordem em que os exemplos de treinamento são processados. As *RNNs*, por outro lado, são projetadas para modelar sequências e são capazes de lembrar informações passadas e processar novos eventos de acordo, o que é uma clara vantagem ao trabalhar com dados de sequência.\n",
    "\n",
    "### As diferentes categorias de modelagem de sequência\n",
    "\n",
    "A modelagem de sequência tem muitas aplicações fascinantes, como tradução de idiomas (por exemplo, tradução de texto de inglês para alemão), legendas de imagens e geração de texto. No entanto, para escolher uma arquitetura e abordagem apropriadas, temos que entender e ser capazes de distinguir entre essas diferentes tarefas de modelagem de sequência. A figura a seguir, baseada nas explicações do excelente artigo The Unreasonable Effectiveness of Recurrent Neural Networks, de Andrej Karpathy (http://karpathy.github.io/2015/05/21/rnn-effectiveness/), resume a sequência mais comum tarefas de modelagem, que dependem das categorias de relacionamento de dados de entrada e saída:\n",
    "\n",
    "![](imagens\\modelagem_de_sequencia.PNG)\n",
    "\n",
    "Vamos discutir as diferentes categorias de relacionamento entre dados de entrada e saída, que foram descritas na figura anterior, com mais detalhes. Se nem os dados de entrada nem de saída representam sequências, então estamos lidando com dados padrão e podemos simplesmente usar um perceptron multicamada (ou outro modelo de classificação abordado anteriormente) para modelar esses dados. No entanto, se a entrada ou a saída for uma sequência, a tarefa de modelagem provavelmente se enquadra em uma destas categorias:\n",
    "* **Muitos para um**: Os dados de entrada são uma sequência, mas a saída é um vetor de tamanho fixo ou escalar, não uma sequência. Por exemplo, na análise de sentimentos, a entrada é baseada em texto (por exemplo, uma resenha de filme) e a saída é um rótulo de classe (por exemplo, um rótulo que indica se um revisor gostou do filme).\n",
    "\n",
    "* **Um para muitos**: Os dados de entrada estão no formato padrão e não em sequência, mas a saída é uma sequência. Um exemplo dessa categoria é a legendagem de imagens — a entrada é uma imagem e a saída é uma frase em inglês que resume o conteúdo dessa imagem.\n",
    "\n",
    "* **Muitos para muitos**: As matrizes de entrada e saída são sequências. Esta categoria pode ser dividida com base na sincronização da entrada e saída. Um exemplo de uma tarefa de modelagem sincronizada de muitos para muitos é a classificação de vídeo, onde cada quadro em um vídeo é rotulado. Um exemplo de uma tarefa de modelagem muitos-para-muitos *atrasada* seria traduzir uma linguagem para outra. Por exemplo, uma frase inteira em inglês deve ser lida e processada por uma máquina antes que sua tradução para o alemão seja produzida. \n",
    "\n",
    "Agora, depois de resumir as três grandes categorias de modelagem de sequência, podemos avançar para discutir a estrutura de uma RNN.\n",
    "\n",
    "### RNNs para modelagem de sequências \n",
    "Nesta seção, antes de começarmos a implementar *RNNs* no *TensorFlow*, discutiremos os principais conceitos de *RNNs*. Começaremos examinando a estrutura típica de uma *RNN*, que inclui um componente recursivo para modelar dados de sequência. Em seguida, examinaremos como as ativações dos neurônios são computadas em uma *RNN* típica. Isso criará um contexto para discutirmos os desafios comuns no treinamento de *RNNs* e, em seguida, discutiremos soluções para esses desafios, como *LSTM* e unidades recorrentes fechadas (*GRUs*).\n",
    "\n",
    "### Entendendo o mecanismo de loop *RNN*\n",
    "Vamos começar com a arquitetura de uma *RNN*. A figura a seguir mostra uma *RN* *feedforward* padrão e um *RNN* lado a lado para comparação:\n",
    "\n",
    "![](imagens\\mecanismo_loop_rnn.PNG)\n",
    "\n",
    "Ambas as redes têm apenas uma camada oculta. Nesta representação, as unidades não são exibidas, mas assumimos que a camada de entrada ($\\small x$), a camada oculta ($\\small h$) e a camada de saída ($\\small o$) são vetores que contêm muitas unidades.\n",
    "\n",
    "> ##### Determinando o tipo de saída de um RNN\n",
    "> Essa arquitetura RNN genérica pode corresponder às duas categorias de modelagem de sequência em que a entrada é uma sequência. Normalmente, uma camada recorrente pode retornar uma sequência como saída, $\\small \\left \\langle o^{(1)},o^{(2)},\\cdots, o^{(T)}  \\right \\rangle$, ou simplesmente retornar a última saída (em $\\small t = T$, ou seja, $\\small o^{(T)}$). Assim, pode ser muitos para muitos ou muitos para um se, por exemplo, usarmos apenas o último elemento, $\\small o^{(T)}$, como a saída final.\n",
    ">\n",
    "> Como você verá mais tarde, na *API TensorFlow Keras*, o comportamento de uma camada recorrente em relação ao retorno de uma sequência como saída ou simplesmente usar a última saída pode ser especificado definindo o argumento `return_sequences` como `True` ou `False`, respectivamente.\n",
    "\n",
    "Em uma rede *feedforward* padrão, as informações fluem da entrada para a camada oculta e, em seguida, da camada oculta para a camada de saída. Por outro lado, em uma *RNN*, a camada oculta recebe sua entrada tanto da camada de entrada da etapa de tempo atual quanto da camada oculta da etapa de tempo anterior.\n",
    "\n",
    "O fluxo de informações em etapas de tempo adjacentes na camada oculta permite que a rede tenha uma memória de eventos passados. Esse fluxo de informações geralmente é exibido como um *loop*, também conhecido como **recurrent edge** (borda recorrente) em notação de gráfico, que é como essa arquitetura *RNN* geral recebeu seu nome.\n",
    "\n",
    "Semelhante aos *perceptrons* multicamadas, as *RNN*s podem consistir em várias camadas ocultas. Observe que é uma convenção comum se referir às *RNNs* com uma camada oculta como uma *RNN* de camada única, que não deve ser confundido com as RNs de camada única sem uma camada oculta, como *Adaline* ou *regressão logística*. A figura a seguir ilustra uma *RNN* com uma camada oculta (superior) e uma *RNN* com duas camadas ocultas (inferior):\n",
    "\n",
    "![](imagens\\rnn_oculta.PNG)\n",
    "\n",
    "Para examinar a arquitetura das *RNNs* e o fluxo de informações, pode-se desdobrar uma representação compacta com uma aresta recorrente, que você pode ver na figura anterior.\n",
    "\n",
    "Como sabemos, cada unidade oculta em uma *RN* padrão recebe apenas uma entrada – a pré-ativação de rede associada à camada de entrada. Em contraste, cada unidade oculta em uma *RNN* recebe dois conjuntos distintos de entrada – a pré-ativação da camada de entrada e a ativação da mesma camada oculta da etapa de tempo anterior, $\\small t – 1$.\n",
    "\n",
    "Na primeira etapa de tempo, $\\small t = 0$, as unidades ocultas são inicializadas com zeros ou pequenos valores aleatórios. Então, em um passo de tempo em que $\\small t > 0$, as unidades ocultas recebem sua entrada do ponto de dados no momento atual, $\\small x^{(T)}$, e os valores anteriores das unidades ocultas em $\\small t – 1$, indicados como $\\small h^{(t-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma, no caso de uma RNN multicamada, podemos resumir o fluxo de informações da seguinte forma:\n",
    "* $\\small layer$ = 1: Aqui, a camada oculta é representada como $h_1^{(t)}$ e recebe sua entrada do ponto de dados, $x^{(t)}$, e os valores ocultos na mesma camada, mas no passo de tempo anterior, $h_1^{(t - 1)}$.\n",
    "* $\\small layer$ = 2: A segunda camada oculta, $h_2^{(t)}$ , recebe suas entradas das saídas da camada abaixo na etapa de tempo atual $(o_1^{(t)})$ e seus próprios valores ocultos da etapa de tempo anterior, $h_2^{(t-1)}$.\n",
    "\n",
    "Como, neste caso, cada camada recorrente deve receber uma sequência como entrada, todas as camadas recorrentes, exceto a última, devem retornar uma sequência como saída (ou seja, `return_sequences=True`). O comportamento da última camada recorrente depende do tipo de problema.\n",
    "\n",
    "### Computando ativações em uma *RNN*\n",
    "Agora que você entende a estrutura e o fluxo geral de informações em uma *RNN*, vamos ser mais específicos e calcular as ativações reais das camadas ocultas, bem como a camada de saída. Para simplificar, consideraremos apenas uma única camada oculta; no entanto, o mesmo conceito se aplica às *RNNs* multicamadas.\n",
    "\n",
    "Cada aresta direcionada (as conexões entre caixas) na representação de uma *RNN* que acabamos de ver está associada a uma matriz de pesos. Esses pesos não dependem do tempo, $\\small t$; portanto, eles são compartilhados ao longo do eixo do tempo. As diferentes matrizes de peso em uma *RNN* de camada única são as seguintes:\n",
    "* $\\small \\textbf{W}_{xh}$: A matriz de peso entre a entrada, $\\small x^{(t)}$, e a camada oculta, $\\small \\textbf{h}$\n",
    "* $\\small \\textbf{W}_{hh}$: A matriz de peso associada à aresta recorrente\n",
    "* $\\small \\textbf{W}_{ho}$: A matriz de peso entre a camada oculta e a camada de saída\n",
    "\n",
    "Essas matrizes de peso são representadas na figura a seguir:\n",
    "\n",
    "![](imagens\\matriz_pesos.PNG)\n",
    "\n",
    "Em certas implementações, você pode observar que as matrizes de peso, $\\small \\textbf{W}_{xh}$ e $\\small \\textbf{W}_{hh}$, são concatenadas a uma matriz combinada, $\\small \\textbf{W}_{h} = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$. Mais adiante, faremos uso dessa notação também.\n",
    "\n",
    "A computação das ativações é muito semelhante aos *perceptrons* multicamadas padrão e outros tipos de *RNs* *feedforward*. Para a camada oculta, a entrada líquida, $\\small \\textbf{z}_ h$ (pré-ativação), é calculada através de uma combinação linear, ou seja, calculamos a soma das multiplicações das matrizes de peso com os vetores correspondentes e somamos a unidade de polarização:\n",
    "$$\n",
    "\\small \\textbf{z}_h^{(t)} = \\text{W}_{xh} \\textbf{x}^{(t)} + \\textbf{W}_{hh}\\textbf{h}^{(t-1)} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "Então, as ativações das unidades ocultas na etapa do tempo, $\\small t$, são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^{(t)} = \\phi_h (\\textbf{z}_h^{(t)}) = \\phi_h (\\textbf{W}_{xh}\\textbf{x}^{(t)} +  \\textbf{W}_{hh}\\textbf{x}^{(t-1)} + \\textbf{b}_h)\n",
    "$$\n",
    " \n",
    "Aqui, $\\small \\textbf{b}_h$ é o vetor de polarização para as unidades ocultas e $\\phi_h(\\cdot)$ é a função de ativação da camada oculta.\n",
    "\n",
    "Caso você queira usar a matriz de peso concatenada, $\\small \\textbf{W}_h = [\\textbf{W}_{xh}; \\textbf{W}_{hh}]$, a fórmula para calcular as unidades ocultas mudará da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\phi_h \\left ([\\textbf{W}_{xh}; \\textbf{W}_{hh}] \\begin{bmatrix}\n",
    "\\textbf{x}^{(t)}\\\\ \n",
    "\\textbf{h}^{t-1}\n",
    "\\end{bmatrix}       \n",
    "+ \\textbf{b}_h          \\right )\n",
    "$$\n",
    "\n",
    "Uma vez computadas as ativações das unidades ocultas no passo de tempo atual, serão computadas as ativações das unidades de saída, como segue:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{o}^{(t)} = \\phi_0 (\\textbf{W}_{ho}\\textbf{h}^{(t)} + \\textbf{b}_0)\n",
    "$$\n",
    "\n",
    "Para ajudar a esclarecer ainda mais, a figura a seguir mostra o processo de cálculo dessas ativações com ambas as formulações:\n",
    "\n",
    "![](imagens\\matriz_sequencial.PNG)\n",
    "\n",
    "> ##### Treinando RNNs usando retropropagação ao longo do tempo (BPTT)\n",
    "> O algoritmo de aprendizado para RNNs foi introduzido em 1990: Backpropagation Through Time: What It Does and How to Do It (Paul Werbos, Proceedings of IEEE, 78(10): 1550-1560, 1990). A derivada dos gradientes pode ser um pouco complicada, mas a ideia básica é que a perda total, $\\small L$, é a soma de todas as funções de perda nos momentos $\\small t = 1$ a $\\small t = T$:\n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^{(t)}\n",
    "$$\n",
    "\n",
    "Como a perda no tempo $\\small t$ depende das unidades ocultas em todos os passos de tempo anteriores $\\small 1 : t$, o gradiente será calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{W}_{hh}} = \\dfrac{\\partial L^{(t)}}{\\partial \\textbf{o}^{(t)}} \\times \\dfrac{\\partial \\textbf{o}^{(t)}}{\\partial \\textbf{h}^{(t)}} \\times \\left ( \\sum^t_{k=1}\\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}} \\times \\dfrac{\\partial \\textbf{h}^{(k)}}{\\partial \\textbf{W}_{hh}} \\right )\n",
    "\n",
    "$$\n",
    "\n",
    "Aqui, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ é calculado como uma multiplicação de passos de tempo adjacentes:\n",
    "$$\n",
    "\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{k}} = \\prod ^t_{i=k+1} \\dfrac{\\partial \\textbf{h}^{(i)}}{\\partial \\textbf{h}^{(i-1)}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrência oculta versus recorrência de saída\n",
    "Até agora, você viu redes recorrentes nas quais a camada oculta tem a propriedade recorrente. No entanto, observe que existe um modelo alternativo em que a conexão recorrente vem da camada de saída. Nesse caso, as ativações líquidas da camada de saída na etapa de tempo anterior, $\\small \\textbf{o}^{(t-1)}$, podem ser adicionadas de duas maneiras:\n",
    "* Para a camada oculta no passo de tempo atual, $\\small \\textbf{h}^t$ (mostrado na figura a seguir como recorrência de saída para oculta)\n",
    "* Para a camada de saída no passo de tempo atual, $\\small \\textbf{o}^{t}$ (mostrado na figura a seguir como recorrência de saída para saída)\n",
    "\n",
    "![](imagens\\recorrencia_saida.PNG)\n",
    "\n",
    "Conforme mostrado na figura anterior, as diferenças entre essas arquiteturas podem ser vistas claramente nas conexões recorrentes. Seguindo nossa notação, os pesos associados à conexão recorrente serão denotados para a recorrência oculta para oculta por $\\small \\textbf{W}_{hh}$, para a recorrência saída para oculta por $\\small \\textbf{W}_{oh}$, e para a recorrência saída para saída por $\\small \\textbf{W}_{oo}$. Em alguns artigos da literatura, os pesos associados às conexões recorrentes também são denotados por $\\small \\textbf{W}_{rec}$.\n",
    "\n",
    "Para ver como isso funciona na prática, vamos calcular manualmente a passagem direta para um desses tipos recorrentes. Usando a *API TensorFlow Keras*, uma camada recorrente pode ser definida por meio da *SimpleRNN*, que é semelhante à recorrência de saída para saída. No código a seguir, criaremos uma camada recorrente do *SimpleRNN* e executaremos uma passagem direta em uma sequência de entrada de comprimento 3 para calcular a saída. Também calcularemos manualmente a passagem direta e compararemos os resultados com os do *SimpleRNN*. Primeiro, vamos criar a camada e atribuir os pesos para nossos cálculos manuais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: (5, 2)\n",
      "W_oo shape: (2, 2)\n",
      "b_h shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(\n",
    "    units=2, use_bias=True, \n",
    "    return_sequences=True)\n",
    "rnn_layer.build(input_shape=(None, None, 5))\n",
    "\n",
    "w_xh, w_oo, b_h = rnn_layer.weights\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_oo shape:', w_oo.shape)\n",
    "print('b_h shape:', b_h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forma de entrada para esta camada é `(None, None, 5)`, onde a primeira dimensão é a dimensão do lote (usando `None` para tamanho de lote variável), a segunda dimensão corresponde à sequência (usando `None` para o comprimento variável da sequência) e a última dimensão corresponde às características. Observe que definimos `return_sequences=True`, que, para uma sequência de entrada de comprimento 3, resultará na sequência de saída $\\small \\left \\langle \\textbf{o}^{(0)},\\textbf{o}^{(1)}, \\textbf{o}^{(2)} \\right \\rangle$. Caso contrário, ele retornaria apenas a saída final, $\\textbf{o}^{(2)}$.\n",
    "\n",
    "Agora, vamos chamar a passagem direta no `rnn_layer` e calcular manualmente as saídas em cada passo de tempo e compará-las:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[0.41464037 0.96012145]]\n",
      "   Output (manual) : [[0.39240566 0.74433106]]\n",
      "   SimpleRNN output: [0.39240566 0.74433106]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[0.82928073 1.9202429 ]]\n",
      "   Output (manual) : [[0.80116504 0.9912947 ]]\n",
      "   SimpleRNN output: [0.80116504 0.9912947 ]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[1.243921  2.8803642]]\n",
      "   Output (manual) : [[0.95468265 0.9993069 ]]\n",
      "   SimpleRNN output: [0.95468265 0.9993069 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = tf.convert_to_tensor(\n",
    "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "\n",
    "## output of SimepleRNN:\n",
    "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(len(x_seq)):\n",
    "    xt = tf.reshape(x_seq[t], (1, 5))\n",
    "    print('Time step {} =>'.format(t))\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = tf.matmul(xt, w_xh) + b_h    \n",
    "    print('   Hidden          :', ht.numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_o = out_man[t-1]\n",
    "    else:\n",
    "        prev_o = tf.zeros(shape=(ht.shape))\n",
    "        \n",
    "    ot = ht + tf.matmul(prev_o, w_oo)\n",
    "    ot = tf.math.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.numpy())\n",
    "    print('   SimpleRNN output:'.format(t), output[0][t].numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso cálculo direto manual, usamos a função de ativação da tangente hiperbólica (tanh), uma vez que também é usada na `SimpleRNN` (a ativação padrão). Como você pode ver nos resultados impressos, as saídas dos cálculos de encaminhamento manual correspondem exatamente à saída da camada `SimpleRNN` em cada etapa de tempo. Espero que esta tarefa prática tenha esclarecido você sobre os mistérios das redes recorrentes.\n",
    "\n",
    "### Os desafios de aprender interações de longo prazo\n",
    "\n",
    "BPTT, que foi brevemente mencionado anteriormente, apresenta alguns novos desafios. Por causa do fator multiplicativo,$\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$, ao calcular os gradientes de uma função de perda, surgem os chamados problemas de **gradiente de fuga e explosão**. Esses problemas são explicados pelos exemplos na figura a seguir, que mostra uma RNN com apenas uma unidade oculta para simplificar:\n",
    "\n",
    "![](imagens\\gradiente_fuga_explosao.PNG)\n",
    "\n",
    "\n",
    "Basicamente, $\\small \\dfrac{\\partial \\textbf{h}^{(t)}}{\\partial \\textbf{h}^{(k)}}$ tem $\\small t – k$ multiplicações; portanto, multiplicar o peso, $\\small w$, por ele mesmo $\\small t – k$ vezes resulta em um fator, $\\small w^{t-w}$ . Como resultado, se $\\small |w|< 1$, esse fator se torna muito pequeno quando $\\small t – k$ é grande. Por outro lado, se o peso da aresta recorrente for $\\small |w|> 1$ , então $w^{t-k}$ se torna muito grande quando $\\small t – k$ é grande. Observe que $\\small t – k$ grande se refere a dependências de longo alcance. Podemos ver que uma solução ingênua para evitar gradientes de fuga ou explosão pode ser alcançada garantindo $\\small |w| = 1$.\n",
    "\n",
    "Na prática, existem pelo menos três soluções para este problema:\n",
    "* *Gradient clipping*\n",
    "* *TBPTT*\n",
    "* *LSTM*\n",
    "\n",
    "Usando o *Gradient clipping*, especificamos um valor de corte ou limite para os gradientes e atribuímos esse valor de corte aos valores de gradiente que excedem esse valor. Em contraste, o *TBPTT* simplesmente limita ao número de passos de tempo que o sinal pode retropropagar após cada passagem para frente. Por exemplo, mesmo que a sequência tenha 100 elementos ou etapas, podemos retropropagar apenas as 20 etapas de tempo mais recentes.\n",
    "\n",
    "Embora tanto o *Gradient clipping* quanto o *TBPTT* possam resolver o problema do **gradiente explosivo**, o truncamento limita o número de etapas que o gradiente pode efetivamente retornar e atualizar adequadamente os pesos. Por outro lado, o *LSTM*, projetado em 1997 por *Sepp Hochreiter* e *Jürgen Schmidhuber*, tem tido mais sucesso em eliminar e explodir problemas de gradiente ao modelar dependências de longo alcance por meio de\n",
    "o uso de células de memória. Vamos discutir *LSTM* com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Células de memória de longo prazo\n",
    "\n",
    "Como afirmado anteriormente, os *LSTMs* foram introduzidos pela primeira vez para superar o problema do gradiente de fuga. O bloco de construção de um *LSTM* é uma célula de memória, que essencialmente representa ou substitui a camada oculta de RNNs padrão.\n",
    "\n",
    "Em cada célula de memória, há uma aresta recorrente que tem o peso desejável, $\\small w = 1$, como discutimos, para superar os problemas de gradiente de fuga e explosão. Os valores associados a essa borda recorrente são chamados coletivamente de estado da célula. A estrutura desdobrada de uma célula *LSTM* moderna é mostrada na figura a seguir:\n",
    "\n",
    "![](imagens\\ltsm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o estado da célula do passo de tempo anterior, $\\small \\textbf{C}^{(t-1)}$, é modificado para obter o estado da célula no passo de tempo atual, $\\small \\textbf{C}^{(t)}$, sem ser multiplicado diretamente por nenhum fator de peso. O fluxo de informação nesta célula de memória é controlado por várias unidades de computação (frequentemente chamadas de portas) que serão descritas aqui. Na figura anterior,$\\small \\bigodot$ refere-se ao produto **elemento-a-elemento** (multiplicação elemento-a-elemento) e $\\small \\bigoplus$ significa s**oma-elemento** (adição elemento-a-elemento). Além disso, $\\small \\textbf{x}^{(t)}$ refere-se aos dados de entrada no tempo $\\small t$, e $\\small \\textbf{h}^{(t-1)}$ indica as unidades ocultas no tempo $\\small t – 1$. Quatro caixas são indicadas com uma função de ativação, seja a função sigmóide ($\\small \\sigma$) ou $\\small tanh$, e um conjunto de pesos; essas caixas aplicam uma combinação linear realizando multiplicações de matriz-vetor em suas entradas (que são $\\small \\textbf{h}^{(t-1)}$ e $\\small \\textbf{x}^{(t)}$). Essas unidades de computação com funções de ativação sigmóides, cujas unidades de saída são passadas por $\\small \\bigodot$, são chamadas de portas.\n",
    "\n",
    "Em uma célula *LSTM*, existem três tipos diferentes de portas, que são conhecidas como **porta de esquecimento**, porta de entrada e porta de saída:\n",
    "\n",
    "* A **porta de esquecimento** ($\\small f_t$) permite que a célula de memória redefina o estado da célula sem crescer indefinidamente. Na verdade, o portão de esquecimento decide quais informações podem passar e quais informações devem ser suprimidas. Agora, $\\small f_t$ é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small f_t = \\sigma(\\textbf{W}_{xf}\\textbf{x}^{(t)} + \\textbf{W}_{hf}\\textbf{h}^{(t-1)} + \\textbf{b}_f)\n",
    "$$\n",
    "\n",
    "\n",
    "* A **porta de entrada** ($\\small i_t$) e o **valor candidato** ($\\breve{C}_t$) são responsáveis ​​por atualizar o estado da célula. Eles são calculados da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small i_t = \\sigma ( \\textbf{W}_{xi}\\textbf{x}^{(t)} +  \\textbf{W}_{hi}\\textbf{h}^{(t-1)} + \\textbf{b}_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\small \\breve{C}_t = tanh (\\textbf{W}_{xc}\\textbf{x}^{(t)} + \\textbf{W}_{hc}\\textbf{h}^{(t-1)} + \\textbf{b}_c)\n",
    "$$\n",
    "\n",
    "O estado da célula no tempo t é calculado da seguinte forma:\n",
    "\n",
    "$$\n",
    "C^{t} = (C^{(t-1)} \\odot f_t) \\otimes (i_t \\odot \\breve{C}_t)\n",
    "$$\n",
    "\n",
    "A porta de saída ($\\small \\textbf{o}_t$) decide como atualizar os valores das unidades ocultas:\n",
    "\n",
    "$$\n",
    "\\small o_t = \\sigma (\\textbf{W}_{xo}\\textbf{x}^{(t)} + \\textbf{W}_{ho}\\textbf{h}^{(t-1)} + \\textbf{b}_o)\n",
    "$$\n",
    "\n",
    "Dado isso, as unidades ocultas no passo de tempo atual são calculadas da seguinte forma:\n",
    "\n",
    "$$\n",
    "\\small \\textbf{h}^{(t)} = \\textbf{o}_t \\odot \\: tanh (\\textbf{C}^{(t)})\n",
    "$$\n",
    "\n",
    "A estrutura de uma célula *LSTM* e seus cálculos subjacentes podem parecer muito complexos e difíceis de implementar. No entanto, a boa notícia é que o *TensorFlow* já implementou tudo em funções otimizadas de wr*apper, o que nos permite definir nossas células *LSTM* de maneira fácil e eficiente. Aplicaremos RNNs e LSTMs a conjuntos de dados do mundo real posteriormente.\n",
    "\n",
    "> #### Outros modelos RNN avançados\n",
    "> *LSTMs* fornecem uma abordagem básica para modelar dependências de longo alcance em sequências. No entanto, é importante notar que existem muitas variações de *LSTMs* descritas na literatura (An Empirical Exploration of Recurrent Network Architectures, Rafal Jozefowicz, Wojciech Zaremba e Ilya Sutskever, Proceedings of ICML, 2342-2350, 2015).\n",
    ">\n",
    "> Também merece destaque uma abordagem mais recente, *Gated Recurrent Unit* (GRU), proposta em 2014. As GRUs possuem uma arquitetura mais simples que as *LSTMs*; portanto, eles são computacionalmente mais eficientes, enquanto seu desempenho em algumas tarefas, como modelagem de música polifônica, é comparável aos *LSTMs*.\n",
    "\n",
    "### Implementando RNNs para modelagem de sequência no TensorFlow\n",
    "\n",
    "Agora que abordamos a teoria subjacente por trás das *RNNs*, estamos prontos para passar para a parte mais prática: implementar *RNNs* no TensorFlow. Durante o restante deste capítulo, aplicaremos *RNNs* a duas tarefas problemáticas comuns:\n",
    "1. Análise de sentimentos\n",
    "2. Modelagem de linguagem\n",
    "\n",
    "Esses dois projetos, que apresentaremos, são fascinantes e envolventes. Assim, em vez de fornecer o código de uma só vez, dividiremos a implementação em várias etapas e discutiremos o código em detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto um – prevendo o sentimento das críticas de filmes do IMDb\n",
    "\n",
    "Nesta seção e nas subseções seguintes, implementaremos uma *RNN* multicamada para análise de sentimentos usando uma arquitetura muitos-para-um.\n",
    "\n",
    "Na próxima seção, implementaremos um *RNN* muitos-para-muitos para uma aplicação de modelagem de linguagem. Embora os exemplos escolhidos sejam propositadamente simples para introduzir os principais conceitos de *RNNs*, a modelagem de linguagem tem uma ampla gama de aplicações interessantes, como a construção de chatbots – dando aos computadores a capacidade de conversar e interagir diretamente com humanos.\n",
    "\n",
    "### Preparando os dados de revisão do filme\n",
    "Nas etapas de pré-processamento, criamos um conjunto de dados limpo chamado `movie_data.csv`, que usaremos novamente agora. Primeiro, importaremos os módulos necessários e leremos os dados em um `DataFrame` pandas, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se de que esse quadro de dados, `df`, consiste em duas colunas, chamadas `'review'` e `'sentiment'`, onde `'review'` contém o texto das resenhas de filmes (os recursos de entrada) e `'sentiment'` representa o rótulo de destino que queremos prever (0 refere-se ao sentimento negativo e 1 refere-se ao sentimento positivo).\n",
    "\n",
    "O componente de texto dessas resenhas de filmes são sequências de palavras, e o modelo *RNN* classifica cada sequência como uma resenha positiva (1) ou negativa (0). No entanto, antes de podermos alimentar os dados em um modelo *RNN*, precisamos aplicar várias etapas de pré-processamento:\n",
    "1. Crie um objeto de conjunto de dados do *TensorFlow* e divida-o em partições separadas de treinamento, teste e validação.\n",
    "2. Identifique as palavras exclusivas no conjunto de dados de treinamento.\n",
    "3. Mapeie cada palavra exclusiva para um número inteiro exclusivo e codifique o texto da revisão em números inteiros codificados (um índice de cada palavra exclusiva).\n",
    "4. Divida o conjunto de dados em minilotes como entrada para o modelo.\n",
    "\n",
    "Vamos prosseguir com a primeira etapa: criar um conjunto de dados do *TensorFlow* a partir deste quadro de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'In 1974, the teenager Martha Moxley (Maggie Grace)' 1\n",
      "b'OK... so... I really like Kris Kristofferson and h' 0\n",
      "b'***SPOILER*** Do not read this, if you think about' 0\n"
     ]
    }
   ],
   "source": [
    "# Passo 1: Criando um Dataset\n",
    "\n",
    "target = df.pop('sentiment')\n",
    "\n",
    "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "    (df.values, target.values))\n",
    "\n",
    "## inspection:\n",
    "for ex in ds_raw.take(3):\n",
    "    tf.print(ex[0].numpy()[0][:50], ex[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos dividi-lo em conjuntos de dados de treinamento, teste e validação. Todo o conjunto de dados contém 50.000 exemplos. Manteremos os primeiros 25.000 exemplos para avaliação (conjunto de dados de teste de retenção) e, em seguida, 20.000 exemplos serão usados para treinamento e 5.000 para validação. O código é o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "ds_raw = ds_raw.shuffle(50000, reshuffle_each_iteration=False)\n",
    "\n",
    "ds_raw_test = ds_raw.take(25000)                 # Separa 25.000 amostras das 50.000\n",
    "ds_raw_train_valid = ds_raw.skip(25000)          # Pula as 25.000 amostras do conjunto anterior e salva em uma variável \n",
    "ds_raw_train = ds_raw_train_valid.take(20000)    # Da variável anterior, vamos pegas 20.000 e salvar numa variável.\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)    # Vamos pular 20.000 amostras e salvar numa variável. Que será somente 5.000 (que sobrou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preparar os dados para entrada em uma RN, precisamos codificá-los em valores numéricos, conforme mencionado nas etapas 2 e 3. Para fazer isso, primeiro encontraremos as palavras exclusivas (*tokens*) no conjunto de dados de treinamento. Embora encontrar *tokens* exclusivos seja um processo para o qual podemos usar conjuntos de dados *Python*, pode ser mais eficiente usar a classe `Counter` do pacote `collections`, que faz parte da biblioteca padrão do *Python*.\n",
    "\n",
    "No código a seguir, instanciaremos um novo objeto `Counter` (`token_counts`) que coletará as frequências de palavras exclusivas. Observe que neste aplicativo específico (e em contraste com o modelo *bag-of-words*), estamos interessados apenas no conjunto de palavras únicas e não exigiremos a contagem de palavras, que é criada como um produto secundário. Para dividir o texto em palavras (ou *tokens*), o pacote `tensorflow_datasets` fornece uma classe `Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passo 2: Encontrar tokens (palavras) unicas\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c198a0bf143da720db8a1ed2fdc43342371f98b364fdf2ef3c0c72d17a471d7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
