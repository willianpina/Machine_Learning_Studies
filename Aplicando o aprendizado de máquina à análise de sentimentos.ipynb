{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Aplicando o aprendizado de máquina à análise de sentimentos</h1>\n",
    "\n",
    "<p align='center'><img src= https://cibersistemas.pt/wp-content/uploads/2020/09/wall-5.jpeg width='500'></p>\n",
    "\n",
    "\n",
    "Na era moderna da internet e das mídias sociais, as opiniões, avaliações e recomendações das pessoas se tornaram um recurso valioso para a ciência política e os negócios. Graças às tecnologias modernas, agora podemos coletar e analisar esses dados com mais eficiência. Neste capítulo, vamos nos aprofundar em um subcampo do processamento de linguagem natural (NLP) chamado análise de sentimentos e aprender a usar algoritmos de aprendizado de máquina para classificar documentos com base em sua polaridade: a atitude do escritor. Em particular, vamos trabalhar com um conjunto de dados de 50.000 críticas de filmes do Internet Movie Database (IMDb) e construir um preditor que possa distinguir entre críticas positivas e negativas.\n",
    "\n",
    "## Preparando os dados de revisão de filme do IMDb para processamento de texto\n",
    "Como mencionado, a análise de sentimentos, às vezes também chamada de **mineração de opinião**, é uma subdisciplina popular do campo mais amplo da *NLP*; preocupa-se em analisar a polaridade dos documentos. Uma tarefa popular na análise de sentimentos é a classificação de documentos com base nas opiniões ou emoções expressas dos autores em relação a um tópico específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Election is a Chinese mob movie, or triads in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was just watching a Forensic Files marathon ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police Story is a stunning series of set piece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Election is a Chinese mob movie, or triads in ...          1\n",
       "1  I was just watching a Forensic Files marathon ...          0\n",
       "2  Police Story is a stunning series of set piece...          1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apresentando o modelo de *bag-of-words*\n",
    "\n",
    "Apresentaremos o modelo *bag-of-words*, que nos permite representar o texto como vetores numéricos de recursos. A ideia por trás do *bag-of-words* é bastante simples e pode ser resumido da seguinte forma:\n",
    "\n",
    "1. Criamos um vocabulário de tokens exclusivos — por exemplo, palavras — de todo o conjunto de documentos.\n",
    "2. Construímos um vetor de características de cada documento que contém as contagens de quantas vezes cada palavra ocorre no documento em particular.\n",
    "\n",
    "Como as palavras únicas em cada documento representam apenas um pequeno subconjunto de todas as palavras no vocabulário do *bag-of-words*, os vetores de recursos consistirão principalmente em zeros, e é por isso que os chamamos de esparsos. Não se preocupe se isso soar muito abstrato; mais pra frente, veremos o passo a passo do processo de criação de um modelo simples de um *bag-of-words*.\n",
    "\n",
    "### Transformando palavras em vetores de recursos\n",
    "Para construir um modelo *bag-of-words* baseado na contagem de palavras nos respectivos documentos, podemos usar a classe *CountVectorizer* implementada no *scikit-learn*. Como você verá no código a seguir, o *CountVectorizer* pega uma matriz de dados de texto, que podem ser documentos ou frases, e constrói o modelo *bag-of-words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "# Agora vamos imprimir o conteúdo do vocabulário para entender melhor os conceitos subjacentes:\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver ao executar o comando anterior, o vocabulário é armazenado em um dicionário *Python* que mapeia as palavras exclusivas para índices inteiros. Em seguida, vamos imprimir os vetores de recursos que acabamos de criar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada posição de índice nos vetores de recursos mostrados aqui corresponde aos valores inteiros armazenados como itens de dicionário no vocabulário *CountVectorizer*. Por exemplo, a primeira *feature* na posição de índice 0 se assemelha à contagem da palavra 'and', que ocorre apenas no último documento, e a palavra 'is', na posição de índice 1 (a segunda feição nos vetores do documento), ocorre nas três frases. Esses valores nos vetores de características também são chamados de frequências de termos brutos: tf(t, d)— número de vezes que um termo, *t*, ocorre em um documento, **d*. Deve-se notar que, no modelo *bag-of-words*, a ordem das palavras ou dos termos em uma frase ou documento não importa.\n",
    "\n",
    "A ordem em que as frequências dos termos aparecem no vetor de características é derivada dos índices de vocabulário, que geralmente são atribuídos em ordem alfabética.\n",
    "\n",
    "<blockquote>\n",
    "<h4><i>Modelos de N-gram</i></h4>\n",
    "<p alig='justify'>A sequência de itens no modelo de <i>bag-of-words</i> que acabamos de criar também é chamada de modelo de <i>1-gram</i> ou <i>unigram</i> – cada item ou <i>token</i> no vocabulário representa <u>uma única palavra</u>. De maneira geral, as sequências contíguas de itens no <i>NLP</i>  – palavras, letras ou símbolos – também são chamadas de <i>n-grams</i>. A escolha do número, <i>n</i>, no modelo <i>n-gram</i> depende da aplicação particular; por exemplo, um estudo de <i>Ioannis Kanaris</i> e outros revelou que <i>n-grams</i> de <b>tamanho 3 e 4</b> rendem bons desempenhos na filtragem anti-spam de mensagens de e-mail.</p>\n",
    "\n",
    "<p alig='justify'>Para resumir o conceito da representação <i>n-gram</i>, as representações de <i>1-gram</i> e <i>2-gram</i> do nosso primeiro documento \"the sun is shining\" seriam construídas da seguinte forma:\n",
    "<ul>\n",
    "<li> <i>1-gram</i>: \"the\", \"sun\", \"is\", \"shining\"</li>\n",
    "<li> <i>2-gram</i>: \"the sun\", \"sun is\", \"is shining\"</li>\n",
    "</ul>\n",
    "<p alig='justify'>A classe <i>CountVectorizer</i> no <i>scikit-learn</i> nos permite usar diferentes modelos de <i>n-gram</i> por meio de seu parâmetro <i>ngram_range</i>. Embora uma representação de <i>1-gram</i> seja usada por padrão, podemos alternar para uma representação de <i>2-gram</i> inicializando uma nova instância <i>CountVectorizer</i> com <b><i>ngram_range=(2,2)</i></b>.\n",
    "</blockquote>\n",
    "\n",
    "### Avaliando a relevância da palavra por meio da frequência do documento inversa da frequência do termo\n",
    "Quando estamos analisando dados de texto, geralmente encontramos palavras que ocorrem em vários documentos de ambas as classes. Essas palavras que ocorrem com frequência geralmente não contêm informações úteis ou discriminatórias. Agora, falaremos sobre uma técnica útil chamada o **termo de frequência e frequência documento inverso (tf-idf)**, que pode ser usado para reduzir o peso dessas palavras que ocorrem com frequência nos vetores de recursos.\n",
    "\n",
    "A biblioteca *scikit-learn* implementa ainda outro transformador, a classe `TfidfTransformer`, que pega as frequências de termos brutos da classe *CountVectorizer* como entrada e as transforma em `tf-idfs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você viu anteriomente, a palavra 'is' teve a maior frequência de termos no terceiro documento, sendo a palavra mais frequente. No entanto, após transformar o mesmo vetor de características em *tf-idfs*, a palavra 'is' agora está associada a um *tf-idf* relativamente pequeno (0,45) no terceiro documento, pois também está presente no primeiro e no segundo documento e, portanto, é improvável que contenha qualquer informação discriminatória útil. No entanto, se tivéssemos calculado manualmente os *tf-idfs* dos termos individuais em nossos vetores de recursos, teríamos notado que o `TfidfTransformer` calcula o *tf-idfs* de forma ligeiramente diferente em comparação com as equações padrão dos livros didáticos que definimos anteriormente.\n",
    "\n",
    "### Limpando dados de texto\n",
    "\n",
    "Aprendemos sobre o modelo *bag-of-words*, frequências de termos e *tf-idfs*. No entanto, o primeiro passo importante antes de construirmos nosso modelo de *bag-of-words* é limpar os dados de texto removendo todos os caracteres indesejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"film ends at the 65 or 70-minute mark, there are still a couple big surprises waiting. Simon Yam was my favorite character here and sort of anchors the picture.<br /><br />Election was quite the award winner at last year's Hong Kong Film Awards, winning for best actor (Tony Leung), best picture, best director (Johnny To, who did Heroic Trio!!), and best screenplay. It also had nominations for cinematography, editing, film score (which I loved), and three more acting performances (including Yam).\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para ilustrar por que isso é importante, vamos exibir os últimos 500 caracteres\n",
    "# do primeiro documento no conjunto de dados de revisão de filme reordenado\n",
    "\n",
    "df.loc[0,'review'][-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver aqui, o texto contém marcação *HTML*, bem como pontuação e outros caracteres que não são letras. Embora a marcação *HTML* não contenha muitas semânticas úteis, os sinais de pontuação podem representar informações úteis e adicionais em determinados contextos de *NLP*. No entanto, para simplificar, agora removeremos todos os sinais de pontuação, exceto os caracteres de *emoticon*, como *:)*, pois certamente são úteis para análise de sentimentos. Para realizar essa tarefa, usaremos a biblioteca *mlxtend*, conforme mostrado aqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', ':)', ':(', ':-)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.text import tokenizer_words_and_emoticons\n",
    "\n",
    "tokenizer_words_and_emoticons(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "<h5>Lidando com a capitalização da palavra</h5>\n",
    "<p align=\"justify\">No contexto desta análise, assumimos que a capitalização de uma palavra – por exemplo, se ela aparece no início de uma frase – não contém informações semanticamente relevantes. No entanto, observe que há exceções; por exemplo, removemos a notação de nomes próprios. Mas, novamente, no contexto desta análise, é uma suposição simplificadora que a letra maiúscula não contém informações relevantes para a análise de sentimentos.</p>\n",
    "</blockquote>\n",
    "\n",
    "Eventualmente, adicionamos os *emoticons* armazenados temporariamente ao final da *string *do documento processado. Além disso, removemos o caractere de nariz (- em :-)) dos *emoticons* para consistência.\n",
    "\n",
    "Embora a adição dos caracteres de *emoticon* ao final das sequências de documentos limpados possa não parecer a abordagem mais elegante, devemos observar que a ordem das palavras não importa em nosso modelo de *bag-of-words* se nosso vocabulário consiste em apenas *tokens* de uma palavra.\n",
    "\n",
    "### Processando documentos em tokens\n",
    "Depois de preparar com sucesso o conjunto de dados de resenhas de filmes, agora precisamos pensar em como dividir os corpora de texto em elementos individuais. Uma maneira de tokenizar documentos é dividi-los em palavras individuais, dividindo os documentos limpos em seus caracteres de espaço em branco.\n",
    "\n",
    "No contexto de tokenização, outra técnica útil é retirar o radical das palavras (*word stemming*), que é o processo de transformar uma palavra em sua forma raiz. Ele nos permite mapear palavras relacionadas ao mesmo radical. O algoritmo de *lematização* original foi desenvolvido por *Martin F. Porter*, em 1979, e, portanto, é conhecido como algoritmo de *lematização* de *Porter*. O *Natural Language Toolkit* (NLTK, http://www.nltk.org) para *Python* implementa o algoritmo de determinação de *Porter*, que usaremos no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenizer_words_and_emoticons('runners like running and thus they run')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o *PorterStemmer* do pacote *nltk*, modificamos nossa função *tokenizer* para reduzir as palavras à sua forma raiz, o que foi ilustrado pelo exemplo simples acima em que a palavra *'running'* foi derivada para sua forma raiz *'run'*.\n",
    "\n",
    "<blockquote>\n",
    "<h5>Algoritmos de derivação</h5>\n",
    "<p>O algoritmo de <i>stemming de Porter</i> é provavelmente o algoritmo de <i>stemming</i> mais antigo e simples. Outros algoritmos de lematização populares incluem o mais recente lematizador <i>Snowball</i> (<i>Porter2</i> ou lematizador inglês) e o lematizador <i>Lancaster</i> (lematizador Paice/Husk). Enquanto as raízes <i>Snowball</i> e <i>Lancaster</i> são mais rápidas do que a original <i>Porter</i>, a <i>Lancaster</i> também é notória por ser mais agressiva que a <i>Porter</i>. Esses algoritmos alternativos também estão disponíveis através do pacote NLTK (http://www.nltk.org/api/nltk.stem.html).\n",
    "\n",
    "<p>Enquanto a <i>Stemming</i> criar palavras não reais, como 'thu' (de 'thus' (\"assim\", do inglês)), como mostrado no exemplo anterior, uma técnica chamada lematização visa obter as formas canônicas (gramaticalmente corretas) de palavras individuais - as chamadas <i>lemmas</i>. No entanto, a lematização é <b>computacionalmente mais difícil e cara</b> em comparação com a <i>stemming</i> e, na prática, observa-se que a <i>stemming</i> e a lematização tem pouco impacto no desempenho da classificação de texto.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film',\n",
       " 'ends',\n",
       " 'at',\n",
       " 'the',\n",
       " '65',\n",
       " 'or',\n",
       " '70',\n",
       " 'minute',\n",
       " 'mark',\n",
       " 'there',\n",
       " 'are',\n",
       " 'still',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'big',\n",
       " 'surprises',\n",
       " 'waiting',\n",
       " 'simon',\n",
       " 'yam',\n",
       " 'was',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'character',\n",
       " 'here',\n",
       " 'and',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'anchors',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'election',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'the',\n",
       " 'award',\n",
       " 'winner',\n",
       " 'at',\n",
       " 'last',\n",
       " 'year',\n",
       " 's',\n",
       " 'hong',\n",
       " 'kong',\n",
       " 'film',\n",
       " 'awards',\n",
       " 'winning',\n",
       " 'for',\n",
       " 'best',\n",
       " 'actor',\n",
       " 'tony',\n",
       " 'leung',\n",
       " 'best',\n",
       " 'picture',\n",
       " 'best',\n",
       " 'director',\n",
       " 'johnny',\n",
       " 'to',\n",
       " 'who',\n",
       " 'did',\n",
       " 'heroic',\n",
       " 'trio',\n",
       " 'and',\n",
       " 'best',\n",
       " 'screenplay',\n",
       " 'it',\n",
       " 'also',\n",
       " 'had',\n",
       " 'nominations',\n",
       " 'for',\n",
       " 'cinematography',\n",
       " 'editing',\n",
       " 'film',\n",
       " 'score',\n",
       " 'which',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'and',\n",
       " 'three',\n",
       " 'more',\n",
       " 'acting',\n",
       " 'performances',\n",
       " 'including',\n",
       " 'yam']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_words_and_emoticons(df.loc[0,'review'][-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, como usaremos os dados de texto limpos repetidamente durante as próximas seções, vamos agora aplicar nossa função de pré-processador a todas as resenhas de filmes em nosso *DataFrame*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[election, is, a, chinese, mob, movie, or, tri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, was, just, watching, a, forensic, files, m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[police, story, is, a, stunning, series, of, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[dear, readers, the, final, battle, between, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, have, seen, the, perfect, son, about, thre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>[if, you, see, the, title, 2069, a, sex, odyss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>[there, were, but, two, reasons, for, me, to, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>[i, saw, this, movie, the, first, seconds, the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>[this, is, another, one, of, those, humans, vs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>[i, saw, this, movie, on, my, local, cable, sy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      [election, is, a, chinese, mob, movie, or, tri...          1\n",
       "1      [i, was, just, watching, a, forensic, files, m...          0\n",
       "2      [police, story, is, a, stunning, series, of, s...          1\n",
       "3      [dear, readers, the, final, battle, between, t...          1\n",
       "4      [i, have, seen, the, perfect, son, about, thre...          1\n",
       "...                                                  ...        ...\n",
       "49995  [if, you, see, the, title, 2069, a, sex, odyss...          0\n",
       "49996  [there, were, but, two, reasons, for, me, to, ...          0\n",
       "49997  [i, saw, this, movie, the, first, seconds, the...          1\n",
       "49998  [this, is, another, one, of, those, humans, vs...          0\n",
       "49999  [i, saw, this, movie, on, my, local, cable, sy...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(tokenizer_words_and_emoticons)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de passarmos para frente, onde treinaremos um modelo de aprendizado de máquina usando o modelo *bag-of-words*, vamos falar brevemente sobre outro tópico útil chamado **remoção de palavras irrelevantes**. *Stop-words* são simplesmente aquelas palavras que são extremamente comuns em todos os tipos de textos e provavelmente não contêm nenhuma (ou apenas uma pequena) informação útil que possa ser usada para distinguir entre diferentes classes de documentos. Exemplos de *Stop-words* são *is*, *and*, *has* e *like*.\n",
    "\n",
    "A remoção de *Stop-words* pode ser útil se estivermos trabalhando com frequências de termos brutos ou normalizados em vez de *tf-idfs*, que já estão reduzindo o peso de palavras que ocorrem com frequência. Para remover *Stop-words* das resenhas de filmes, usaremos o conjunto de 127 *Stop-words*, em inglês, que está disponível na biblioteca *NLTK*, que pode ser obtido chamando a função *nltk.download*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[porter.stem(word) for word in tokenizer_words_and_emoticons('a runner likes running and runs a lot')[-10:] if word not in stop]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8bbe703e4409461e5c1796f0c401e26e62f32801f1a5b19455b89c31c613fbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
